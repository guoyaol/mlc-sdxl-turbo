{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoyaol/miniconda3/envs/sdxl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from web_stable_diffusion import utils\n",
    "from web_stable_diffusion import trace\n",
    "from web_stable_diffusion.utils import get_clip, get_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            result = self.clip(text_input_ids, output_hidden_states=True)\n",
    "            text_embeddings = result.hidden_states[-2]\n",
    "            pool_text_embeddings = result[0]\n",
    "            return text_embeddings, pool_text_embeddings\n",
    "\n",
    "    clip = pipe.text_encoder\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings2(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            result = self.clip(text_input_ids, output_hidden_states=True)\n",
    "            text_embeddings = result.hidden_states[-2]\n",
    "            pool_text_embeddings = result.text_embeds\n",
    "            return text_embeddings, pool_text_embeddings\n",
    "\n",
    "\n",
    "    clip = get_clip(pipe)\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip2\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_latents() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    latents = relax.Var(\"latents\", R.Tensor([1, 4, 64, 64], \"float32\"))\n",
    "\n",
    "    with bb.function(\"cat_latents\", [latents]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([latents, latents], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_latents_to_noise_pred(pipe, device_str: str) -> tvm.IRModule:\n",
    "    class UNetModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, unet):\n",
    "            super().__init__()\n",
    "            self.unet = unet\n",
    "            # Default guidance scale factor in stable diffusion.\n",
    "            # self.guidance_scale = 0.0\n",
    "\n",
    "        def forward(self, latents, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids):\n",
    "            # UNet forward.\n",
    "            noise_pred = self.unet(latents, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids)\n",
    "            return noise_pred\n",
    "\n",
    "    unet = utils.get_unet(pipe, device_str)\n",
    "    unet_to_noise_pred = UNetModelWrapper(unet)\n",
    "    graph = fx.symbolic_trace(unet_to_noise_pred)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\"), ((), \"int32\"), ((1, 77, 2048), \"float32\"), \n",
    "         ((1, 1280), \"float32\"), ((1, 6), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"unet\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_to_image(pipe) -> tvm.IRModule:\n",
    "    class VAEModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            # Scale the latents so that it can be decoded by VAE.\n",
    "            latents = 1 / 0.13025 * latents\n",
    "            # VAE decode\n",
    "            # z = self.vae.post_quant_conv(latents)\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "            # Image normalization\n",
    "            image = (image / 2 + 0.5).clamp(min=0, max=1)\n",
    "            image = (image.permute(0, 2, 3, 1) * 255).round()\n",
    "            return image\n",
    "\n",
    "    vae = utils.get_vae(pipe)\n",
    "    vae_to_image = VAEModelWrapper(vae)\n",
    "\n",
    "    graph = fx.symbolic_trace(vae_to_image)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"vae\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 2048], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 2048], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_enocder_outputs() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 768], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 1280], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_enocder_outputs\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=-1)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_pool_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 1280], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 1280], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_pool_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_rgba() -> tvm.IRModule:\n",
    "    from tvm import te\n",
    "\n",
    "    def f_image_to_rgba(A):\n",
    "        def fcompute(y, x):\n",
    "            return (\n",
    "                A[0, y, x, 0].astype(\"uint32\")\n",
    "                | (A[0, y, x, 1].astype(\"uint32\") << 8)\n",
    "                | (A[0, y, x, 2].astype(\"uint32\") << 16)\n",
    "                | tvm.tir.const(255 << 24, \"uint32\")\n",
    "            )\n",
    "\n",
    "        return te.compute((512, 512), fcompute, name=\"image_to_rgba\")\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", R.Tensor([1, 512, 512, 3], \"float32\"))\n",
    "    with bb.function(\"image_to_rgba\", [x]):\n",
    "        image = bb.emit(\n",
    "            bb.call_te(f_image_to_rgba, x, primfunc_name_hint=\"tir_image_to_rgba\")\n",
    "        )\n",
    "        bb.emit_func_output(image)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_ancestral_discrete_scheduler_steps() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # step, the function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    sigma_from = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    sigma_to = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    noise = relax.Var(\"noise\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "\n",
    "    with bb.function(\n",
    "        \"euler_ancestral_discrete_scheduler_step\",\n",
    "        [sample, model_output, sigma_from, sigma_to, noise],\n",
    "    ):\n",
    "        pred_original_sample = sample - sigma_from * model_output\n",
    "\n",
    "        sigma_up = (sigma_to**relax.const(2.0) * (sigma_from**relax.const(2.0) - sigma_to**relax.const(2.0)) / sigma_from**relax.const(2.0)) ** relax.const(0.5)\n",
    "        sigma_down = (sigma_to**relax.const(2.0) - sigma_up**relax.const(2.0)) ** relax.const(0.5)\n",
    "\n",
    "        derivative = (sample - pred_original_sample) / sigma_from\n",
    "\n",
    "        dt = sigma_down - sigma_from\n",
    "\n",
    "        prev_sample = sample + derivative * dt\n",
    "\n",
    "\n",
    "        prev_sample = bb.emit(\n",
    "            prev_sample + noise * sigma_up,\n",
    "            \"prev_sample\",\n",
    "        )\n",
    "        bb.emit_func_output(prev_sample)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_ancestral_discrete_scheduler_scale() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # scale, the function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "\n",
    "    with bb.function(\n",
    "        \"euler_ancestral_discrete_scheduler_scale\",\n",
    "        [sample, sigma],\n",
    "    ):\n",
    "        scaled_latent_model_input = bb.emit(\n",
    "            sample / ((sigma** relax.const(2.0) + relax.const(1.0)) ** relax.const(0.5)),\n",
    "            \"scaled_latent_model_input\",\n",
    "        )\n",
    "        bb.emit_func_output(scaled_latent_model_input)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'feature_extractor': [None, None], 'image_encoder': [None, None]} were passed to StableDiffusionXLPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'feature_extractor': [None, None], 'image_encoder': [None, None]} are not expected by StableDiffusionXLPipeline and will be ignored.\n",
      "The config attributes {'attention_type': 'default', 'dropout': 0.0, 'reverse_transformer_layers_per_block': None} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/sdxl-turbo\")\n",
    "\n",
    "torch_dev_key = \"cpu\"\n",
    "\n",
    "clip = clip_to_text_embeddings(pipe)\n",
    "clip2 = clip_to_text_embeddings2(pipe)\n",
    "unet = unet_latents_to_noise_pred(pipe, torch_dev_key)\n",
    "vae = vae_to_image(pipe)\n",
    "concat_embeddings = concat_embeddings()\n",
    "concat_pool_embeddings = concat_pool_embeddings()\n",
    "concat_enocder_outputs = concat_enocder_outputs()\n",
    "image_to_rgba = image_to_rgba()\n",
    "scheduler_step = euler_ancestral_discrete_scheduler_steps()\n",
    "scheduler_scale = euler_ancestral_discrete_scheduler_scale()\n",
    "cat_latents = cat_latents()\n",
    "\n",
    "mod: tvm.IRModule = utils.merge_irmodules(\n",
    "    clip,\n",
    "    clip2,\n",
    "    unet,\n",
    "    cat_latents,\n",
    "    vae,\n",
    "    concat_embeddings,\n",
    "    concat_pool_embeddings,\n",
    "    concat_enocder_outputs,\n",
    "    image_to_rgba,\n",
    "    scheduler_step,\n",
    "    scheduler_scale,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relax.frontend.detach_params(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:15:18] /Users/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[03:15:18] /Users/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[03:15:18] /Users/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[03:15:18] /Users/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[03:15:19] /Users/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n"
     ]
    }
   ],
   "source": [
    "mod = relax.pipeline.get_pipeline()(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"clip\", \"clip2\", \"unet\", \"vae\"]\n",
    "scheduler_func_names = [\"euler_ancestral_discrete_scheduler_step\", \"euler_ancestral_discrete_scheduler_scale\"]\n",
    "entry_funcs = (\n",
    "    model_names + scheduler_func_names  + [\"image_to_rgba\", \"concat_embeddings\", \"concat_enocder_outputs\", \"concat_pool_embeddings\", \"cat_latents\"]\n",
    ")\n",
    "\n",
    "# Clean up unused parts of the IRModule.\n",
    "mod = relax.transform.DeadCodeElimination(entry_funcs)(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = relax.transform.LiftTransformParams()(mod)\n",
    "mod = relax.transform.BundleModelParams()(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_transform, mod_deploy = utils.split_transform_deploy_mod(\n",
    "    mod, model_names, entry_funcs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IRModule for build stage:\n",
      "clip_transform_params\n",
      "unet_transform_params\n",
      "vae_transform_params\n",
      "clip2_transform_params\n",
      "\n",
      "In IRModule for deployment stage:\n",
      "concat_embeddings\n",
      "euler_ancestral_discrete_scheduler_scale\n",
      "concat_enocder_outputs\n",
      "image_to_rgba\n",
      "euler_ancestral_discrete_scheduler_step\n",
      "clip2\n",
      "unet\n",
      "vae\n",
      "clip\n",
      "concat_pool_embeddings\n",
      "cat_latents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_relax_funcnames(mod: tvm.IRModule):\n",
    "    for global_var, func in mod.functions.items():\n",
    "        if isinstance(func, relax.Function):\n",
    "            print(global_var.name_hint)\n",
    "    print()\n",
    "    \n",
    "print(\"In IRModule for build stage:\")\n",
    "print_relax_funcnames(mod_transform)\n",
    "\n",
    "print(\"In IRModule for deployment stage:\")\n",
    "print_relax_funcnames(mod_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache dist/params\n",
      "[2533/2533] saving clip2_516\n",
      "All finished, 231 total shards committed, record saved to dist/params/ndarray-cache.json\n",
      "Also saved a bf16 record to dist/params/ndarray-cache-b16.json\n"
     ]
    }
   ],
   "source": [
    "# Compute and save the scheduler constants.\n",
    "\n",
    "# trace.compute_save_scheduler_consts(artifact_path=\"dist\")\n",
    "#TODO: add this compute\n",
    "\n",
    "# Compute and save the models's weight parameters.\n",
    "new_params = utils.transform_params(mod_transform, params)\n",
    "utils.save_params(new_params, artifact_path=\"dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_deploy.script(show_meta=True), file=open(\"dist/before_scheduling.py\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import meta_schedule as ms\n",
    "\n",
    "target = tvm.target.Target(\"apple/m2-gpu\")\n",
    "device = tvm.metal()\n",
    "\n",
    "with target, tvm.transform.PassContext(opt_level=3):\n",
    "    mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)\n",
    "# db = ms.database.create(work_dir=\"log_db_prune_main\")\n",
    "# with target, db, tvm.transform.PassContext(opt_level=3):\n",
    "#     mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_deploy.script(show_meta=True), file=open(\"dist/after_scheduling.py\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tvm import meta_schedule as ms\n",
    "\n",
    "# target = tvm.target.Target(\n",
    "#             \"webgpu\", host=\"llvm -mtriple=wasm32-unknown-unknown-wasm\"\n",
    "#         )\n",
    "\n",
    "# # with target, tvm.transform.PassContext(opt_level=3):\n",
    "# #     mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)\n",
    "# db = ms.database.create(work_dir=\"log_db_prune_main\")\n",
    "# with target, db, tvm.transform.PassContext(opt_level=3):\n",
    "#     mod_deploy = relax.transform.MetaScheduleApplyDatabase(enable_warning=True)(mod_deploy)\n",
    "#     mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)\n",
    "\n",
    "# ex = relax.build(mod=mod_deploy, target=target)\n",
    "# ex.export_library(\"dist/stable_diffusio_xl.wasm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = relax.build(mod=mod_deploy, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.export_library(\"dist/stable_diffusion.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
