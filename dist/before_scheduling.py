metadata = tvm.ir.load_json("""{
  \"root\": 1, 
  \"nodes\": [
    {
      \"type_key\": \"\"
    }, 
    {
      \"type_key\": \"Map\", 
      \"keys\": [
        \"relax.expr.Constant\"
      ], 
      \"data\": [2]
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [3, 14, 25, 34]
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"13\", 
        \"data\": \"0\", 
        \"span\": \"0\", 
        \"struct_info_\": \"4\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"shape\": \"5\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"12\", 
        \"span\": \"0\", 
        \"struct_info_\": \"11\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [7, 8, 9, 10]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"24\", 
        \"data\": \"1\", 
        \"span\": \"0\", 
        \"struct_info_\": \"15\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"shape\": \"16\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"23\", 
        \"span\": \"0\", 
        \"struct_info_\": \"22\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [18, 19, 20, 21]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"33\", 
        \"data\": \"2\", 
        \"span\": \"0\", 
        \"struct_info_\": \"26\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"shape\": \"27\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"32\", 
        \"span\": \"0\", 
        \"struct_info_\": \"31\", 
        \"values\": \"28\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [29, 30]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"160\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\", 
        \"values\": \"28\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"42\", 
        \"data\": \"3\", 
        \"span\": \"0\", 
        \"struct_info_\": \"35\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"shape\": \"36\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"41\", 
        \"span\": \"0\", 
        \"struct_info_\": \"40\", 
        \"values\": \"37\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [38, 39]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"128\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\", 
        \"values\": \"37\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }
  ], 
  \"b64ndarrays\": [
    \"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\", 
    \"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\", 
    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAKAAAAAAAAAAgAIAAAAAAAAAAIA/+a1xPwUpZD+sZVc/GFlLPxH5Pz/vOzU/lhgrP2yGIT9QfRg/mvUPPwvoBz/PTQA/4UDyPrez5D6a6Nc+tNTLPsJtwD4ZqrU+l4CrPpvooT4C2pg+HE2QPqg6iD7Mm4A+ItRyPro+ZT7Ya1g+nFBMPrviQD6HGDY+1ugrPgZLIj7sNhk+0qQQPneNCD756QA+wGfzPRTK5T1o79g9zMzMPflXwT03h7Y9VVGsPa2toj0NlJk9wPyQPXjgiD1XOIE9s/tzPcFVZj1Ec1k9SUlNPYHNQT0q9jY9FbosPZIQIz1p8Rk94VQRPaozCT3ihgE9BJD0PMLh5jxx99k8FMbNPFFDwjxjZbc8EyOtPK1zozz7Tpo8Oa2RPBKHiTyd1YE8qCR1PCBuZzzxe1o8J0NOPGO5Qjzf1Dc8U4wtPArXIzzHrBo8wwUSPKzaCTyKJAI8rbn1O8j65zvCANs7isDOO8IvwzuaRLg7zfWtO6M6pDvOCps7iF6SO3Uuijunc4I7Dk92O82HaDvdhVs7Mz5PO2mmQzuetDg7jV8uO3OeJDsNaRs7grcSO3WCCjvwwgI7weT2OigV6TpQC9w6NbzPOlkdxDrmJLk6iMmuOoUCpTqCx5s6rhCTOqjWijpuEoM633p3OtmiaToTkVw6cDpQOoqURDptlTk6yTMvOtVmJTo1Jhw6GWoTOg8rCzobYgM6QRH4Odkw6jkhF905BbnQOQsMxTk+Bro5UJ6vOWHLpTkhhZw5scOTOaR/izn1sYM5Dah4OS6/ajmPnV057TdROdODRTlTdzo5DAkwOSUwJjlC5Bw5gB0UOW3UCzkIAgQ5PT/5OOpN6zhAJN44\", 
    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAIAAAAAAAAAAAAIAAAAAAAAAAIA/+DluP9avXT+sS04/Efk/PwalMj/gPSY/K7MaP5r1Dz/u9gU/zlP5PmAE6D6a6Nc+I+vIPhv4uj7//K0+m+ihPuqqlj4DNYw+CHmCPiLUcj42+GE+7UdSPnyuQz6HGDY+CXQpPj+wHT6QvRI+d40IPt8k/j3Vf+w9ZBTcPczMzD3HlL49eFmxPVcJpT0NlJk9bOqOPUz+hD0RhXc9wVVmPeNXVj1Adkc9GJ05PRW6LD0bvCA9SJMVPcswCz3ihgE9dxHxPNFU4DyowdA8TkPCPJHGtDyJOag8loucPDmtkTwJkIc8ME18PObIajzxe1o8tVBLPA4zPTxQEDA8CtcjPAZ3GDwu4Q08dwcEPK259TtFquQ7FcrUOwoExjuaRLg7gnmrO8yRnzuvfZQ7dS6KO3yWgDsNUm87erReOzM+TzvC2kA7DXczO1MBJzsNaRs72J4QO22UBjvwePo6KBXpOnXm2DpZ18k669O7Oo3Jrjr3pqI6C1yXOtnZjDpuEoM6ovFzOuMBYzokP1M6ipREOp3uNjpEOyo6omkeOhRqEzoBLgk6rE//OeSV7TkhF905k73NOdZ0vzn7KbI5YculOaBImjl1ko85pJqFOQ2oeDmMZGc54lNXOb9gSDlTdzo5KoUtORl5ITkmQxY5bdQLOSgfAjndLPI4jlzhOA==\"
  ], 
  \"attrs\": {\"tvm_version\": \"0.14.dev0\"}
}""")
# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def add34(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] + B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def add35(A: T.Buffer((), "float32"), T_add: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_add"):
            vi = T.axis.spatial(1, T.int64(0))
            T.reads(A[()])
            T.writes(T_add[()])
            T_add[()] = A[()] + T.float32(1)

    @T.prim_func(private=True)
    def argmax(A: T.Buffer((T.int64(1), T.int64(77)), "int32"), A_red: T.Buffer((T.int64(1),), "int64")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1),), "int64")
        A_red_temp_v1 = T.alloc_buffer((T.int64(1),), "int32")
        for ax0, k1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("A_red_temp"):
                v_ax0, v_k1 = T.axis.remap("SR", [ax0, k1])
                T.reads(A[v_ax0, v_k1])
                T.writes(A_red_temp_v0[v_ax0], A_red_temp_v1[v_ax0])
                with T.init():
                    A_red_temp_v0[v_ax0] = T.int64(-1)
                    A_red_temp_v1[v_ax0] = -2147483648
                v_A_red_temp_v0: T.int64 = T.Select(A_red_temp_v1[v_ax0] > A[v_ax0, v_k1] or A_red_temp_v1[v_ax0] == A[v_ax0, v_k1] and A_red_temp_v0[v_ax0] < v_k1, A_red_temp_v0[v_ax0], v_k1)
                v_A_red_temp_v1: T.int32 = T.Select(A_red_temp_v1[v_ax0] > A[v_ax0, v_k1], A_red_temp_v1[v_ax0], A[v_ax0, v_k1])
                A_red_temp_v0[v_ax0] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0] = v_A_red_temp_v1
        for ax0 in range(T.int64(1)):
            with T.block("A_red"):
                v_ax0 = T.axis.spatial(T.int64(1), ax0)
                T.reads(A_red_temp_v0[v_ax0])
                T.writes(A_red[v_ax0])
                A_red[v_ax0] = A_red_temp_v0[v_ax0]

    @T.prim_func(private=True)
    def cast3(A: T.Buffer((T.int64(1), T.int64(77)), "int32"), compute: T.Buffer((T.int64(1), T.int64(77)), "int32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(A[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = A[v_i0, v_i1]

    @T.prim_func(private=True)
    def concatenate(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), T_concat: T.Buffer((T.int64(1), T.int64(77), T.int64(2048)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(2048)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(B[v_ax0, v_ax1, v_ax2 - T.int64(768)], A[v_ax0, v_ax1, v_ax2])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2])
                T_concat[v_ax0, v_ax1, v_ax2] = T.if_then_else(T.int64(768) <= v_ax2, B[v_ax0, v_ax1, v_ax2 - T.int64(768)], A[v_ax0, v_ax1, v_ax2])

    @T.prim_func(private=True)
    def concatenate10(A: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(320) <= v_ax1, B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def concatenate11(A: T.Buffer((T.int64(1), T.int64(77), T.int64(2048)), "float32"), B: T.Buffer((T.int64(1), T.int64(77), T.int64(2048)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(77), T.int64(2048)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(77), T.int64(2048)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2])
                T_concat[v_ax0, v_ax1, v_ax2] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])

    @T.prim_func(private=True)
    def concatenate12(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def concatenate13(A: T.Buffer((T.int64(1), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1), T.int64(1280)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(2), T.int64(1280)):
            with T.block("T_concat"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(B[v_ax0 - T.int64(1), v_ax1], A[v_ax0, v_ax1])
                T.writes(T_concat[v_ax0, v_ax1])
                T_concat[v_ax0, v_ax1] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1], A[v_ax0, v_ax1])

    @T.prim_func(private=True)
    def concatenate4(A: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def concatenate6(A: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(32), T.int64(32)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def concatenate9(A: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(64), T.int64(64)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def divide11(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[()])
                T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] / B[()]

    @T.prim_func(private=True)
    def divide12(A: T.Buffer((), "float32"), B: T.Buffer((), "float32"), T_divide: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_divide"):
            vi = T.axis.spatial(1, T.int64(0))
            T.reads(A[()], B[()])
            T.writes(T_divide[()])
            T_divide[()] = A[()] / B[()]

    @T.prim_func(private=True)
    def fused_broadcast_to_strided_slice_reshape_cast_multiply_multiply1_tir_sin_tir_cos_concatenate1_strided_slice1_reshape1_strided_slice2_reshape1_concatenate1_cast1(inp_1: T.Buffer((), "int32"), param_0: T.Buffer((T.int64(1), T.int64(160)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(320)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_broadcast_to_intermediate = T.alloc_buffer((T.int64(1),), "int32")
        var_T_strided_slice_with_axes_intermediate = T.alloc_buffer((T.int64(1),), "int32")
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1)), "int32")
        var_compute_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1)))
        var_T_multiply_intermediate = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_multiply_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_compute_intermediate_2 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_compute_intermediate_3 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_concat_intermediate = T.alloc_buffer((T.int64(1), T.int64(320)))
        var_T_strided_slice_with_axes_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_strided_slice_with_axes_intermediate_2 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_reshape_intermediate_2 = T.alloc_buffer((T.int64(1), T.int64(160)))
        var_T_concat_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(320)))
        for ax0 in range(T.int64(1)):
            with T.block("T_broadcast_to"):
                v_ax0 = T.axis.spatial(T.int64(1), ax0)
                T.reads(inp_1[()])
                T.writes(var_T_broadcast_to_intermediate[v_ax0])
                var_T_broadcast_to_intermediate[v_ax0] = inp_1[()]
        for ax0 in range(T.int64(1)):
            with T.block("T_strided_slice_with_axes"):
                v_ax0 = T.axis.spatial(T.int64(1), ax0)
                T.reads(var_T_broadcast_to_intermediate[v_ax0])
                T.writes(var_T_strided_slice_with_axes_intermediate[v_ax0])
                var_T_strided_slice_with_axes_intermediate[v_ax0] = var_T_broadcast_to_intermediate[v_ax0]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
            with T.block("T_reshape"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate[T.int64(0)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1])
                var_T_reshape_intermediate[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate[T.int64(0)]
        for i0, i1 in T.grid(T.int64(1), T.int64(1)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1])
                T.writes(var_compute_intermediate_1[v_i0, v_i1])
                var_compute_intermediate_1[v_i0, v_i1] = T.Cast("float32", var_T_reshape_intermediate[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_multiply"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_compute_intermediate_1[v_ax0, T.int64(0)], param_0[v_ax0, v_ax1])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                var_T_multiply_intermediate[v_ax0, v_ax1] = var_compute_intermediate_1[v_ax0, T.int64(0)] * param_0[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_multiply_intermediate[v_ax0, v_ax1])
                T.writes(var_T_multiply_intermediate_1[v_ax0, v_ax1])
                var_T_multiply_intermediate_1[v_ax0, v_ax1] = var_T_multiply_intermediate[v_ax0, v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("compute_1"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_multiply_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate_2[v_i0, v_i1])
                var_compute_intermediate_2[v_i0, v_i1] = T.sin(var_T_multiply_intermediate_1[v_i0, v_i1])
        for i0, i1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("compute_2"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_multiply_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate_3[v_i0, v_i1])
                var_compute_intermediate_3[v_i0, v_i1] = T.cos(var_T_multiply_intermediate_1[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(320)):
            with T.block("T_concat"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_compute_intermediate_3[v_ax0, v_ax1 - T.int64(160)], var_compute_intermediate_2[v_ax0, v_ax1])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1])
                var_T_concat_intermediate[v_ax0, v_ax1] = T.if_then_else(T.int64(160) <= v_ax1, var_compute_intermediate_3[v_ax0, v_ax1 - T.int64(160)], var_compute_intermediate_2[v_ax0, v_ax1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_strided_slice_with_axes_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_concat_intermediate[v_ax0, v_ax1 + T.int64(160)])
                T.writes(var_T_strided_slice_with_axes_intermediate_1[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate_1[v_ax0, v_ax1] = var_T_concat_intermediate[v_ax0, v_ax1 + T.int64(160)]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate_1[T.int64(0), v_ax1 % T.int64(160)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1])
                var_T_reshape_intermediate_1[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate_1[T.int64(0), v_ax1 % T.int64(160)]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_strided_slice_with_axes_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_concat_intermediate[v_ax0, v_ax1])
                T.writes(var_T_strided_slice_with_axes_intermediate_2[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate_2[v_ax0, v_ax1] = var_T_concat_intermediate[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(160)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate_2[T.int64(0), v_ax1 % T.int64(160)])
                T.writes(var_T_reshape_intermediate_2[v_ax0, v_ax1])
                var_T_reshape_intermediate_2[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate_2[T.int64(0), v_ax1 % T.int64(160)]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(320)):
            with T.block("T_concat_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_reshape_intermediate_2[v_ax0, v_ax1 - T.int64(160)], var_T_reshape_intermediate_1[v_ax0, v_ax1])
                T.writes(var_T_concat_intermediate_1[v_ax0, v_ax1])
                var_T_concat_intermediate_1[v_ax0, v_ax1] = T.if_then_else(T.int64(160) <= v_ax1, var_T_reshape_intermediate_2[v_ax0, v_ax1 - T.int64(160)], var_T_reshape_intermediate_1[v_ax0, v_ax1])
        for i0, i1 in T.grid(T.int64(1), T.int64(320)):
            with T.block("compute_3"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_concat_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = var_T_concat_intermediate_1[v_i0, v_i1]

    @T.prim_func(private=True)
    def fused_cast3_reshape32(lv: T.Buffer((T.int64(1), T.int64(77)), "int32"), var_T_reshape_intermediate: T.Buffer((T.int64(77),), "int32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_compute_intermediate = T.alloc_buffer((T.int64(1), T.int64(77)), "int32")
        for i0, i1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(lv[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = lv[v_i0, v_i1]
        for ax0 in range(T.int64(77)):
            with T.block("T_reshape"):
                v_ax0 = T.axis.spatial(T.int64(77), ax0)
                T.reads(var_compute_intermediate[T.int64(0), v_ax0 % T.int64(77)])
                T.writes(var_T_reshape_intermediate[v_ax0])
                var_T_reshape_intermediate[v_ax0] = var_compute_intermediate[T.int64(0), v_ax0 % T.int64(77)]

    @T.prim_func(private=True)
    def fused_conv2d10_add13_add13(lv2553: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), lv2555: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv2562: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(18), T.int64(18)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(2560), T.int64(18), T.int64(18)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv2553[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(17) and T.int64(1) <= v_i3 and v_i3 < T.int64(17), lv2553[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(2560), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv2555[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv2555[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv2562[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv2562[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d11_add13(lv2551: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), lv2570: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv2551[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv2551[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(2560), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv2570[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv2570[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d12_add13_add13(lv3963: T.Buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_2_conv1_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv3965: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv3972: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(18), T.int64(18)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(18), T.int64(18)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv3963[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(17) and T.int64(1) <= v_i3 and v_i3 < T.int64(17), lv3963[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(1920), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_2_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_2_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3965[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv3965[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv3972[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv3972[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d13_add13(lv3961: T.Buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv3980: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(16), T.int64(16)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv3961[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv3961[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(1920), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_2_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_2_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3980[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv3980[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d14_add18(lv4666: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv4668: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv4666[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv4666[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32), T.int64(1280), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4668[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4668[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d15_add6_add6(lv4672: T.Buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv4674: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv4681: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv4672[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv4672[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(1920), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4674[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4674[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv4681[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv4681[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d16_add6(lv4670: T.Buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv4689: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(32), T.int64(32)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv4670[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv4670[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(1920), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4689[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4689[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d17_add6_add6(lv4841: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv4843: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv4850: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv4841[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv4841[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(1280), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4843[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4843[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv4850[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv4850[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d18_add6(lv4839: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv4858: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv4839[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv4839[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(1280), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_1_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_1_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4858[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4858[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d19_add6_add6(lv5010: T.Buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_2_conv1_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv5012: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv5019: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5010[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv5010[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(960), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5012[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5012[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv5019[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv5019[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d1_add2_add2(lv50: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv52: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv59: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv50[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv50[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(320), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv52[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv52[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv59[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv59[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d1_add2_add4_divide(lv62: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv2_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv64: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv48: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv62[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv62[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(320), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv64[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv64[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv48[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv48[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d20_add6(lv5008: T.Buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv5027: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(32), T.int64(32)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5008[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv5008[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(960), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5027[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5027[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d21_add19(lv5177: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv5179: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5177[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv5177[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5179[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5179[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d22_add2_add2(lv5183: T.Buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv5185: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv5192: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5183[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv5183[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(960), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5185[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5185[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv5192[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv5192[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d23_add2(lv5181: T.Buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv5200: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(64), T.int64(64)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5181[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv5181[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(960), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5200[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5200[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d24_add2_add2(lv5206: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv5208: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv5215: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5206[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv5206[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5208[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5208[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv5215[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv5215[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d25_add2(lv5204: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv5223: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5204[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv5204[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(640), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5223[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5223[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d26_add20(lv5251: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_conv_out_weight: T.Buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv5253: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv5251[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv5251[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64), T.int64(320), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_out_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_out_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5253[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5253[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d27_add20(lv: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_post_quant_conv_weight: T.Buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), "float32"), lv2: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64), T.int64(4), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_post_quant_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_post_quant_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv2[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d28_add21(lv3: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_decoder_conv_in_weight: T.Buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(4), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv3[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv3[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64), T.int64(4), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_in_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_in_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d29_add21(lv8: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv1_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv10: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv8[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv8[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv10[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv10[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d29_add21_add22_divide6(lv13: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv15: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv13[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv13[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv15[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv15[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv6[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv6[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d29_add21_add22_divide6_divide6(lv61: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_1_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv63: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv54: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_divide_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv61[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv61[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_1_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_1_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv63[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv63[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv54[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv54[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_divide_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_divide_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_divide_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d2_add5(lv86: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_downsamplers_0_conv_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv88: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv86[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), lv86[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32), T.int64(320), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_0_downsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_0_downsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv88[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv88[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d30_add24(lv104: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv106: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(130), T.int64(130)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv104[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(129) and T.int64(1) <= v_i3 and v_i3 < T.int64(129), lv104[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv106[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv106[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d30_add24_add25_divide7(lv114: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv116: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(130), T.int64(130)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv114[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(129) and T.int64(1) <= v_i3 and v_i3 < T.int64(129), lv114[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv116[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv116[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv107[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv107[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d31_add26(lv144: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv146: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(258), T.int64(258)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv144[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(257) and T.int64(1) <= v_i3 and v_i3 < T.int64(257), lv144[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv146[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv146[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d32_add27(lv149: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv151: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(258), T.int64(258)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv149[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(257) and T.int64(1) <= v_i3 and v_i3 < T.int64(257), lv149[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256), T.int64(512), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv151[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv151[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d33_add27(lv164: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv166: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(258), T.int64(258)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv164[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(257) and T.int64(1) <= v_i3 and v_i3 < T.int64(257), lv164[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256), T.int64(256), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv166[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv166[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d33_add27_add28_divide8(lv154: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv156: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(258), T.int64(258)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv154[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(257) and T.int64(1) <= v_i3 and v_i3 < T.int64(257), lv154[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256), T.int64(256), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv156[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv156[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv160[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv160[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d34_add27(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv159: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv147[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv147[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256), T.int64(512), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv159[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv159[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d35_add29(lv187: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_2_upsamplers_0_conv_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv189: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(514), T.int64(514)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv187[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(513) and T.int64(1) <= v_i3 and v_i3 < T.int64(513), lv187[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512), T.int64(256), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv189[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv189[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d36_add30(lv192: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv194: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(514), T.int64(514)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv192[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(513) and T.int64(1) <= v_i3 and v_i3 < T.int64(513), lv192[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512), T.int64(256), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv194[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv194[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d37_add30(lv207: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_1_conv1_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv209: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(514), T.int64(514)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv207[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(513) and T.int64(1) <= v_i3 and v_i3 < T.int64(513), lv207[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512), T.int64(128), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv209[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv209[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d37_add30_add31_divide9(lv197: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv2_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv199: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), lv203: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(514), T.int64(514)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv197[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(513) and T.int64(1) <= v_i3 and v_i3 < T.int64(513), lv197[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512), T.int64(128), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv199[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv199[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv203[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv203[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d38_add30(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv202: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv190[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv190[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512), T.int64(256), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv202[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv202[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d39_add32_divide10_add33_tir_clip(lv231: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_conv_out_weight: T.Buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv233: T.Buffer((T.int64(1), T.int64(3), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)))
        var_T_divide_intermediate = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(514), T.int64(514)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv231[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(513) and T.int64(1) <= v_i3 and v_i3 < T.int64(513), lv231[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(3), T.int64(512), T.int64(512), T.int64(128), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_out_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_out_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(3), T.int64(512), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv233[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv233[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(3), T.int64(512), T.int64(512)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(0.5)
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(3), T.int64(512), T.int64(512)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(0.5)
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(3), T.int64(512), T.int64(512)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_add_intermediate_1[v_i0, v_i1, v_i2, v_i3])
                T.writes(var_compute_intermediate[v_i0, v_i1, v_i2, v_i3])
                var_compute_intermediate[v_i0, v_i1, v_i2, v_i3] = T.max(T.min(var_T_add_intermediate_1[v_i0, v_i1, v_i2, v_i3], T.float32(1)), T.float32(0))

    @T.prim_func(private=True)
    def fused_conv2d3_add6_add6(lv91: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv93: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv100: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv91[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv91[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(320), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv93[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv93[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv100[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv100[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d4_add6_add6(lv259: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv261: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv268: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv259[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv259[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv261[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv261[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv268[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv268[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d4_add6_add8_divide1(lv103: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv105: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv109: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv103[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv103[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv105[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv105[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv109[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv109[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d5_add6(lv89: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv108: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv89[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv89[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32), T.int64(320), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv108[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv108[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d6_add12(lv422: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_downsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv424: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(34), T.int64(34)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(34), T.int64(34)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv422[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(33) and T.int64(1) <= v_i3 and v_i3 < T.int64(33), lv422[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_1_downsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_1_downsamplers_0_conv_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv424[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv424[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d7_add13_add13(lv427: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv429: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv436: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(18), T.int64(18)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(18), T.int64(18)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv427[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(17) and T.int64(1) <= v_i3 and v_i3 < T.int64(17), lv427[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(640), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv429[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv429[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv436[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv436[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d8_add13_add13(lv1131: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv1133: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1140: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(18), T.int64(18)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(18), T.int64(18)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv1131[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(17) and T.int64(1) <= v_i3 and v_i3 < T.int64(17), lv1131[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(1280), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv1133[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv1133[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], lv1140[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + lv1140[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d8_add13_add14_divide4(lv439: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv441: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv445: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(18), T.int64(18)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(18), T.int64(18)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv439[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(17) and T.int64(1) <= v_i3 and v_i3 < T.int64(17), lv439[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(1280), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv2_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv441[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv441[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv445[v_ax0, v_ax1, v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv445[v_ax0, v_ax1, v_ax2, v_ax3] + var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_conv2d9_add13(lv425: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv444: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv425[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = lv425[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16), T.int64(640), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv_shortcut_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv444[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv444[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv2d_add2(inp_0: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), unet_conv_in_weight: T.Buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv47: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(66), T.int64(66)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(4), T.int64(66), T.int64(66)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(inp_0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(65) and T.int64(1) <= v_i3 and v_i3 < T.int64(65), inp_0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64), T.int64(4), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_in_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_in_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv47[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv47[v_ax0, v_ax1, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_group_norm10_silu9(lv4839: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_1_norm1_weight: T.Buffer((T.int64(1280),), "float32"), unet_up_blocks_1_resnets_1_norm1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(32), T.int64(32)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv4839[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv4839[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_1_norm1_weight[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_1_resnets_1_norm1_weight[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_1_norm1_bias[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_1_resnets_1_norm1_bias[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm11_silu10(lv5008: T.Buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_2_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_1_resnets_2_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(30), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(30)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(30)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(30), T.int64(32), T.int64(32)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)))
        compute = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv5008[T.int64(0), (v_ax1 * T.int64(30) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(960), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv5008[T.int64(0), (v_ax1 * T.int64(30) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(960), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(30)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_2_norm1_weight[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_1_resnets_2_norm1_weight[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(30)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_2_norm1_bias[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_1_resnets_2_norm1_bias[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.2552083333333333e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(3.2552083333333333e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.2552083333333333e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(32), T.int64(32)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(32), T.int64(32)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm12_silu11(lv5181: T.Buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(30), T.int64(64), T.int64(64)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(30)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(30)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(30), T.int64(64), T.int64(64)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)))
        compute = T.alloc_buffer((T.int64(1), T.int64(960), T.int64(64), T.int64(64)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(64), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv5181[T.int64(0), (v_ax1 * T.int64(30) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(960), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv5181[T.int64(0), (v_ax1 * T.int64(30) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(960), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(64), T.int64(64)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(30)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(30)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(30) + v_ax1) % T.int64(960)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(30), T.int64(64), T.int64(64)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(8.1380208333333332e-06)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(8.1380208333333332e-06) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(8.1380208333333332e-06) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(64), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(960), T.int64(64), T.int64(64)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm13_silu12(lv5204: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_resnets_1_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_up_blocks_2_resnets_1_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(64), T.int64(64)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(64), T.int64(64)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)))
        compute = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(64), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv5204[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(640), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv5204[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(640), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(64), T.int64(64)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_2_resnets_1_norm1_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_2_resnets_1_norm1_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_2_resnets_1_norm1_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_2_resnets_1_norm1_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(64), T.int64(64)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.2207031250000001e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.2207031250000001e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.2207031250000001e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm14_silu13(lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_mid_block_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(64), T.int64(64)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(64), T.int64(64)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        compute = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(64), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(512), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(512), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(64), T.int64(64)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_mid_block_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_mid_block_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_mid_block_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_mid_block_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(64), T.int64(64)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm16_silu14(lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(128), T.int64(128)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(128), T.int64(128)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        compute = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(128), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(128) + v_ax3) // T.int64(128) + v_ax2) % T.int64(512), (v_ax4 // T.int64(128) + v_ax3) % T.int64(128), v_ax4 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(128) + v_ax3) // T.int64(128) + v_ax2) % T.int64(512), (v_ax4 // T.int64(128) + v_ax3) % T.int64(128), v_ax4 % T.int64(128)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(128), T.int64(128)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_up_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_up_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(128), T.int64(128)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.814697265625e-06)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(3.814697265625e-06) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.814697265625e-06) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16), (v_ax3 // T.int64(128) + v_ax2) % T.int64(128), v_ax3 % T.int64(128)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16), (v_ax3 // T.int64(128) + v_ax2) % T.int64(128), v_ax3 % T.int64(128)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm17_silu15(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(256), T.int64(256)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(256), T.int64(256)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)))
        compute = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(256), T.int64(256)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(256) + v_ax3) // T.int64(256) + v_ax2) % T.int64(512), (v_ax4 // T.int64(256) + v_ax3) % T.int64(256), v_ax4 % T.int64(256)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_ax4 // T.int64(256) + v_ax3) // T.int64(256) + v_ax2) % T.int64(512), (v_ax4 // T.int64(256) + v_ax3) % T.int64(256), v_ax4 % T.int64(256)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(256), T.int64(256)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_up_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_up_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(256), T.int64(256)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.5367431640625e-07) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm18_silu16(lv152: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(8), T.int64(256), T.int64(256)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(8)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(8)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(8), T.int64(256), T.int64(256)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        compute = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(256), T.int64(256)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_ax4 // T.int64(256) + v_ax3) // T.int64(256) + v_ax2) % T.int64(256), (v_ax4 // T.int64(256) + v_ax3) % T.int64(256), v_ax4 % T.int64(256)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_ax4 // T.int64(256) + v_ax3) // T.int64(256) + v_ax2) % T.int64(256), (v_ax4 // T.int64(256) + v_ax3) % T.int64(256), v_ax4 % T.int64(256)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(256), T.int64(256)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(8)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_2_resnets_0_norm2_weight[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_up_blocks_2_resnets_0_norm2_weight[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(8)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_2_resnets_0_norm2_bias[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_up_blocks_2_resnets_0_norm2_bias[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(256), T.int64(256)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.9073486328125e-06)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.9073486328125e-06) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.9073486328125e-06) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(256), T.int64(256)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm19_silu17(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(8), T.int64(512), T.int64(512)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(8)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(8)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(8), T.int64(512), T.int64(512)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)))
        compute = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(512), T.int64(512)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_ax4 // T.int64(512) + v_ax3) // T.int64(512) + v_ax2) % T.int64(256), (v_ax4 // T.int64(512) + v_ax3) % T.int64(512), v_ax4 % T.int64(512)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_ax4 // T.int64(512) + v_ax3) // T.int64(512) + v_ax2) % T.int64(256), (v_ax4 // T.int64(512) + v_ax3) % T.int64(512), v_ax4 % T.int64(512)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(512), T.int64(512)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(8)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_3_resnets_0_norm1_weight[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_up_blocks_3_resnets_0_norm1_weight[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(8)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_3_resnets_0_norm1_bias[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_up_blocks_3_resnets_0_norm1_bias[(v_ax0 * T.int64(8) + v_ax1) % T.int64(256)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(8), T.int64(512), T.int64(512)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.76837158203125e-07)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.76837158203125e-07) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.76837158203125e-07) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm1_silu2(lv89: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(10), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(10)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(10)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(10), T.int64(32), T.int64(32)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)))
        compute = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv89[T.int64(0), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(320), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv89[T.int64(0), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(320), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(10)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_down_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(10)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_down_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(32), T.int64(32)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm20_silu18(lv195: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_weight: T.Buffer((T.int64(128),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_bias: T.Buffer((T.int64(128),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(4), T.int64(512), T.int64(512)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(4)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(4)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(4), T.int64(512), T.int64(512)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        compute = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(4), T.int64(512), T.int64(512)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_ax4 // T.int64(512) + v_ax3) // T.int64(512) + v_ax2) % T.int64(128), (v_ax4 // T.int64(512) + v_ax3) % T.int64(512), v_ax4 % T.int64(512)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_ax4 // T.int64(512) + v_ax3) // T.int64(512) + v_ax2) % T.int64(128), (v_ax4 // T.int64(512) + v_ax3) % T.int64(512), v_ax4 % T.int64(512)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(4), T.int64(512), T.int64(512)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(4)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_3_resnets_0_norm2_weight[(v_ax0 * T.int64(4) + v_ax1) % T.int64(128)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = vae_decoder_up_blocks_3_resnets_0_norm2_weight[(v_ax0 * T.int64(4) + v_ax1) % T.int64(128)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(4)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(vae_decoder_up_blocks_3_resnets_0_norm2_bias[(v_ax0 * T.int64(4) + v_ax1) % T.int64(128)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = vae_decoder_up_blocks_3_resnets_0_norm2_bias[(v_ax0 * T.int64(4) + v_ax1) % T.int64(128)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(4), T.int64(512), T.int64(512)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.5367431640625e-07) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(512), T.int64(512)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm2_silu3(lv101: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm2_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_1_resnets_0_norm2_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        compute = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv101[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv101[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_1_resnets_0_norm2_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_down_blocks_1_resnets_0_norm2_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_1_resnets_0_norm2_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_down_blocks_1_resnets_0_norm2_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm4_silu4(lv425: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(16), T.int64(16)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(16), T.int64(16)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)))
        compute = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(16), T.int64(16)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv425[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(640), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv425[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(640), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(16), T.int64(16)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_down_blocks_2_resnets_0_norm1_weight[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_down_blocks_2_resnets_0_norm1_bias[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(16), T.int64(16)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00019531250000000001) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00019531250000000001) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(16), T.int64(16)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm5_silu5(lv437: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(1280),), "float32"), unet_down_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv437[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv437[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_2_resnets_0_norm2_weight[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_down_blocks_2_resnets_0_norm2_weight[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_2_resnets_0_norm2_bias[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_down_blocks_2_resnets_0_norm2_bias[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm7_silu6(lv2551: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(2560),), "float32"), unet_up_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(80), T.int64(16), T.int64(16)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(80)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(80)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(80), T.int64(16), T.int64(16)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)))
        compute = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(80), T.int64(16), T.int64(16)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv2551[T.int64(0), (v_ax1 * T.int64(80) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(2560), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv2551[T.int64(0), (v_ax1 * T.int64(80) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(2560), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(80), T.int64(16), T.int64(16)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(80)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_0_resnets_0_norm1_weight[(v_ax0 * T.int64(80) + v_ax1) % T.int64(2560)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_0_resnets_0_norm1_weight[(v_ax0 * T.int64(80) + v_ax1) % T.int64(2560)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(80)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_0_resnets_0_norm1_bias[(v_ax0 * T.int64(80) + v_ax1) % T.int64(2560)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_0_resnets_0_norm1_bias[(v_ax0 * T.int64(80) + v_ax1) % T.int64(2560)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(80), T.int64(16), T.int64(16)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm8_silu7(lv3961: T.Buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_resnets_2_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_0_resnets_2_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(60), T.int64(16), T.int64(16)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(60)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(60)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(60), T.int64(16), T.int64(16)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(16), T.int64(16)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv3961[T.int64(0), (v_ax1 * T.int64(60) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1920), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv3961[T.int64(0), (v_ax1 * T.int64(60) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1920), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(16), T.int64(16)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(60)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_0_resnets_2_norm1_weight[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_0_resnets_2_norm1_weight[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(60)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_0_resnets_2_norm1_bias[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_0_resnets_2_norm1_bias[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(16), T.int64(16)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(6.5104166666666666e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(6.5104166666666666e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(6.5104166666666666e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(16), T.int64(16)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(16), T.int64(16)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(16), T.int64(16)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm9_silu8(lv4670: T.Buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(60), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(60)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(60)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(60), T.int64(32), T.int64(32)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1920), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv4670[T.int64(0), (v_ax1 * T.int64(60) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(1920), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv4670[T.int64(0), (v_ax1 * T.int64(60) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(1920), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(60)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_up_blocks_1_resnets_0_norm1_weight[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(60)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_up_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_up_blocks_1_resnets_0_norm1_bias[(v_ax0 * T.int64(60) + v_ax1) % T.int64(1920)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(60), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.6276041666666666e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.6276041666666666e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.6276041666666666e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1920), T.int64(32), T.int64(32)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(32), T.int64(32)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_group_norm_silu1(lv48: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(10), T.int64(64), T.int64(64)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_1 = T.alloc_buffer((T.int64(32), T.int64(10)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(10)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(10), T.int64(64), T.int64(64)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        compute = T.alloc_buffer((T.int64(1), T.int64(320), T.int64(64), T.int64(64)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(64), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(lv48[T.int64(0), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = lv48[T.int64(0), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(64), T.int64(64)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(10)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_0_resnets_0_norm1_weight[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)])
                T.writes(T_reshape_1[v_ax0, v_ax1])
                T_reshape_1[v_ax0, v_ax1] = unet_down_blocks_0_resnets_0_norm1_weight[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(10)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(unet_down_blocks_0_resnets_0_norm1_bias[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = unet_down_blocks_0_resnets_0_norm1_bias[(v_ax0 * T.int64(10) + v_ax1) % T.int64(320)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(10), T.int64(64), T.int64(64)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_1[v_ax1, v_ax2], T_reshape_2[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * T_reshape_1[v_ax1, v_ax2] + T_reshape_2[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(compute[v_i0, v_i1, v_i2, v_i3])
                compute[v_i0, v_i1, v_i2, v_i3] = T.sigmoid(var_T_reshape_intermediate[v_i0, v_i1, v_i2, v_i3])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], compute[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * compute[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_matmul11_add11(lv172: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv173: T.Buffer((T.int64(640), T.int64(5120)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: T.Buffer((T.int64(5120),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(5120)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(5120)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1024), T.int64(5120), T.int64(640)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv172[v_i0, v_i1, v_k], lv173[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv172[v_i0, v_i1, v_k] * lv173[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(5120)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul12_add9_add10(lv180: T.Buffer((T.int64(1), T.int64(1024), T.int64(2560)), "float32"), lv181: T.Buffer((T.int64(2560), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(640),), "float32"), lv171: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1024), T.int64(640), T.int64(2560)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv180[v_i0, v_i1, v_k], lv181[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv180[v_i0, v_i1, v_k] * lv181[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2], lv171[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] + lv171[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul13_add15(lv450: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv451: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_proj_in_bias: T.Buffer((T.int64(1280),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(256), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv450[v_i0, v_i1, v_k], lv451[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv450[v_i0, v_i1, v_k] * lv451[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_proj_in_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_proj_in_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul13_add15_divide5_add16(lv475: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv476: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(1280),), "float32"), lv453: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        var_T_divide_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(256), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv475[v_i0, v_i1, v_k], lv476[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv475[v_i0, v_i1, v_k] * lv476[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2], lv453[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_divide_intermediate[v_ax0, v_ax1, v_ax2] + lv453[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul14_multiply7(lv462: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32"), lv469: T.Buffer((T.int64(1), T.int64(20), T.int64(64), T.int64(256)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)))
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(lv462[v_i0, v_i1, v_i2, v_k], lv469[v_i0, v_i1, v_k, v_i3])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] + lv462[v_i0, v_i1, v_i2, v_k] * lv469[v_i0, v_i1, v_k, v_i3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], param_0[()])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * param_0[()]

    @T.prim_func(private=True)
    def fused_matmul17_multiply8(lv489: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32"), lv496: T.Buffer((T.int64(1), T.int64(20), T.int64(64), T.int64(77)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)))
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(lv489[v_i0, v_i1, v_i2, v_k], lv496[v_i0, v_i1, v_k, v_i3])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] + lv489[v_i0, v_i1, v_i2, v_k] * lv496[v_i0, v_i1, v_k, v_i3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], param_0[()])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * param_0[()]

    @T.prim_func(private=True)
    def fused_matmul19_add17(lv508: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv509: T.Buffer((T.int64(1280), T.int64(10240)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias: T.Buffer((T.int64(10240),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(10240)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(10240)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(256), T.int64(10240), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv508[v_i0, v_i1, v_k], lv509[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv508[v_i0, v_i1, v_k] * lv509[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(10240)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul1_add(lv41: T.Buffer((T.int64(1), T.int64(1280)), "float32"), lv42: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_add_embedding_linear_2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv41[v_i0, v_k], lv42[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv41[v_i0, v_k] * lv42[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_add_embedding_linear_2_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_add_embedding_linear_2_bias[v_ax1]

    @T.prim_func(private=True)
    def fused_matmul1_add_add1(lv18: T.Buffer((T.int64(1), T.int64(1280)), "float32"), lv19: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_time_embedding_linear_2_bias: T.Buffer((T.int64(1280),), "float32"), lv44: T.Buffer((T.int64(1), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv18[v_i0, v_k], lv19[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv18[v_i0, v_k] * lv19[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_time_embedding_linear_2_bias[v_ax1])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1])
                var_T_add_intermediate_1[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_2_bias[v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1], lv44[v_ax0, v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_T_add_intermediate_1[v_ax0, v_ax1] + lv44[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def fused_matmul1_add_strided_slice7(lv431: T.Buffer((T.int64(1), T.int64(1280)), "float32"), lv432: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(1280),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv431[v_i0, v_k], lv432[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv431[v_i0, v_k] * lv432[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_down_blocks_2_resnets_0_time_emb_proj_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_down_blocks_2_resnets_0_time_emb_proj_bias[v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_strided_slice_with_axes"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1])
                T.writes(var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1] = var_T_add_intermediate[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def fused_matmul20_add15_add16(lv516: T.Buffer((T.int64(1), T.int64(256), T.int64(5120)), "float32"), lv517: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(1280),), "float32"), lv507: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(256), T.int64(1280), T.int64(5120)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv516[v_i0, v_i1, v_k], lv517[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv516[v_i0, v_i1, v_k] * lv517[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2], lv507[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] + lv507[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul21_add23(lv23: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv24: T.Buffer((T.int64(512), T.int64(512)), "float32"), vae_decoder_mid_block_attentions_0_to_q_bias: T.Buffer((T.int64(512),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(4096), T.int64(512), T.int64(512)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv23[v_i0, v_i1, v_k], lv24[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv23[v_i0, v_i1, v_k] * lv24[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], vae_decoder_mid_block_attentions_0_to_q_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + vae_decoder_mid_block_attentions_0_to_q_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul22_multiply11(lv34: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv41: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)))
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096), T.int64(512)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(lv34[v_i0, v_i1, v_i2, v_k], lv41[v_i0, v_i1, v_k, v_i3])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] + lv34[v_i0, v_i1, v_i2, v_k] * lv41[v_i0, v_i1, v_k, v_i3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], param_0[()])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * param_0[()]

    @T.prim_func(private=True)
    def fused_matmul24_add38(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv26: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(768), T.int64(768)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv21[v_i0, v_i1, v_k], lv26[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv21[v_i0, v_i1, v_k] * lv26[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul24_add38_add36(lv50: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv51: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(768),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(768), T.int64(768)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv50[v_i0, v_i1, v_k], lv51[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv50[v_i0, v_i1, v_k] * lv51[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv9[v_ax0, v_ax1, v_ax2], var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv9[v_ax0, v_ax1, v_ax2] + var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul24_add38_multiply16(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv22: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(768), T.int64(768)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv21[v_i0, v_i1, v_k], lv22[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv21[v_i0, v_i1, v_k] * lv22[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.125)

    @T.prim_func(private=True)
    def fused_matmul27_add40_multiply17_tir_sigmoid_multiply18(lv55: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv56: T.Buffer((T.int64(768), T.int64(3072)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc1_bias: T.Buffer((T.int64(3072),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        var_T_multiply_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        var_compute_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(3072), T.int64(768)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv55[v_i0, v_i1, v_k], lv56[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv55[v_i0, v_i1, v_k] * lv56[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(3072)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(3072)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = T.float32(1.7020000219345093) * var_T_add_intermediate[v_ax0, v_ax1, v_ax2]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(77), T.int64(3072)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(var_T_multiply_intermediate_1[v_i0, v_i1, v_i2])
                T.writes(var_compute_intermediate[v_i0, v_i1, v_i2])
                var_compute_intermediate[v_i0, v_i1, v_i2] = T.sigmoid(var_T_multiply_intermediate_1[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(3072)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2], var_compute_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2] * var_compute_intermediate[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul28_add38_add36(lv61: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32"), lv62: T.Buffer((T.int64(3072), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc2_bias: T.Buffer((T.int64(768),), "float32"), lv54: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(768), T.int64(3072)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv61[v_i0, v_i1, v_k], lv62[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv61[v_i0, v_i1, v_k] * lv62[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv54[v_ax0, v_ax1, v_ax2], var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv54[v_ax0, v_ax1, v_ax2] + var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul29_add42(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), lv26: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias: T.Buffer((T.int64(1280),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv21[v_i0, v_i1, v_k], lv26[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv21[v_i0, v_i1, v_k] * lv26[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul29_add42_add41(lv52: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), lv53: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(1280),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv52[v_i0, v_i1, v_k], lv53[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv52[v_i0, v_i1, v_k] * lv53[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv9[v_ax0, v_ax1, v_ax2], var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv9[v_ax0, v_ax1, v_ax2] + var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul29_add42_multiply19(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), lv22: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv21[v_i0, v_i1, v_k], lv22[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv21[v_i0, v_i1, v_k] * lv22[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.125)

    @T.prim_func(private=True)
    def fused_matmul2_add_silu(lv37: T.Buffer((T.int64(1), T.int64(2816)), "float32"), lv38: T.Buffer((T.int64(2816), T.int64(1280)), "float32"), unet_add_embedding_linear_1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(2816)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv37[v_i0, v_k], lv38[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv37[v_i0, v_k] * lv38[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_add_embedding_linear_1_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_add_embedding_linear_1_bias[v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_add_intermediate[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.sigmoid(var_T_add_intermediate[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_multiply"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1], compute[v_ax0, v_ax1])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                var_T_multiply_intermediate[v_ax0, v_ax1] = var_T_add_intermediate[v_ax0, v_ax1] * compute[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def fused_matmul32_add44_gelu2(lv57: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), lv58: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc1_bias: T.Buffer((T.int64(5120),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(5120)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        compute = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(5120)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(5120), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv57[v_i0, v_i1, v_k], lv58[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv57[v_i0, v_i1, v_k] * lv58[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(5120)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul33_add42_add41(lv61: T.Buffer((T.int64(1), T.int64(77), T.int64(5120)), "float32"), lv62: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc2_bias: T.Buffer((T.int64(1280),), "float32"), lv56: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(1280), T.int64(5120)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv61[v_i0, v_i1, v_k], lv62[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv61[v_i0, v_i1, v_k] * lv62[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv56[v_ax0, v_ax1, v_ax2], var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv56[v_ax0, v_ax1, v_ax2] + var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul3_add3_cast1(lv54: T.Buffer((T.int64(1), T.int64(1280)), "float32"), lv55: T.Buffer((T.int64(1280), T.int64(320)), "float32"), unet_down_blocks_0_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(320),), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(320)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(320)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(320)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(320), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv54[v_i0, v_k], lv55[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv54[v_i0, v_k] * lv55[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(320)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_down_blocks_0_resnets_0_time_emb_proj_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_down_blocks_0_resnets_0_time_emb_proj_bias[v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(320)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_add_intermediate[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = var_T_add_intermediate[v_i0, v_i1]

    @T.prim_func(private=True)
    def fused_matmul4_add7_strided_slice6(lv95: T.Buffer((T.int64(1), T.int64(1280)), "float32"), lv96: T.Buffer((T.int64(1280), T.int64(640)), "float32"), unet_down_blocks_1_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(640),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(1), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(640)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(640)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(640), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv95[v_i0, v_k], lv96[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv95[v_i0, v_k] * lv96[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(640)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_down_blocks_1_resnets_0_time_emb_proj_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_down_blocks_1_resnets_0_time_emb_proj_bias[v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(640)):
            with T.block("T_strided_slice_with_axes"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1])
                T.writes(var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1] = var_T_add_intermediate[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def fused_matmul5_add9(lv114: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv115: T.Buffer((T.int64(640), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_proj_in_bias: T.Buffer((T.int64(640),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1024), T.int64(640), T.int64(640)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv114[v_i0, v_i1, v_k], lv115[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv114[v_i0, v_i1, v_k] * lv115[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_proj_in_bias[v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_proj_in_bias[v_ax2]

    @T.prim_func(private=True)
    def fused_matmul5_add9_divide3_add10(lv139: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv140: T.Buffer((T.int64(640), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(640),), "float32"), lv117: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        var_T_add_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        var_T_divide_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(640)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1024), T.int64(640), T.int64(640)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv139[v_i0, v_i1, v_k], lv140[v_k, v_i2])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv139[v_i0, v_i1, v_k] * lv140[v_k, v_i2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v_ax2])
                T.writes(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2], lv117[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_divide_intermediate[v_ax0, v_ax1, v_ax2] + lv117[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_matmul6_multiply4(lv126: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32"), lv133: T.Buffer((T.int64(1), T.int64(10), T.int64(64), T.int64(1024)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)))
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(lv126[v_i0, v_i1, v_i2, v_k], lv133[v_i0, v_i1, v_k, v_i3])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] + lv126[v_i0, v_i1, v_i2, v_k] * lv133[v_i0, v_i1, v_k, v_i3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], param_0[()])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * param_0[()]

    @T.prim_func(private=True)
    def fused_matmul9_multiply5(lv153: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(10), T.int64(64), T.int64(77)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)))
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(lv153[v_i0, v_i1, v_i2, v_k], lv160[v_i0, v_i1, v_k, v_i3])
                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate[v_i0, v_i1, v_i2, v_i3] + lv153[v_i0, v_i1, v_i2, v_k] * lv160[v_i0, v_i1, v_k, v_i3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], param_0[()])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * param_0[()]

    @T.prim_func(private=True)
    def fused_matmul_add_silu(lv14: T.Buffer((T.int64(1), T.int64(320)), "float32"), lv15: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_time_embedding_linear_1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(320)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(lv14[v_i0, v_k], lv15[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv14[v_i0, v_k] * lv15[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_time_embedding_linear_1_bias[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_1_bias[v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_add_intermediate[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.sigmoid(var_T_add_intermediate[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_multiply"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1], compute[v_ax0, v_ax1])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                var_T_multiply_intermediate[v_ax0, v_ax1] = var_T_add_intermediate[v_ax0, v_ax1] * compute[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def fused_reshape11_transpose7(lv120: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(10), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1024), T.int64(10), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv120[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv120[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape11_transpose7_transpose8(lv122: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(64), T.int64(1024)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(10), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1024), T.int64(10), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv122[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv122[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(64), T.int64(1024)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape13_transpose11(lv151: T.Buffer((T.int64(1), T.int64(77), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(10), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(10), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv151[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv151[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape13_transpose11_transpose12(lv149: T.Buffer((T.int64(1), T.int64(77), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(10), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(10), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(10), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv149[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv149[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(64), T.int64(77)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape14_transpose15_add8(lv254: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv111: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(32), T.int64(640)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(32), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv254[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv254[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv111[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv111[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape14_transpose15_add8_concatenate7(lv4835: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv4692: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), lv257: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(32), T.int64(640)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(32), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv4835[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv4835[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4692[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4692[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv257[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, lv257[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def fused_reshape14_transpose15_add8_concatenate8(lv5004: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv4861: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), lv89: T.Buffer((T.int64(1), T.int64(320), T.int64(32), T.int64(32)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(1), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(32), T.int64(640)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(32), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv5004[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv5004[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4861[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4861[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(960), T.int64(32), T.int64(32)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv89[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, lv89[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def fused_reshape14_transpose15_add8_resize2d1(lv5173: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), lv5030: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_resize_intermediate: T.Buffer((T.int64(1), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(32), T.int64(640)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(32), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv5173[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv5173[T.int64(0), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv5030[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv5030[v_ax0, v_ax1, v_ax2, v_ax3]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(640), T.int64(64), T.int64(64)):
            with T.block("resize"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_add_intermediate[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))])
                T.writes(var_resize_intermediate[v_i0, v_i1, v_i2, v_i3])
                var_resize_intermediate[v_i0, v_i1, v_i2, v_i3] = var_T_add_intermediate[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))]

    @T.prim_func(private=True)
    def fused_reshape18_transpose17(lv456: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(20), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv456[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv456[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape18_transpose17_transpose18(lv458: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(64), T.int64(256)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(20), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv458[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv458[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(64), T.int64(256)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape20_transpose21(lv487: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(20), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv487[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv487[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape20_transpose21_reshape43(lv33: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(20), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(20), T.int64(64)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_reshape20_transpose21_reshape43_transpose39(lv28: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(20), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(20), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(20), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2 in T.grid(T.int64(20), T.int64(64), T.int64(77)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1]

    @T.prim_func(private=True)
    def fused_reshape20_transpose21_transpose22(lv485: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(20), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(20), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(20), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv485[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv485[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(64), T.int64(77)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape21_transpose25_add14(lv1126: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv447: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(1280)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv1126[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv1126[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv447[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv447[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape21_transpose25_add14_concatenate4(lv3252: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv2573: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), lv1129: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(1), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(1280)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv3252[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv3252[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv2573[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv2573[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2560), T.int64(16), T.int64(16)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv1129[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, lv1129[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def fused_reshape21_transpose25_add14_concatenate5(lv3957: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv3278: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), lv425: T.Buffer((T.int64(1), T.int64(640), T.int64(16), T.int64(16)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(1), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(1280)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv3957[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv3957[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3278[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv3278[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1920), T.int64(16), T.int64(16)):
            with T.block("T_concat"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv425[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_concat_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, lv425[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func(private=True)
    def fused_reshape21_transpose25_add14_resize2d(lv4662: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), lv3983: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_resize_intermediate: T.Buffer((T.int64(1), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(1280)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv4662[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv4662[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv3983[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv3983[v_ax0, v_ax1, v_ax2, v_ax3]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1280), T.int64(32), T.int64(32)):
            with T.block("resize"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_add_intermediate[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))])
                T.writes(var_resize_intermediate[v_i0, v_i1, v_i2, v_i3])
                var_resize_intermediate[v_i0, v_i1, v_i2, v_i3] = var_T_add_intermediate[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))]

    @T.prim_func(private=True)
    def fused_reshape24_transpose26_transpose27(lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(4096)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(512), T.int64(4096)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(512), T.int64(4096)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax2, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate_1[v_ax0, v_ax2, v_ax1]

    @T.prim_func(private=True)
    def fused_reshape25_transpose29(lv26: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(1), T.int64(512)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4096), T.int64(1), T.int64(512)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def fused_reshape25_transpose29_transpose30(lv29: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(1), T.int64(512)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4096), T.int64(1), T.int64(512)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(512), T.int64(4096)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax3, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape2_strided_slice3_reshape3_cast2_multiply2_multiply3_tir_sin1_tir_cos1_concatenate2_strided_slice4_reshape4_strided_slice5_reshape4_concatenate2_reshape5_concatenate3(inp_4: T.Buffer((T.int64(1), T.int64(6)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(128)), "float32"), inp_3: T.Buffer((T.int64(1), T.int64(1280)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(1), T.int64(2816)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(6),))
        var_T_strided_slice_with_axes_intermediate = T.alloc_buffer((T.int64(6),))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(6), T.int64(1)))
        var_compute_intermediate = T.alloc_buffer((T.int64(6), T.int64(1)))
        var_T_multiply_intermediate = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_multiply_intermediate_1 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_compute_intermediate_1 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_compute_intermediate_2 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_concat_intermediate_1 = T.alloc_buffer((T.int64(6), T.int64(256)))
        var_T_strided_slice_with_axes_intermediate_1 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_reshape_intermediate_2 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_strided_slice_with_axes_intermediate_2 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_reshape_intermediate_3 = T.alloc_buffer((T.int64(6), T.int64(128)))
        var_T_concat_intermediate_2 = T.alloc_buffer((T.int64(6), T.int64(256)))
        var_T_reshape_intermediate_4 = T.alloc_buffer((T.int64(1), T.int64(1536)))
        for ax0 in range(T.int64(6)):
            with T.block("T_reshape"):
                v_ax0 = T.axis.spatial(T.int64(6), ax0)
                T.reads(inp_4[T.int64(0), v_ax0 % T.int64(6)])
                T.writes(var_T_reshape_intermediate[v_ax0])
                var_T_reshape_intermediate[v_ax0] = inp_4[T.int64(0), v_ax0 % T.int64(6)]
        for ax0 in range(T.int64(6)):
            with T.block("T_strided_slice_with_axes"):
                v_ax0 = T.axis.spatial(T.int64(6), ax0)
                T.reads(var_T_reshape_intermediate[v_ax0])
                T.writes(var_T_strided_slice_with_axes_intermediate[v_ax0])
                var_T_strided_slice_with_axes_intermediate[v_ax0] = var_T_reshape_intermediate[v_ax0]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(1)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate[(v_ax0 + v_ax1) % T.int64(6)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1])
                var_T_reshape_intermediate_1[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate[(v_ax0 + v_ax1) % T.int64(6)]
        for i0, i1 in T.grid(T.int64(6), T.int64(1)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_reshape_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = var_T_reshape_intermediate_1[v_i0, v_i1]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_multiply"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_compute_intermediate[v_ax0, T.int64(0)], param_0[T.int64(0), v_ax1])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                var_T_multiply_intermediate[v_ax0, v_ax1] = var_compute_intermediate[v_ax0, T.int64(0)] * param_0[T.int64(0), v_ax1]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_multiply_intermediate[v_ax0, v_ax1])
                T.writes(var_T_multiply_intermediate_1[v_ax0, v_ax1])
                var_T_multiply_intermediate_1[v_ax0, v_ax1] = var_T_multiply_intermediate[v_ax0, v_ax1]
        for i0, i1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("compute_1"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_multiply_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate_1[v_i0, v_i1])
                var_compute_intermediate_1[v_i0, v_i1] = T.sin(var_T_multiply_intermediate_1[v_i0, v_i1])
        for i0, i1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("compute_2"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_multiply_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate_2[v_i0, v_i1])
                var_compute_intermediate_2[v_i0, v_i1] = T.cos(var_T_multiply_intermediate_1[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(6), T.int64(256)):
            with T.block("T_concat"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_compute_intermediate_2[v_ax0, v_ax1 - T.int64(128)], var_compute_intermediate_1[v_ax0, v_ax1])
                T.writes(var_T_concat_intermediate_1[v_ax0, v_ax1])
                var_T_concat_intermediate_1[v_ax0, v_ax1] = T.if_then_else(T.int64(128) <= v_ax1, var_compute_intermediate_2[v_ax0, v_ax1 - T.int64(128)], var_compute_intermediate_1[v_ax0, v_ax1])
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_strided_slice_with_axes_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_concat_intermediate_1[v_ax0, v_ax1 + T.int64(128)])
                T.writes(var_T_strided_slice_with_axes_intermediate_1[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate_1[v_ax0, v_ax1] = var_T_concat_intermediate_1[v_ax0, v_ax1 + T.int64(128)]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate_1[(v_ax1 // T.int64(128) + v_ax0) % T.int64(6), v_ax1 % T.int64(128)])
                T.writes(var_T_reshape_intermediate_2[v_ax0, v_ax1])
                var_T_reshape_intermediate_2[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate_1[(v_ax1 // T.int64(128) + v_ax0) % T.int64(6), v_ax1 % T.int64(128)]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_strided_slice_with_axes_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_concat_intermediate_1[v_ax0, v_ax1])
                T.writes(var_T_strided_slice_with_axes_intermediate_2[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate_2[v_ax0, v_ax1] = var_T_concat_intermediate_1[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(128)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate_2[(v_ax1 // T.int64(128) + v_ax0) % T.int64(6), v_ax1 % T.int64(128)])
                T.writes(var_T_reshape_intermediate_3[v_ax0, v_ax1])
                var_T_reshape_intermediate_3[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate_2[(v_ax1 // T.int64(128) + v_ax0) % T.int64(6), v_ax1 % T.int64(128)]
        for ax0, ax1 in T.grid(T.int64(6), T.int64(256)):
            with T.block("T_concat_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_reshape_intermediate_3[v_ax0, v_ax1 - T.int64(128)], var_T_reshape_intermediate_2[v_ax0, v_ax1])
                T.writes(var_T_concat_intermediate_2[v_ax0, v_ax1])
                var_T_concat_intermediate_2[v_ax0, v_ax1] = T.if_then_else(T.int64(128) <= v_ax1, var_T_reshape_intermediate_3[v_ax0, v_ax1 - T.int64(128)], var_T_reshape_intermediate_2[v_ax0, v_ax1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1536)):
            with T.block("T_reshape_4"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_concat_intermediate_2[v_ax1 % T.int64(1536) // T.int64(256), v_ax1 % T.int64(256)])
                T.writes(var_T_reshape_intermediate_4[v_ax0, v_ax1])
                var_T_reshape_intermediate_4[v_ax0, v_ax1] = var_T_concat_intermediate_2[v_ax1 % T.int64(1536) // T.int64(256), v_ax1 % T.int64(256)]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(2816)):
            with T.block("T_concat_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_reshape_intermediate_4[v_ax0, v_ax1 - T.int64(1280)], inp_3[v_ax0, v_ax1])
                T.writes(var_T_concat_intermediate[v_ax0, v_ax1])
                var_T_concat_intermediate[v_ax0, v_ax1] = T.if_then_else(T.int64(1280) <= v_ax1, var_T_reshape_intermediate_4[v_ax0, v_ax1 - T.int64(1280)], inp_3[v_ax0, v_ax1])

    @T.prim_func(private=True)
    def fused_reshape31_cast3_reshape32(inp_0: T.Buffer((T.int64(1), T.int64(77)), "int32"), var_T_reshape_intermediate: T.Buffer((T.int64(77),), "int32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77)), "int32")
        var_compute_intermediate = T.alloc_buffer((T.int64(1), T.int64(77)), "int32")
        for ax0, ax1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("T_reshape"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(inp_0[T.int64(0), v_ax1 % T.int64(77)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1])
                var_T_reshape_intermediate_1[v_ax0, v_ax1] = inp_0[T.int64(0), v_ax1 % T.int64(77)]
        for i0, i1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(var_T_reshape_intermediate_1[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = var_T_reshape_intermediate_1[v_i0, v_i1]
        for ax0 in range(T.int64(77)):
            with T.block("T_reshape_1"):
                v_ax0 = T.axis.spatial(T.int64(77), ax0)
                T.reads(var_compute_intermediate[T.int64(0), v_ax0 % T.int64(77)])
                T.writes(var_T_reshape_intermediate[v_ax0])
                var_T_reshape_intermediate[v_ax0] = var_compute_intermediate[T.int64(0), v_ax0 % T.int64(77)]

    @T.prim_func(private=True)
    def fused_reshape33_reshape33_add36(lv3: T.Buffer((T.int64(77), T.int64(768)), "float32"), lv7: T.Buffer((T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2], var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] + var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape36_transpose34_reshape37(lv33: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(12), T.int64(64)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(12), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(12), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(12), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(12), T.int64(77), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_reshape36_transpose34_reshape37_transpose35(lv28: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(12), T.int64(64)))
        var_T_transpose_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(12), T.int64(77), T.int64(64)))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(12), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(12), T.int64(77), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(12), T.int64(77), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(64) + v_ax1) % T.int64(77), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2 in T.grid(T.int64(12), T.int64(64), T.int64(77)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1]

    @T.prim_func(private=True)
    def fused_reshape38_add39_reshape39(lv42: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(1), T.int64(77), T.int64(77)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(12), T.int64(77), T.int64(77)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(12), T.int64(77), T.int64(77)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], param_0[v_ax0, T.int64(0), v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + param_0[v_ax0, T.int64(0), v_ax2, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(12), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)]

    @T.prim_func(private=True)
    def fused_reshape40_transpose36_reshape41(lv47: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(12), T.int64(77), T.int64(64)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(12), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(12), T.int64(77), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(12), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_reshape42_reshape42_add41(lv3: T.Buffer((T.int64(77), T.int64(1280)), "float32"), lv7: T.Buffer((T.int64(77), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(1280)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv3[(v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv7[(v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = lv7[(v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2], var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] + var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_reshape44_add43_reshape45(lv42: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(1), T.int64(77), T.int64(77)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(77)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(77)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3], param_0[v_ax0, T.int64(0), v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] + param_0[v_ax0, T.int64(0), v_ax2, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_add_intermediate[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_add_intermediate[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)]

    @T.prim_func(private=True)
    def fused_reshape44_reshape45(lv46: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(77)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv46[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv46[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)]
        for ax0, ax1, ax2 in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_reshape_intermediate_1[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_reshape_intermediate_1[T.int64(0), ((v_ax2 // T.int64(77) + v_ax1) // T.int64(77) + v_ax0) % T.int64(20), (v_ax2 // T.int64(77) + v_ax1) % T.int64(77), v_ax2 % T.int64(77)]

    @T.prim_func(private=True)
    def fused_reshape46_transpose40_reshape47(lv49: T.Buffer((T.int64(20), T.int64(77), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_reshape_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)))
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(20), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(20), T.int64(77), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv49[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)])
                T.writes(var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv49[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(20), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(77), T.int64(20), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate_1[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(77), v_ax2 % T.int64(1280) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_split1_gelu1_multiply9(lv511: T.Buffer((T.int64(1), T.int64(256), T.int64(10240)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_split_sections_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        var_T_split_sections_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        compute = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        var_T_multiply_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(5120)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_split_sections"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv511[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] = lv511[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_split_sections_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv511[v_ax0, v_ax1, v_ax2 + T.int64(5120)])
                T.writes(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] = lv511[v_ax0, v_ax1, v_ax2 + T.int64(5120)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(5120)):
            with T.block("T_multiply_3"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2], var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] * var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_split_gelu_multiply6(lv175: T.Buffer((T.int64(1), T.int64(1024), T.int64(5120)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_split_sections_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        var_T_split_sections_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        var_T_multiply_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(2560)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_split_sections"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv175[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] = lv175[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_split_sections_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv175[v_ax0, v_ax1, v_ax2 + T.int64(2560)])
                T.writes(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] = lv175[v_ax0, v_ax1, v_ax2 + T.int64(2560)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(2560)):
            with T.block("T_multiply_3"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2], var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = var_T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] * var_T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_strided_slice7_reshape48(lv1465: T.Buffer((T.int64(1), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_strided_slice_with_axes_intermediate = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_strided_slice_with_axes"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(lv1465[v_ax0, v_ax1])
                T.writes(var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1])
                var_T_strided_slice_with_axes_intermediate[v_ax0, v_ax1] = lv1465[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(var_T_strided_slice_with_axes_intermediate[T.int64(0), v_ax1 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1])
                var_T_reshape_intermediate[v_ax0, v_ax1] = var_T_strided_slice_with_axes_intermediate[T.int64(0), v_ax1 % T.int64(1280)]

    @T.prim_func(private=True)
    def fused_transpose16_reshape17(lv448: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(1280)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1280)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv448[v_ax0, v_ax3, v_ax1, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv448[v_ax0, v_ax3, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(256) // T.int64(16), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(16), v_ax2 % T.int64(1280)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(256) // T.int64(16), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(16), v_ax2 % T.int64(1280)]

    @T.prim_func(private=True)
    def fused_transpose19_reshape19(lv473: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(20), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(256), T.int64(20), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv473[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv473[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(256), v_ax2 % T.int64(1280) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(1280) + v_ax1) % T.int64(256), v_ax2 % T.int64(1280) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_transpose27_reshape27_add22_divide6(lv50: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(4096)))
        var_T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(512), T.int64(4096)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv50[v_ax0, v_ax2, v_ax1])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = lv50[v_ax0, v_ax2, v_ax1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(4096) + v_ax1) % T.int64(512), (v_ax2 * T.int64(64) + v_ax3) % T.int64(4096)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(4096) + v_ax1) % T.int64(512), (v_ax2 * T.int64(64) + v_ax3) % T.int64(4096)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv18[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv18[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(64), T.int64(64)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_divide_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def fused_transpose31_reshape26(lv45: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(1), T.int64(512)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4096), T.int64(1), T.int64(512)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv45[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv45[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(512) + v_ax1) % T.int64(4096), T.int64(0), v_ax2 % T.int64(512)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(512) + v_ax1) % T.int64(4096), T.int64(0), v_ax2 % T.int64(512)]

    @T.prim_func(private=True)
    def fused_transpose32_multiply12_tir_round(lv237: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)))
        var_T_multiply_intermediate = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(512), T.int64(3)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv237[v_ax0, v_ax3, v_ax1, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv237[v_ax0, v_ax3, v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(512), T.int64(3)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(255)
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(512), T.int64(3)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_multiply_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(var_compute_intermediate[v_i0, v_i1, v_i2, v_i3])
                var_compute_intermediate[v_i0, v_i1, v_i2, v_i3] = T.round(var_T_multiply_intermediate[v_i0, v_i1, v_i2, v_i3])

    @T.prim_func(private=True)
    def fused_transpose5_reshape10(lv112: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(32), T.int64(640)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(32), T.int64(640)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv112[v_ax0, v_ax3, v_ax1, v_ax2])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv112[v_ax0, v_ax3, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(640) + v_ax1) % T.int64(1024) // T.int64(32), (v_ax2 // T.int64(640) + v_ax1) % T.int64(32), v_ax2 % T.int64(640)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(640) + v_ax1) % T.int64(1024) // T.int64(32), (v_ax2 // T.int64(640) + v_ax1) % T.int64(32), v_ax2 % T.int64(640)]

    @T.prim_func(private=True)
    def fused_transpose9_reshape12(lv137: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_T_transpose_intermediate = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(10), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1024), T.int64(10), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv137[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv137[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(640) + v_ax1) % T.int64(1024), v_ax2 % T.int64(640) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2])
                var_T_reshape_intermediate[v_ax0, v_ax1, v_ax2] = var_T_transpose_intermediate[T.int64(0), (v_ax2 // T.int64(640) + v_ax1) % T.int64(1024), v_ax2 % T.int64(640) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def group_norm15(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), B: T.Buffer((T.int64(512),), "float32"), C: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_1 = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(4096)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_reshape_3 = T.alloc_buffer((T.int64(32), T.int64(16)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16), T.int64(4096)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(4096)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3] = A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)]
        for ax0, ax1, k2, k3 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(4096)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3 = T.axis.remap("SSRR", [ax0, ax1, k2, k3])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_k2, v_k3])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3] * T_reshape_1[v_ax0, v_ax1, v_k2, v_k3]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(B[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = B[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(16)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(C[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)])
                T.writes(T_reshape_3[v_ax0, v_ax1])
                T_reshape_3[v_ax0, v_ax1] = C[(v_ax0 * T.int64(16) + v_ax1) % T.int64(512)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(16), T.int64(4096)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_2[v_ax1, v_ax2], T_reshape_3[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3] = (T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * T_reshape_2[v_ax1, v_ax2] + T_reshape_3[v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(512), T.int64(4096)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_group_norm[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512) // T.int64(16), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(16), v_ax2 % T.int64(4096)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = T_group_norm[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512) // T.int64(16), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(16), v_ax2 % T.int64(4096)]

    @T.prim_func(private=True)
    def group_norm3(A: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_1 = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_reshape_3 = T.alloc_buffer((T.int64(32), T.int64(20)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(A[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = A[T.int64(0), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(B[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = B[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(20)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(C[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)])
                T.writes(T_reshape_3[v_ax0, v_ax1])
                T_reshape_3[v_ax0, v_ax1] = C[(v_ax0 * T.int64(20) + v_ax1) % T.int64(640)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(20), T.int64(32), T.int64(32)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_2[v_ax1, v_ax2], T_reshape_3[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(9.9999999999999995e-07)) * T_reshape_2[v_ax1, v_ax2] + T_reshape_3[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(32), T.int64(32)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)]

    @T.prim_func(private=True)
    def group_norm6(A: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_1 = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)))
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        T_reshape_2 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_reshape_3 = T.alloc_buffer((T.int64(32), T.int64(40)))
        T_group_norm = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)))
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(A[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = A[T.int64(0), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)]
        for ax0, ax1, k2, k3, k4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2, v_k3, v_k4 = T.axis.remap("SSRRR", [ax0, ax1, k2, k3, k4])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4] * T_reshape_1[v_ax0, v_ax1, v_k2, v_k3, v_k4]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(B[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_2[v_ax0, v_ax1])
                T_reshape_2[v_ax0, v_ax1] = B[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1 in T.grid(T.int64(32), T.int64(40)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(C[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)])
                T.writes(T_reshape_3[v_ax0, v_ax1])
                T_reshape_3[v_ax0, v_ax1] = C[(v_ax0 * T.int64(40) + v_ax1) % T.int64(1280)]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(40), T.int64(16), T.int64(16)):
            with T.block("T_group_norm"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], T_reshape_2[v_ax1, v_ax2], T_reshape_3[v_ax1, v_ax2])
                T.writes(T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T_group_norm[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = (T_reshape_1[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(9.9999999999999995e-07)) * T_reshape_2[v_ax1, v_ax2] + T_reshape_3[v_ax1, v_ax2]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(16), T.int64(16)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = T_group_norm[T.int64(0), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)]

    @T.prim_func(private=True)
    def layer_norm(A: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(1024)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(1024)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(A[v_ax0, v_ax1, v_k2])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1024), T.int64(640)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0015625000000000001) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func(private=True)
    def layer_norm1(A: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(256)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(256)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(A[v_ax0, v_ax1, v_k2])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(256), T.int64(1280)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func(private=True)
    def layer_norm2(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768),), "float32"), C: T.Buffer((T.int64(768),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(77)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(77)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(A[v_ax0, v_ax1, v_k2])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(768)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0013020833333333333) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func(private=True)
    def layer_norm3(A: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(77)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(77)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("A_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(A[v_ax0, v_ax1, v_k2])
                T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                    A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(77), T.int64(1280)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func(private=True)
    def matmul1(A: T.Buffer((T.int64(1), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, k in T.grid(T.int64(1), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(A[v_i0, v_k], B[v_k, v_i1])
                T.writes(matmul[v_i0, v_i1])
                with T.init():
                    matmul[v_i0, v_i1] = T.float32(0)
                matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + A[v_i0, v_k] * B[v_k, v_i1]

    @T.prim_func(private=True)
    def matmul10(A: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)), "float32"), B: T.Buffer((T.int64(1), T.int64(10), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(64), T.int64(77)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k], B[v_i0, v_i1, v_k, v_i3])
                T.writes(matmul[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    matmul[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                matmul[v_i0, v_i1, v_i2, v_i3] = matmul[v_i0, v_i1, v_i2, v_i3] + A[v_i0, v_i1, v_i2, v_k] * B[v_i0, v_i1, v_k, v_i3]

    @T.prim_func(private=True)
    def matmul13(A: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(256), T.int64(1280), T.int64(1280)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul15(A: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)), "float32"), B: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(64), T.int64(256)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k], B[v_i0, v_i1, v_k, v_i3])
                T.writes(matmul[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    matmul[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                matmul[v_i0, v_i1, v_i2, v_i3] = matmul[v_i0, v_i1, v_i2, v_i3] + A[v_i0, v_i1, v_i2, v_k] * B[v_i0, v_i1, v_k, v_i3]

    @T.prim_func(private=True)
    def matmul16(A: T.Buffer((T.int64(1), T.int64(77), T.int64(2048)), "float32"), B: T.Buffer((T.int64(2048), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(1280), T.int64(2048)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul18(A: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)), "float32"), B: T.Buffer((T.int64(1), T.int64(20), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(64), T.int64(77)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k], B[v_i0, v_i1, v_k, v_i3])
                T.writes(matmul[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    matmul[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                matmul[v_i0, v_i1, v_i2, v_i3] = matmul[v_i0, v_i1, v_i2, v_i3] + A[v_i0, v_i1, v_i2, v_k] * B[v_i0, v_i1, v_k, v_i3]

    @T.prim_func(private=True)
    def matmul23(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), B: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(512), T.int64(4096)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k], B[v_i0, v_i1, v_k, v_i3])
                T.writes(matmul[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    matmul[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                matmul[v_i0, v_i1, v_i2, v_i3] = matmul[v_i0, v_i1, v_i2, v_i3] + A[v_i0, v_i1, v_i2, v_k] * B[v_i0, v_i1, v_k, v_i3]

    @T.prim_func(private=True)
    def matmul25(A: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), B: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(12), T.int64(77), T.int64(77), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_i0, v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_i0, v_k, v_i2]

    @T.prim_func(private=True)
    def matmul26(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), B: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(12), T.int64(77), T.int64(64), T.int64(77)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_i0, v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_i0, v_k, v_i2]

    @T.prim_func(private=True)
    def matmul30(A: T.Buffer((T.int64(20), T.int64(77), T.int64(64)), "float32"), B: T.Buffer((T.int64(20), T.int64(64), T.int64(77)), "float32"), matmul: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(20), T.int64(77), T.int64(77), T.int64(64)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_i0, v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_i0, v_k, v_i2]

    @T.prim_func(private=True)
    def matmul31(A: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32"), B: T.Buffer((T.int64(20), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(20), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(20), T.int64(77), T.int64(64), T.int64(77)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_i0, v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_i0, v_k, v_i2]

    @T.prim_func(private=True)
    def matmul5(A: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1024), T.int64(640), T.int64(640)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul7(A: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)), "float32"), B: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(64), T.int64(1024)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_i3, v_k = T.axis.remap("SSSSR", [i0, i1, i2, i3, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k], B[v_i0, v_i1, v_k, v_i3])
                T.writes(matmul[v_i0, v_i1, v_i2, v_i3])
                with T.init():
                    matmul[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                matmul[v_i0, v_i1, v_i2, v_i3] = matmul[v_i0, v_i1, v_i2, v_i3] + A[v_i0, v_i1, v_i2, v_k] * B[v_i0, v_i1, v_k, v_i3]

    @T.prim_func(private=True)
    def matmul8(A: T.Buffer((T.int64(1), T.int64(77), T.int64(2048)), "float32"), B: T.Buffer((T.int64(2048), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(77), T.int64(640)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(77), T.int64(640), T.int64(2048)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_k], B[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + A[v_i0, v_i1, v_k] * B[v_k, v_i2]

    @T.prim_func(private=True)
    def multiply10(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(7.6775431632995605) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def multiply13(A: T.Buffer((), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[()], B[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = A[()] * B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def multiply14(A: T.Buffer((), "float32"), B: T.Buffer((), "float32"), T_multiply: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_multiply"):
            vi = T.axis.spatial(1, T.int64(0))
            T.reads(A[()], B[()])
            T.writes(T_multiply[()])
            T_multiply[()] = A[()] * B[()]

    @T.prim_func(private=True)
    def multiply15(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[()])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] * B[()]

    @T.prim_func(private=True)
    def power(A: T.Buffer((), "float32"), T_power: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_power"):
            vi = T.axis.spatial(T.int64(1), T.int64(0))
            T.reads(A[()])
            T.writes(T_power[()])
            T_power[()] = T.pow(A[()], T.float32(2))

    @T.prim_func(private=True)
    def power1(A: T.Buffer((), "float32"), T_power: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_power"):
            vi = T.axis.spatial(T.int64(1), T.int64(0))
            T.reads(A[()])
            T.writes(T_power[()])
            T_power[()] = T.pow(A[()], T.float32(0.5))

    @T.prim_func(private=True)
    def reshape16(A: T.Buffer((T.int64(1), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1280), T.int64(1), T.int64(1)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)]

    @T.prim_func(private=True)
    def reshape31(A: T.Buffer((T.int64(1), T.int64(77)), "int32"), T_reshape: T.Buffer((T.int64(1), T.int64(77)), "int32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(77)):
            with T.block("T_reshape"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[T.int64(0), v_ax1 % T.int64(77)])
                T.writes(T_reshape[v_ax0, v_ax1])
                T_reshape[v_ax0, v_ax1] = A[T.int64(0), v_ax1 % T.int64(77)]

    @T.prim_func(private=True)
    def reshape7(A: T.Buffer((T.int64(1), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(320), T.int64(1), T.int64(1)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)]

    @T.prim_func(private=True)
    def reshape9(A: T.Buffer((T.int64(1), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(640), T.int64(1), T.int64(1)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[T.int64(0), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)]

    @T.prim_func(private=True)
    def resize2d2(A: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(128), T.int64(128)):
            with T.block("resize"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))])
                T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))]

    @T.prim_func(private=True)
    def resize2d3(A: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(256), T.int64(256)):
            with T.block("resize"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))])
                T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))]

    @T.prim_func(private=True)
    def resize2d4(A: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), resize: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(512), T.int64(512)):
            with T.block("resize"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))])
                T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))]

    @T.prim_func(private=True)
    def silu(A: T.Buffer((T.int64(1), T.int64(1280)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for i0, i1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(A[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.sigmoid(A[v_i0, v_i1])
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_multiply"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[v_ax0, v_ax1], compute[v_ax0, v_ax1])
                T.writes(T_multiply[v_ax0, v_ax1])
                T_multiply[v_ax0, v_ax1] = A[v_ax0, v_ax1] * compute[v_ax0, v_ax1]

    @T.prim_func(private=True)
    def softmax(A: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024)))
        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)))
        T_softmax_expsum = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem[v_i0, v_i1, v_i2])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2, v_i3])
                T_softmax_exp[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem[v_i0, v_i1, v_i2])
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1, v_i2] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1, v_i2] = T_softmax_expsum[v_i0, v_i1, v_i2] + T_softmax_exp[v_i0, v_i1, v_i2, v_k]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(1024)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_i3], T_softmax_expsum[v_i0, v_i1, v_i2])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                T.block_attr({"axis": 3})
                T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T_softmax_exp[v_i0, v_i1, v_i2, v_i3] / T_softmax_expsum[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def softmax1(A: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024)))
        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024), T.int64(77)))
        T_softmax_expsum = T.alloc_buffer((T.int64(1), T.int64(10), T.int64(1024)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem[v_i0, v_i1, v_i2])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2, v_i3])
                T_softmax_exp[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem[v_i0, v_i1, v_i2])
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1, v_i2] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1, v_i2] = T_softmax_expsum[v_i0, v_i1, v_i2] + T_softmax_exp[v_i0, v_i1, v_i2, v_k]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(10), T.int64(1024), T.int64(77)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_i3], T_softmax_expsum[v_i0, v_i1, v_i2])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                T.block_attr({"axis": 3})
                T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T_softmax_exp[v_i0, v_i1, v_i2, v_i3] / T_softmax_expsum[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def softmax2(A: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256)))
        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(256)))
        T_softmax_expsum = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem[v_i0, v_i1, v_i2])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2, v_i3])
                T_softmax_exp[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem[v_i0, v_i1, v_i2])
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1, v_i2] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1, v_i2] = T_softmax_expsum[v_i0, v_i1, v_i2] + T_softmax_exp[v_i0, v_i1, v_i2, v_k]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(256)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_i3], T_softmax_expsum[v_i0, v_i1, v_i2])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                T.block_attr({"axis": 3})
                T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T_softmax_exp[v_i0, v_i1, v_i2, v_i3] / T_softmax_expsum[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def softmax3(A: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256)))
        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256), T.int64(77)))
        T_softmax_expsum = T.alloc_buffer((T.int64(1), T.int64(20), T.int64(256)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem[v_i0, v_i1, v_i2])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2, v_i3])
                T_softmax_exp[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem[v_i0, v_i1, v_i2])
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1, v_i2] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1, v_i2] = T_softmax_expsum[v_i0, v_i1, v_i2] + T_softmax_exp[v_i0, v_i1, v_i2, v_k]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(20), T.int64(256), T.int64(77)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_i3], T_softmax_expsum[v_i0, v_i1, v_i2])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                T.block_attr({"axis": 3})
                T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T_softmax_exp[v_i0, v_i1, v_i2, v_i3] / T_softmax_expsum[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def softmax4(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)))
        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)))
        T_softmax_expsum = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)))
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(A[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem[v_i0, v_i1, v_i2])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2, v_i3])
                T_softmax_exp[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem[v_i0, v_i1, v_i2])
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1, v_i2])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1, v_i2] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1, v_i2] = T_softmax_expsum[v_i0, v_i1, v_i2] + T_softmax_exp[v_i0, v_i1, v_i2, v_k]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2, v_i3], T_softmax_expsum[v_i0, v_i1, v_i2])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                T.block_attr({"axis": 3})
                T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T_softmax_exp[v_i0, v_i1, v_i2, v_i3] / T_softmax_expsum[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def softmax5(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(12), T.int64(77)))
        T_softmax_exp = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)))
        T_softmax_expsum = T.alloc_buffer((T.int64(12), T.int64(77)))
        for i0, i1, k in T.grid(T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(A[v_i0, v_i1, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1] = T.max(T_softmax_maxelem[v_i0, v_i1], A[v_i0, v_i1, v_k])
        for i0, i1, i2 in T.grid(T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem[v_i0, v_i1])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2])
                T_softmax_exp[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem[v_i0, v_i1])
        for i0, i1, k in T.grid(T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1] = T_softmax_expsum[v_i0, v_i1] + T_softmax_exp[v_i0, v_i1, v_k]
        for i0, i1, i2 in T.grid(T.int64(12), T.int64(77), T.int64(77)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2], T_softmax_expsum[v_i0, v_i1])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                T.block_attr({"axis": 2})
                T_softmax_norm[v_i0, v_i1, v_i2] = T_softmax_exp[v_i0, v_i1, v_i2] / T_softmax_expsum[v_i0, v_i1]

    @T.prim_func(private=True)
    def softmax6(A: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(20), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem = T.alloc_buffer((T.int64(20), T.int64(77)))
        T_softmax_exp = T.alloc_buffer((T.int64(20), T.int64(77), T.int64(77)))
        T_softmax_expsum = T.alloc_buffer((T.int64(20), T.int64(77)))
        for i0, i1, k in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(A[v_i0, v_i1, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[v_i0, v_i1] = T.max(T_softmax_maxelem[v_i0, v_i1], A[v_i0, v_i1, v_k])
        for i0, i1, i2 in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem[v_i0, v_i1])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2])
                T_softmax_exp[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem[v_i0, v_i1])
        for i0, i1, k in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1] = T.float32(0)
                T_softmax_expsum[v_i0, v_i1] = T_softmax_expsum[v_i0, v_i1] + T_softmax_exp[v_i0, v_i1, v_k]
        for i0, i1, i2 in T.grid(T.int64(20), T.int64(77), T.int64(77)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2], T_softmax_expsum[v_i0, v_i1])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                T.block_attr({"axis": 2})
                T_softmax_norm[v_i0, v_i1, v_i2] = T_softmax_exp[v_i0, v_i1, v_i2] / T_softmax_expsum[v_i0, v_i1]

    @T.prim_func(private=True)
    def squeeze(A: T.Buffer((T.int64(1), T.int64(77), T.int64(1280)), "float32"), T_squeeze: T.Buffer((T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(77), T.int64(1280)):
            with T.block("T_squeeze"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[T.int64(0), v_ax0, v_ax1])
                T.writes(T_squeeze[v_ax0, v_ax1])
                T_squeeze[v_ax0, v_ax1] = A[T.int64(0), v_ax0, v_ax1]

    @T.prim_func(private=True)
    def subtract(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(64), T.int64(64)):
            with T.block("T_subtract"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] - B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func(private=True)
    def subtract1(A: T.Buffer((), "float32"), B: T.Buffer((), "float32"), T_subtract: T.Buffer((), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        with T.block("T_subtract"):
            vi = T.axis.spatial(1, T.int64(0))
            T.reads(A[()], B[()])
            T.writes(T_subtract[()])
            T_subtract[()] = A[()] - B[()]

    @T.prim_func(private=True)
    def take(A: T.Buffer((T.int64(49408), T.int64(768)), "float32"), B: T.Buffer((T.int64(77),), "int32"), T_take: T.Buffer((T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"op_pattern": 8, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(77), T.int64(768)):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[B[v_ax0], v_ax1], B[v_ax0])
                T.writes(T_take[v_ax0, v_ax1])
                T_take[v_ax0, v_ax1] = A[B[v_ax0], v_ax1]

    @T.prim_func(private=True)
    def take2(A: T.Buffer((T.int64(49408), T.int64(1280)), "float32"), B: T.Buffer((T.int64(77),), "int32"), T_take: T.Buffer((T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 8, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(77), T.int64(1280)):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[B[v_ax0], v_ax1], B[v_ax0])
                T.writes(T_take[v_ax0, v_ax1])
                T_take[v_ax0, v_ax1] = A[B[v_ax0], v_ax1]

    @T.prim_func(private=True)
    def take4(A: T.Buffer((T.int64(77), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1),), "int64"), T_take: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"op_pattern": 8, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(A[B[v_ax0], v_ax1], B[v_ax0])
                T.writes(T_take[v_ax0, v_ax1])
                T_take[v_ax0, v_ax1] = A[B[v_ax0], v_ax1]

    @T.prim_func(private=True)
    def tir_image_to_rgba(A: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32"), image_to_rgba: T.Buffer((T.int64(512), T.int64(512)), "uint32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for y, x in T.grid(T.int64(512), T.int64(512)):
            with T.block("image_to_rgba"):
                v_y, v_x = T.axis.remap("SS", [y, x])
                T.reads(A[T.int64(0), v_y, v_x, T.int64(0):T.int64(3)])
                T.writes(image_to_rgba[v_y, v_x])
                image_to_rgba[v_y, v_x] = T.bitwise_or(T.bitwise_or(T.bitwise_or(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(0)]), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(1)]), T.uint32(8))), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(2)]), T.uint32(16))), T.uint32(4278190080))

    @T.prim_func(private=True)
    def transpose26(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4096), T.int64(512)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(A[v_ax0, v_ax2, v_ax1])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2])
                T_transpose[v_ax0, v_ax1, v_ax2] = A[v_ax0, v_ax2, v_ax1]

    @R.function
    def cat_latents(latents: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((2, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.concatenate12, (latents, latents), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
        return gv

    @R.function
    def clip(inp_0: R.Tensor((1, 77), dtype="int32"), model_params: R.Tuple(R.Tensor((49408, 768), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((77, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"))) -> R.Tuple(R.Tensor((1, 77, 768), dtype="float32"), R.Tensor((1, 77, 768), dtype="float32")):
        R.func_attr({"global_symbol": "subgraph_0", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv1280 = R.call_tir(cls.fused_reshape31_cast3_reshape32, (inp_0,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv2337: R.Tensor((49408, 768), dtype="float32") = model_params[0]
            lv3 = R.call_tir(cls.take, (lv2337, lv1280), out_sinfo=R.Tensor((77, 768), dtype="float32"))
            lv2338: R.Tensor((77, 768), dtype="float32") = model_params[123]
            lv1281 = R.call_tir(cls.fused_reshape33_reshape33_add36, (lv3, lv2338), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2339: R.Tensor((768,), dtype="float32") = model_params[2]
            lv2340: R.Tensor((768,), dtype="float32") = model_params[1]
            lv21 = R.call_tir(cls.layer_norm2, (lv1281, lv2339, lv2340), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2341: R.Tensor((768, 768), dtype="float32") = model_params[124]
            lv2342: R.Tensor((768,), dtype="float32") = model_params[9]
            lv1282 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv21, lv2341, lv2342), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2343: R.Tensor((768, 768), dtype="float32") = model_params[125]
            lv2344: R.Tensor((768,), dtype="float32") = model_params[7]
            lv1283 = R.call_tir(cls.fused_matmul24_add38, (lv21, lv2343, lv2344), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1284 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1283,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2345: R.Tensor((768, 768), dtype="float32") = model_params[126]
            lv2346: R.Tensor((768,), dtype="float32") = model_params[10]
            lv1285 = R.call_tir(cls.fused_matmul24_add38, (lv21, lv2345, lv2346), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1286 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1285,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1287 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1282,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv42 = R.call_tir(cls.matmul25, (lv1287, lv1284), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1288 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv42, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv46 = R.call_tir(cls.softmax5, (lv1288,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv47 = R.call_tir(cls.matmul26, (lv46, lv1286), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1289 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv47,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2347: R.Tensor((768, 768), dtype="float32") = model_params[127]
            lv2348: R.Tensor((768,), dtype="float32") = model_params[8]
            lv1290 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1289, lv2347, lv2348, lv1281), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2349: R.Tensor((768,), dtype="float32") = model_params[4]
            lv2350: R.Tensor((768,), dtype="float32") = model_params[3]
            lv55 = R.call_tir(cls.layer_norm2, (lv1290, lv2349, lv2350), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2351: R.Tensor((768, 3072), dtype="float32") = model_params[128]
            lv2352: R.Tensor((3072,), dtype="float32") = model_params[5]
            lv1291 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv55, lv2351, lv2352), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2353: R.Tensor((3072, 768), dtype="float32") = model_params[129]
            lv2354: R.Tensor((768,), dtype="float32") = model_params[6]
            lv1292 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1291, lv2353, lv2354, lv1290), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2355: R.Tensor((768,), dtype="float32") = model_params[32]
            lv2356: R.Tensor((768,), dtype="float32") = model_params[31]
            lv66 = R.call_tir(cls.layer_norm2, (lv1292, lv2355, lv2356), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2357: R.Tensor((768, 768), dtype="float32") = model_params[130]
            lv2358: R.Tensor((768,), dtype="float32") = model_params[39]
            lv1293 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv66, lv2357, lv2358), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2359: R.Tensor((768, 768), dtype="float32") = model_params[131]
            lv2360: R.Tensor((768,), dtype="float32") = model_params[37]
            lv1294 = R.call_tir(cls.fused_matmul24_add38, (lv66, lv2359, lv2360), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1295 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1294,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2361: R.Tensor((768, 768), dtype="float32") = model_params[132]
            lv2362: R.Tensor((768,), dtype="float32") = model_params[40]
            lv1296 = R.call_tir(cls.fused_matmul24_add38, (lv66, lv2361, lv2362), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1297 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1296,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1298 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1293,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv87 = R.call_tir(cls.matmul25, (lv1298, lv1295), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1299 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv87, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv91 = R.call_tir(cls.softmax5, (lv1299,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv92 = R.call_tir(cls.matmul26, (lv91, lv1297), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1300 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv92,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2363: R.Tensor((768, 768), dtype="float32") = model_params[133]
            lv2364: R.Tensor((768,), dtype="float32") = model_params[38]
            lv1301 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1300, lv2363, lv2364, lv1292), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2365: R.Tensor((768,), dtype="float32") = model_params[34]
            lv2366: R.Tensor((768,), dtype="float32") = model_params[33]
            lv100 = R.call_tir(cls.layer_norm2, (lv1301, lv2365, lv2366), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2367: R.Tensor((768, 3072), dtype="float32") = model_params[134]
            lv2368: R.Tensor((3072,), dtype="float32") = model_params[35]
            lv1302 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv100, lv2367, lv2368), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2369: R.Tensor((3072, 768), dtype="float32") = model_params[135]
            lv2370: R.Tensor((768,), dtype="float32") = model_params[36]
            lv1303 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1302, lv2369, lv2370, lv1301), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2371: R.Tensor((768,), dtype="float32") = model_params[42]
            lv2372: R.Tensor((768,), dtype="float32") = model_params[41]
            lv111 = R.call_tir(cls.layer_norm2, (lv1303, lv2371, lv2372), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2373: R.Tensor((768, 768), dtype="float32") = model_params[136]
            lv2374: R.Tensor((768,), dtype="float32") = model_params[49]
            lv1304 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv111, lv2373, lv2374), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2375: R.Tensor((768, 768), dtype="float32") = model_params[137]
            lv2376: R.Tensor((768,), dtype="float32") = model_params[47]
            lv1305 = R.call_tir(cls.fused_matmul24_add38, (lv111, lv2375, lv2376), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1306 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1305,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2377: R.Tensor((768, 768), dtype="float32") = model_params[138]
            lv2378: R.Tensor((768,), dtype="float32") = model_params[50]
            lv1307 = R.call_tir(cls.fused_matmul24_add38, (lv111, lv2377, lv2378), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1308 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1307,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1309 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1304,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv132 = R.call_tir(cls.matmul25, (lv1309, lv1306), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1310 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv132, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv136 = R.call_tir(cls.softmax5, (lv1310,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv137 = R.call_tir(cls.matmul26, (lv136, lv1308), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1311 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv137,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2379: R.Tensor((768, 768), dtype="float32") = model_params[139]
            lv2380: R.Tensor((768,), dtype="float32") = model_params[48]
            lv1312 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1311, lv2379, lv2380, lv1303), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2381: R.Tensor((768,), dtype="float32") = model_params[44]
            lv2382: R.Tensor((768,), dtype="float32") = model_params[43]
            lv145 = R.call_tir(cls.layer_norm2, (lv1312, lv2381, lv2382), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2383: R.Tensor((768, 3072), dtype="float32") = model_params[140]
            lv2384: R.Tensor((3072,), dtype="float32") = model_params[45]
            lv1313 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv145, lv2383, lv2384), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2385: R.Tensor((3072, 768), dtype="float32") = model_params[141]
            lv2386: R.Tensor((768,), dtype="float32") = model_params[46]
            lv1314 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1313, lv2385, lv2386, lv1312), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2387: R.Tensor((768,), dtype="float32") = model_params[52]
            lv2388: R.Tensor((768,), dtype="float32") = model_params[51]
            lv156 = R.call_tir(cls.layer_norm2, (lv1314, lv2387, lv2388), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2389: R.Tensor((768, 768), dtype="float32") = model_params[142]
            lv2390: R.Tensor((768,), dtype="float32") = model_params[59]
            lv1315 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv156, lv2389, lv2390), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2391: R.Tensor((768, 768), dtype="float32") = model_params[143]
            lv2392: R.Tensor((768,), dtype="float32") = model_params[57]
            lv1316 = R.call_tir(cls.fused_matmul24_add38, (lv156, lv2391, lv2392), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1317 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1316,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2393: R.Tensor((768, 768), dtype="float32") = model_params[144]
            lv2394: R.Tensor((768,), dtype="float32") = model_params[60]
            lv1318 = R.call_tir(cls.fused_matmul24_add38, (lv156, lv2393, lv2394), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1319 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1318,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1320 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1315,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv177 = R.call_tir(cls.matmul25, (lv1320, lv1317), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1321 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv177, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv181 = R.call_tir(cls.softmax5, (lv1321,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv182 = R.call_tir(cls.matmul26, (lv181, lv1319), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1322 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv182,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2395: R.Tensor((768, 768), dtype="float32") = model_params[145]
            lv2396: R.Tensor((768,), dtype="float32") = model_params[58]
            lv1323 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1322, lv2395, lv2396, lv1314), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2397: R.Tensor((768,), dtype="float32") = model_params[54]
            lv2398: R.Tensor((768,), dtype="float32") = model_params[53]
            lv190 = R.call_tir(cls.layer_norm2, (lv1323, lv2397, lv2398), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2399: R.Tensor((768, 3072), dtype="float32") = model_params[146]
            lv2400: R.Tensor((3072,), dtype="float32") = model_params[55]
            lv1324 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv190, lv2399, lv2400), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2401: R.Tensor((3072, 768), dtype="float32") = model_params[147]
            lv2402: R.Tensor((768,), dtype="float32") = model_params[56]
            lv1325 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1324, lv2401, lv2402, lv1323), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2403: R.Tensor((768,), dtype="float32") = model_params[62]
            lv2404: R.Tensor((768,), dtype="float32") = model_params[61]
            lv201 = R.call_tir(cls.layer_norm2, (lv1325, lv2403, lv2404), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2405: R.Tensor((768, 768), dtype="float32") = model_params[148]
            lv2406: R.Tensor((768,), dtype="float32") = model_params[69]
            lv1326 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv201, lv2405, lv2406), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2407: R.Tensor((768, 768), dtype="float32") = model_params[149]
            lv2408: R.Tensor((768,), dtype="float32") = model_params[67]
            lv1327 = R.call_tir(cls.fused_matmul24_add38, (lv201, lv2407, lv2408), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1328 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1327,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2409: R.Tensor((768, 768), dtype="float32") = model_params[150]
            lv2410: R.Tensor((768,), dtype="float32") = model_params[70]
            lv1329 = R.call_tir(cls.fused_matmul24_add38, (lv201, lv2409, lv2410), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1330 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1329,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1331 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1326,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv222 = R.call_tir(cls.matmul25, (lv1331, lv1328), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1332 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv222, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv226 = R.call_tir(cls.softmax5, (lv1332,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv227 = R.call_tir(cls.matmul26, (lv226, lv1330), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1333 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv227,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2411: R.Tensor((768, 768), dtype="float32") = model_params[151]
            lv2412: R.Tensor((768,), dtype="float32") = model_params[68]
            lv1334 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1333, lv2411, lv2412, lv1325), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2413: R.Tensor((768,), dtype="float32") = model_params[64]
            lv2414: R.Tensor((768,), dtype="float32") = model_params[63]
            lv235 = R.call_tir(cls.layer_norm2, (lv1334, lv2413, lv2414), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2415: R.Tensor((768, 3072), dtype="float32") = model_params[152]
            lv2416: R.Tensor((3072,), dtype="float32") = model_params[65]
            lv1335 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv235, lv2415, lv2416), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2417: R.Tensor((3072, 768), dtype="float32") = model_params[153]
            lv2418: R.Tensor((768,), dtype="float32") = model_params[66]
            lv1336 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1335, lv2417, lv2418, lv1334), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2419: R.Tensor((768,), dtype="float32") = model_params[72]
            lv2420: R.Tensor((768,), dtype="float32") = model_params[71]
            lv246 = R.call_tir(cls.layer_norm2, (lv1336, lv2419, lv2420), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2421: R.Tensor((768, 768), dtype="float32") = model_params[154]
            lv2422: R.Tensor((768,), dtype="float32") = model_params[79]
            lv1337 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv246, lv2421, lv2422), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2423: R.Tensor((768, 768), dtype="float32") = model_params[155]
            lv2424: R.Tensor((768,), dtype="float32") = model_params[77]
            lv1338 = R.call_tir(cls.fused_matmul24_add38, (lv246, lv2423, lv2424), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1339 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1338,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2425: R.Tensor((768, 768), dtype="float32") = model_params[156]
            lv2426: R.Tensor((768,), dtype="float32") = model_params[80]
            lv1340 = R.call_tir(cls.fused_matmul24_add38, (lv246, lv2425, lv2426), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1341 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1340,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1342 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1337,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv267 = R.call_tir(cls.matmul25, (lv1342, lv1339), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1343 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv267, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv271 = R.call_tir(cls.softmax5, (lv1343,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv272 = R.call_tir(cls.matmul26, (lv271, lv1341), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1344 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv272,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2427: R.Tensor((768, 768), dtype="float32") = model_params[157]
            lv2428: R.Tensor((768,), dtype="float32") = model_params[78]
            lv1345 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1344, lv2427, lv2428, lv1336), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2429: R.Tensor((768,), dtype="float32") = model_params[74]
            lv2430: R.Tensor((768,), dtype="float32") = model_params[73]
            lv280 = R.call_tir(cls.layer_norm2, (lv1345, lv2429, lv2430), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2431: R.Tensor((768, 3072), dtype="float32") = model_params[158]
            lv2432: R.Tensor((3072,), dtype="float32") = model_params[75]
            lv1346 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv280, lv2431, lv2432), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2433: R.Tensor((3072, 768), dtype="float32") = model_params[159]
            lv2434: R.Tensor((768,), dtype="float32") = model_params[76]
            lv1347 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1346, lv2433, lv2434, lv1345), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2435: R.Tensor((768,), dtype="float32") = model_params[82]
            lv2436: R.Tensor((768,), dtype="float32") = model_params[81]
            lv291 = R.call_tir(cls.layer_norm2, (lv1347, lv2435, lv2436), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2437: R.Tensor((768, 768), dtype="float32") = model_params[160]
            lv2438: R.Tensor((768,), dtype="float32") = model_params[89]
            lv1348 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv291, lv2437, lv2438), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2439: R.Tensor((768, 768), dtype="float32") = model_params[161]
            lv2440: R.Tensor((768,), dtype="float32") = model_params[87]
            lv1349 = R.call_tir(cls.fused_matmul24_add38, (lv291, lv2439, lv2440), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1350 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1349,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2441: R.Tensor((768, 768), dtype="float32") = model_params[162]
            lv2442: R.Tensor((768,), dtype="float32") = model_params[90]
            lv1351 = R.call_tir(cls.fused_matmul24_add38, (lv291, lv2441, lv2442), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1352 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1351,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1353 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1348,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv312 = R.call_tir(cls.matmul25, (lv1353, lv1350), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1354 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv312, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv316 = R.call_tir(cls.softmax5, (lv1354,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv317 = R.call_tir(cls.matmul26, (lv316, lv1352), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1355 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv317,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2443: R.Tensor((768, 768), dtype="float32") = model_params[163]
            lv2444: R.Tensor((768,), dtype="float32") = model_params[88]
            lv1356 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1355, lv2443, lv2444, lv1347), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2445: R.Tensor((768,), dtype="float32") = model_params[84]
            lv2446: R.Tensor((768,), dtype="float32") = model_params[83]
            lv325 = R.call_tir(cls.layer_norm2, (lv1356, lv2445, lv2446), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2447: R.Tensor((768, 3072), dtype="float32") = model_params[164]
            lv2448: R.Tensor((3072,), dtype="float32") = model_params[85]
            lv1357 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv325, lv2447, lv2448), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2449: R.Tensor((3072, 768), dtype="float32") = model_params[165]
            lv2450: R.Tensor((768,), dtype="float32") = model_params[86]
            lv1358 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1357, lv2449, lv2450, lv1356), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2451: R.Tensor((768,), dtype="float32") = model_params[92]
            lv2452: R.Tensor((768,), dtype="float32") = model_params[91]
            lv336 = R.call_tir(cls.layer_norm2, (lv1358, lv2451, lv2452), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2453: R.Tensor((768, 768), dtype="float32") = model_params[166]
            lv2454: R.Tensor((768,), dtype="float32") = model_params[99]
            lv1359 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv336, lv2453, lv2454), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2455: R.Tensor((768, 768), dtype="float32") = model_params[167]
            lv2456: R.Tensor((768,), dtype="float32") = model_params[97]
            lv1360 = R.call_tir(cls.fused_matmul24_add38, (lv336, lv2455, lv2456), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1361 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1360,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2457: R.Tensor((768, 768), dtype="float32") = model_params[168]
            lv2458: R.Tensor((768,), dtype="float32") = model_params[100]
            lv1362 = R.call_tir(cls.fused_matmul24_add38, (lv336, lv2457, lv2458), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1363 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1362,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1364 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1359,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv357 = R.call_tir(cls.matmul25, (lv1364, lv1361), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1365 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv357, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv361 = R.call_tir(cls.softmax5, (lv1365,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv362 = R.call_tir(cls.matmul26, (lv361, lv1363), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1366 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv362,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2459: R.Tensor((768, 768), dtype="float32") = model_params[169]
            lv2460: R.Tensor((768,), dtype="float32") = model_params[98]
            lv1367 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1366, lv2459, lv2460, lv1358), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2461: R.Tensor((768,), dtype="float32") = model_params[94]
            lv2462: R.Tensor((768,), dtype="float32") = model_params[93]
            lv370 = R.call_tir(cls.layer_norm2, (lv1367, lv2461, lv2462), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2463: R.Tensor((768, 3072), dtype="float32") = model_params[170]
            lv2464: R.Tensor((3072,), dtype="float32") = model_params[95]
            lv1368 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv370, lv2463, lv2464), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2465: R.Tensor((3072, 768), dtype="float32") = model_params[171]
            lv2466: R.Tensor((768,), dtype="float32") = model_params[96]
            lv1369 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1368, lv2465, lv2466, lv1367), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2467: R.Tensor((768,), dtype="float32") = model_params[102]
            lv2468: R.Tensor((768,), dtype="float32") = model_params[101]
            lv381 = R.call_tir(cls.layer_norm2, (lv1369, lv2467, lv2468), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2469: R.Tensor((768, 768), dtype="float32") = model_params[172]
            lv2470: R.Tensor((768,), dtype="float32") = model_params[109]
            lv1370 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv381, lv2469, lv2470), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2471: R.Tensor((768, 768), dtype="float32") = model_params[173]
            lv2472: R.Tensor((768,), dtype="float32") = model_params[107]
            lv1371 = R.call_tir(cls.fused_matmul24_add38, (lv381, lv2471, lv2472), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1372 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1371,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2473: R.Tensor((768, 768), dtype="float32") = model_params[174]
            lv2474: R.Tensor((768,), dtype="float32") = model_params[110]
            lv1373 = R.call_tir(cls.fused_matmul24_add38, (lv381, lv2473, lv2474), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1374 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1373,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1375 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1370,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv402 = R.call_tir(cls.matmul25, (lv1375, lv1372), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1376 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv402, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv406 = R.call_tir(cls.softmax5, (lv1376,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv407 = R.call_tir(cls.matmul26, (lv406, lv1374), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1377 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv407,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2475: R.Tensor((768, 768), dtype="float32") = model_params[175]
            lv2476: R.Tensor((768,), dtype="float32") = model_params[108]
            lv1378 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1377, lv2475, lv2476, lv1369), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2477: R.Tensor((768,), dtype="float32") = model_params[104]
            lv2478: R.Tensor((768,), dtype="float32") = model_params[103]
            lv415 = R.call_tir(cls.layer_norm2, (lv1378, lv2477, lv2478), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2479: R.Tensor((768, 3072), dtype="float32") = model_params[176]
            lv2480: R.Tensor((3072,), dtype="float32") = model_params[105]
            lv1379 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv415, lv2479, lv2480), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2481: R.Tensor((3072, 768), dtype="float32") = model_params[177]
            lv2482: R.Tensor((768,), dtype="float32") = model_params[106]
            lv1380 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1379, lv2481, lv2482, lv1378), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2483: R.Tensor((768,), dtype="float32") = model_params[112]
            lv2484: R.Tensor((768,), dtype="float32") = model_params[111]
            lv426 = R.call_tir(cls.layer_norm2, (lv1380, lv2483, lv2484), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2485: R.Tensor((768, 768), dtype="float32") = model_params[178]
            lv2486: R.Tensor((768,), dtype="float32") = model_params[119]
            lv1381 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv426, lv2485, lv2486), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2487: R.Tensor((768, 768), dtype="float32") = model_params[179]
            lv2488: R.Tensor((768,), dtype="float32") = model_params[117]
            lv1382 = R.call_tir(cls.fused_matmul24_add38, (lv426, lv2487, lv2488), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1383 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1382,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2489: R.Tensor((768, 768), dtype="float32") = model_params[180]
            lv2490: R.Tensor((768,), dtype="float32") = model_params[120]
            lv1384 = R.call_tir(cls.fused_matmul24_add38, (lv426, lv2489, lv2490), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1385 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1384,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1386 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1381,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv447 = R.call_tir(cls.matmul25, (lv1386, lv1383), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1387 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv447, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv451 = R.call_tir(cls.softmax5, (lv1387,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv452 = R.call_tir(cls.matmul26, (lv451, lv1385), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1388 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv452,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2491: R.Tensor((768, 768), dtype="float32") = model_params[181]
            lv2492: R.Tensor((768,), dtype="float32") = model_params[118]
            lv1389 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1388, lv2491, lv2492, lv1380), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2493: R.Tensor((768,), dtype="float32") = model_params[114]
            lv2494: R.Tensor((768,), dtype="float32") = model_params[113]
            lv460 = R.call_tir(cls.layer_norm2, (lv1389, lv2493, lv2494), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2495: R.Tensor((768, 3072), dtype="float32") = model_params[182]
            lv2496: R.Tensor((3072,), dtype="float32") = model_params[115]
            lv1390 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv460, lv2495, lv2496), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2497: R.Tensor((3072, 768), dtype="float32") = model_params[183]
            lv2498: R.Tensor((768,), dtype="float32") = model_params[116]
            lv1391 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1390, lv2497, lv2498, lv1389), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2499: R.Tensor((768,), dtype="float32") = model_params[12]
            lv2500: R.Tensor((768,), dtype="float32") = model_params[11]
            lv471 = R.call_tir(cls.layer_norm2, (lv1391, lv2499, lv2500), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2501: R.Tensor((768, 768), dtype="float32") = model_params[184]
            lv2502: R.Tensor((768,), dtype="float32") = model_params[19]
            lv1392 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv471, lv2501, lv2502), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2503: R.Tensor((768, 768), dtype="float32") = model_params[185]
            lv2504: R.Tensor((768,), dtype="float32") = model_params[17]
            lv1393 = R.call_tir(cls.fused_matmul24_add38, (lv471, lv2503, lv2504), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1394 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1393,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2505: R.Tensor((768, 768), dtype="float32") = model_params[186]
            lv2506: R.Tensor((768,), dtype="float32") = model_params[20]
            lv1395 = R.call_tir(cls.fused_matmul24_add38, (lv471, lv2505, lv2506), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1396 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1395,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1397 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1392,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv492 = R.call_tir(cls.matmul25, (lv1397, lv1394), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1398 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv492, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv496 = R.call_tir(cls.softmax5, (lv1398,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv497 = R.call_tir(cls.matmul26, (lv496, lv1396), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1399 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv497,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2507: R.Tensor((768, 768), dtype="float32") = model_params[187]
            lv2508: R.Tensor((768,), dtype="float32") = model_params[18]
            lv1400 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1399, lv2507, lv2508, lv1391), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2509: R.Tensor((768,), dtype="float32") = model_params[14]
            lv2510: R.Tensor((768,), dtype="float32") = model_params[13]
            lv505 = R.call_tir(cls.layer_norm2, (lv1400, lv2509, lv2510), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2511: R.Tensor((768, 3072), dtype="float32") = model_params[188]
            lv2512: R.Tensor((3072,), dtype="float32") = model_params[15]
            lv1401 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv505, lv2511, lv2512), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2513: R.Tensor((3072, 768), dtype="float32") = model_params[189]
            lv2514: R.Tensor((768,), dtype="float32") = model_params[16]
            lv1402 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1401, lv2513, lv2514, lv1400), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2515: R.Tensor((768,), dtype="float32") = model_params[22]
            lv2516: R.Tensor((768,), dtype="float32") = model_params[21]
            lv516 = R.call_tir(cls.layer_norm2, (lv1402, lv2515, lv2516), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2517: R.Tensor((768, 768), dtype="float32") = model_params[190]
            lv2518: R.Tensor((768,), dtype="float32") = model_params[29]
            lv1403 = R.call_tir(cls.fused_matmul24_add38_multiply16, (lv516, lv2517, lv2518), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2519: R.Tensor((768, 768), dtype="float32") = model_params[191]
            lv2520: R.Tensor((768,), dtype="float32") = model_params[27]
            lv1404 = R.call_tir(cls.fused_matmul24_add38, (lv516, lv2519, lv2520), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1405 = R.call_tir(cls.fused_reshape36_transpose34_reshape37_transpose35, (lv1404,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv2521: R.Tensor((768, 768), dtype="float32") = model_params[192]
            lv2522: R.Tensor((768,), dtype="float32") = model_params[30]
            lv1406 = R.call_tir(cls.fused_matmul24_add38, (lv516, lv2521, lv2522), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv1407 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1406,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1408 = R.call_tir(cls.fused_reshape36_transpose34_reshape37, (lv1403,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv537 = R.call_tir(cls.matmul25, (lv1408, lv1405), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv1409 = R.call_tir(cls.fused_reshape38_add39_reshape39, (lv537, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv541 = R.call_tir(cls.softmax5, (lv1409,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv542 = R.call_tir(cls.matmul26, (lv541, lv1407), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv1410 = R.call_tir(cls.fused_reshape40_transpose36_reshape41, (lv542,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2523: R.Tensor((768, 768), dtype="float32") = model_params[193]
            lv2524: R.Tensor((768,), dtype="float32") = model_params[28]
            lv1411 = R.call_tir(cls.fused_matmul24_add38_add36, (lv1410, lv2523, lv2524, lv1402), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2525: R.Tensor((768,), dtype="float32") = model_params[24]
            lv2526: R.Tensor((768,), dtype="float32") = model_params[23]
            lv550 = R.call_tir(cls.layer_norm2, (lv1411, lv2525, lv2526), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2527: R.Tensor((768, 3072), dtype="float32") = model_params[194]
            lv2528: R.Tensor((3072,), dtype="float32") = model_params[25]
            lv1412 = R.call_tir(cls.fused_matmul27_add40_multiply17_tir_sigmoid_multiply18, (lv550, lv2527, lv2528), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv2529: R.Tensor((3072, 768), dtype="float32") = model_params[195]
            lv2530: R.Tensor((768,), dtype="float32") = model_params[26]
            lv1413 = R.call_tir(cls.fused_matmul28_add38_add36, (lv1412, lv2529, lv2530, lv1411), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2531: R.Tensor((768,), dtype="float32") = model_params[122]
            lv2532: R.Tensor((768,), dtype="float32") = model_params[121]
            lv561 = R.call_tir(cls.layer_norm2, (lv1413, lv2531, lv2532), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 77, 768), dtype="float32"), R.Tensor((1, 77, 768), dtype="float32")) = lv1402, lv561
            R.output(gv)
        return gv

    @R.function
    def clip2(inp_0: R.Tensor((1, 77), dtype="int32"), model_params: R.Tuple(R.Tensor((49408, 1280), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((77, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"))) -> R.Tuple(R.Tensor((1, 77, 1280), dtype="float32"), R.Tensor((1, 1280), dtype="float32")):
        R.func_attr({"global_symbol": "subgraph_0", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.reshape31, (inp_0,), out_sinfo=R.Tensor((1, 77), dtype="int32"))
            lv1414 = R.call_tir(cls.fused_cast3_reshape32, (lv,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv_1: R.Tensor((49408, 1280), dtype="float32") = model_params[0]
            lv3 = R.call_tir(cls.take2, (lv_1, lv1414), out_sinfo=R.Tensor((77, 1280), dtype="float32"))
            lv1: R.Tensor((77, 1280), dtype="float32") = model_params[323]
            lv1415 = R.call_tir(cls.fused_reshape42_reshape42_add41, (lv3, lv1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv2: R.Tensor((1280,), dtype="float32") = model_params[2]
            lv3_1: R.Tensor((1280,), dtype="float32") = model_params[1]
            lv21 = R.call_tir(cls.layer_norm3, (lv1415, lv2, lv3_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv4: R.Tensor((1280, 1280), dtype="float32") = model_params[324]
            lv5: R.Tensor((1280,), dtype="float32") = model_params[9]
            lv1416 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv21, lv4, lv5), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv6: R.Tensor((1280, 1280), dtype="float32") = model_params[325]
            lv7: R.Tensor((1280,), dtype="float32") = model_params[7]
            lv1417 = R.call_tir(cls.fused_matmul29_add42, (lv21, lv6, lv7), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1418 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1417,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv8: R.Tensor((1280, 1280), dtype="float32") = model_params[326]
            lv9: R.Tensor((1280,), dtype="float32") = model_params[10]
            lv1419 = R.call_tir(cls.fused_matmul29_add42, (lv21, lv8, lv9), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1420 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1419,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1421 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1416,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv42 = R.call_tir(cls.matmul30, (lv1421, lv1418), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1422 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv42, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv46 = R.call_tir(cls.softmax6, (lv1422,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1423 = R.call_tir(cls.fused_reshape44_reshape45, (lv46,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv49 = R.call_tir(cls.matmul31, (lv1423, lv1420), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1424 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv49,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv10: R.Tensor((1280, 1280), dtype="float32") = model_params[327]
            lv11: R.Tensor((1280,), dtype="float32") = model_params[8]
            lv1425 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1424, lv10, lv11, lv1415), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv12: R.Tensor((1280,), dtype="float32") = model_params[4]
            lv13: R.Tensor((1280,), dtype="float32") = model_params[3]
            lv57 = R.call_tir(cls.layer_norm3, (lv1425, lv12, lv13), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv14: R.Tensor((1280, 5120), dtype="float32") = model_params[328]
            lv15: R.Tensor((5120,), dtype="float32") = model_params[5]
            lv1426 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv57, lv14, lv15), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv16: R.Tensor((5120, 1280), dtype="float32") = model_params[329]
            lv17: R.Tensor((1280,), dtype="float32") = model_params[6]
            lv1427 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1426, lv16, lv17, lv1425), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv18: R.Tensor((1280,), dtype="float32") = model_params[112]
            lv19: R.Tensor((1280,), dtype="float32") = model_params[111]
            lv66 = R.call_tir(cls.layer_norm3, (lv1427, lv18, lv19), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv20: R.Tensor((1280, 1280), dtype="float32") = model_params[330]
            lv21_1: R.Tensor((1280,), dtype="float32") = model_params[119]
            lv1428 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv66, lv20, lv21_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv22: R.Tensor((1280, 1280), dtype="float32") = model_params[331]
            lv23: R.Tensor((1280,), dtype="float32") = model_params[117]
            lv1429 = R.call_tir(cls.fused_matmul29_add42, (lv66, lv22, lv23), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1430 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1429,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv24: R.Tensor((1280, 1280), dtype="float32") = model_params[332]
            lv25: R.Tensor((1280,), dtype="float32") = model_params[120]
            lv1431 = R.call_tir(cls.fused_matmul29_add42, (lv66, lv24, lv25), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1432 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1431,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1433 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1428,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv87 = R.call_tir(cls.matmul30, (lv1433, lv1430), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1434 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv87, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv91 = R.call_tir(cls.softmax6, (lv1434,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1435 = R.call_tir(cls.fused_reshape44_reshape45, (lv91,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv94 = R.call_tir(cls.matmul31, (lv1435, lv1432), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1436 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv94,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv26: R.Tensor((1280, 1280), dtype="float32") = model_params[333]
            lv27: R.Tensor((1280,), dtype="float32") = model_params[118]
            lv1437 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1436, lv26, lv27, lv1427), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv28: R.Tensor((1280,), dtype="float32") = model_params[114]
            lv29: R.Tensor((1280,), dtype="float32") = model_params[113]
            lv102 = R.call_tir(cls.layer_norm3, (lv1437, lv28, lv29), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv30: R.Tensor((1280, 5120), dtype="float32") = model_params[334]
            lv31: R.Tensor((5120,), dtype="float32") = model_params[115]
            lv1438 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv102, lv30, lv31), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv32: R.Tensor((5120, 1280), dtype="float32") = model_params[335]
            lv33: R.Tensor((1280,), dtype="float32") = model_params[116]
            lv1439 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1438, lv32, lv33, lv1437), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv34: R.Tensor((1280,), dtype="float32") = model_params[222]
            lv35: R.Tensor((1280,), dtype="float32") = model_params[221]
            lv111 = R.call_tir(cls.layer_norm3, (lv1439, lv34, lv35), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv36: R.Tensor((1280, 1280), dtype="float32") = model_params[336]
            lv37: R.Tensor((1280,), dtype="float32") = model_params[229]
            lv1440 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv111, lv36, lv37), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv38: R.Tensor((1280, 1280), dtype="float32") = model_params[337]
            lv39: R.Tensor((1280,), dtype="float32") = model_params[227]
            lv1441 = R.call_tir(cls.fused_matmul29_add42, (lv111, lv38, lv39), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1442 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1441,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv40: R.Tensor((1280, 1280), dtype="float32") = model_params[338]
            lv41: R.Tensor((1280,), dtype="float32") = model_params[230]
            lv1443 = R.call_tir(cls.fused_matmul29_add42, (lv111, lv40, lv41), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1444 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1443,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1445 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1440,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv132 = R.call_tir(cls.matmul30, (lv1445, lv1442), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1446 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv132, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv136 = R.call_tir(cls.softmax6, (lv1446,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1447 = R.call_tir(cls.fused_reshape44_reshape45, (lv136,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv139 = R.call_tir(cls.matmul31, (lv1447, lv1444), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1448 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv139,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv42_1: R.Tensor((1280, 1280), dtype="float32") = model_params[339]
            lv43: R.Tensor((1280,), dtype="float32") = model_params[228]
            lv1449 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1448, lv42_1, lv43, lv1439), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv44: R.Tensor((1280,), dtype="float32") = model_params[224]
            lv45: R.Tensor((1280,), dtype="float32") = model_params[223]
            lv147 = R.call_tir(cls.layer_norm3, (lv1449, lv44, lv45), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv46_1: R.Tensor((1280, 5120), dtype="float32") = model_params[340]
            lv47: R.Tensor((5120,), dtype="float32") = model_params[225]
            lv1450 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv147, lv46_1, lv47), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv48: R.Tensor((5120, 1280), dtype="float32") = model_params[341]
            lv49_1: R.Tensor((1280,), dtype="float32") = model_params[226]
            lv1451 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1450, lv48, lv49_1, lv1449), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv50: R.Tensor((1280,), dtype="float32") = model_params[252]
            lv51: R.Tensor((1280,), dtype="float32") = model_params[251]
            lv156 = R.call_tir(cls.layer_norm3, (lv1451, lv50, lv51), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv52: R.Tensor((1280, 1280), dtype="float32") = model_params[342]
            lv53: R.Tensor((1280,), dtype="float32") = model_params[259]
            lv1452 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv156, lv52, lv53), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv54: R.Tensor((1280, 1280), dtype="float32") = model_params[343]
            lv55: R.Tensor((1280,), dtype="float32") = model_params[257]
            lv1453 = R.call_tir(cls.fused_matmul29_add42, (lv156, lv54, lv55), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1454 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1453,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv56: R.Tensor((1280, 1280), dtype="float32") = model_params[344]
            lv57_1: R.Tensor((1280,), dtype="float32") = model_params[260]
            lv1455 = R.call_tir(cls.fused_matmul29_add42, (lv156, lv56, lv57_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1456 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1455,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1457 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1452,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv177 = R.call_tir(cls.matmul30, (lv1457, lv1454), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1458 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv177, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv181 = R.call_tir(cls.softmax6, (lv1458,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1459 = R.call_tir(cls.fused_reshape44_reshape45, (lv181,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv184 = R.call_tir(cls.matmul31, (lv1459, lv1456), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1460 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv184,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv58: R.Tensor((1280, 1280), dtype="float32") = model_params[345]
            lv59: R.Tensor((1280,), dtype="float32") = model_params[258]
            lv1461 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1460, lv58, lv59, lv1451), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv60: R.Tensor((1280,), dtype="float32") = model_params[254]
            lv61: R.Tensor((1280,), dtype="float32") = model_params[253]
            lv192 = R.call_tir(cls.layer_norm3, (lv1461, lv60, lv61), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv62: R.Tensor((1280, 5120), dtype="float32") = model_params[346]
            lv63: R.Tensor((5120,), dtype="float32") = model_params[255]
            lv1462 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv192, lv62, lv63), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv64: R.Tensor((5120, 1280), dtype="float32") = model_params[347]
            lv65: R.Tensor((1280,), dtype="float32") = model_params[256]
            lv1463 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1462, lv64, lv65, lv1461), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv66_1: R.Tensor((1280,), dtype="float32") = model_params[262]
            lv67: R.Tensor((1280,), dtype="float32") = model_params[261]
            lv201 = R.call_tir(cls.layer_norm3, (lv1463, lv66_1, lv67), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv68: R.Tensor((1280, 1280), dtype="float32") = model_params[348]
            lv69: R.Tensor((1280,), dtype="float32") = model_params[269]
            lv1464 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv201, lv68, lv69), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv70: R.Tensor((1280, 1280), dtype="float32") = model_params[349]
            lv71: R.Tensor((1280,), dtype="float32") = model_params[267]
            lv1465 = R.call_tir(cls.fused_matmul29_add42, (lv201, lv70, lv71), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1466 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1465,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv72: R.Tensor((1280, 1280), dtype="float32") = model_params[350]
            lv73: R.Tensor((1280,), dtype="float32") = model_params[270]
            lv1467 = R.call_tir(cls.fused_matmul29_add42, (lv201, lv72, lv73), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1468 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1467,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1469 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1464,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv222 = R.call_tir(cls.matmul30, (lv1469, lv1466), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1470 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv222, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv226 = R.call_tir(cls.softmax6, (lv1470,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1471 = R.call_tir(cls.fused_reshape44_reshape45, (lv226,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv229 = R.call_tir(cls.matmul31, (lv1471, lv1468), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1472 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv229,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv74: R.Tensor((1280, 1280), dtype="float32") = model_params[351]
            lv75: R.Tensor((1280,), dtype="float32") = model_params[268]
            lv1473 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1472, lv74, lv75, lv1463), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv76: R.Tensor((1280,), dtype="float32") = model_params[264]
            lv77: R.Tensor((1280,), dtype="float32") = model_params[263]
            lv237 = R.call_tir(cls.layer_norm3, (lv1473, lv76, lv77), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv78: R.Tensor((1280, 5120), dtype="float32") = model_params[352]
            lv79: R.Tensor((5120,), dtype="float32") = model_params[265]
            lv1474 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv237, lv78, lv79), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv80: R.Tensor((5120, 1280), dtype="float32") = model_params[353]
            lv81: R.Tensor((1280,), dtype="float32") = model_params[266]
            lv1475 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1474, lv80, lv81, lv1473), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv82: R.Tensor((1280,), dtype="float32") = model_params[272]
            lv83: R.Tensor((1280,), dtype="float32") = model_params[271]
            lv246 = R.call_tir(cls.layer_norm3, (lv1475, lv82, lv83), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv84: R.Tensor((1280, 1280), dtype="float32") = model_params[354]
            lv85: R.Tensor((1280,), dtype="float32") = model_params[279]
            lv1476 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv246, lv84, lv85), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv86: R.Tensor((1280, 1280), dtype="float32") = model_params[355]
            lv87_1: R.Tensor((1280,), dtype="float32") = model_params[277]
            lv1477 = R.call_tir(cls.fused_matmul29_add42, (lv246, lv86, lv87_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1478 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1477,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv88: R.Tensor((1280, 1280), dtype="float32") = model_params[356]
            lv89: R.Tensor((1280,), dtype="float32") = model_params[280]
            lv1479 = R.call_tir(cls.fused_matmul29_add42, (lv246, lv88, lv89), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1480 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1479,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1481 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1476,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv267 = R.call_tir(cls.matmul30, (lv1481, lv1478), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1482 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv267, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv271 = R.call_tir(cls.softmax6, (lv1482,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1483 = R.call_tir(cls.fused_reshape44_reshape45, (lv271,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv274 = R.call_tir(cls.matmul31, (lv1483, lv1480), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1484 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv274,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv90: R.Tensor((1280, 1280), dtype="float32") = model_params[357]
            lv91_1: R.Tensor((1280,), dtype="float32") = model_params[278]
            lv1485 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1484, lv90, lv91_1, lv1475), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv92: R.Tensor((1280,), dtype="float32") = model_params[274]
            lv93: R.Tensor((1280,), dtype="float32") = model_params[273]
            lv282 = R.call_tir(cls.layer_norm3, (lv1485, lv92, lv93), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv94_1: R.Tensor((1280, 5120), dtype="float32") = model_params[358]
            lv95: R.Tensor((5120,), dtype="float32") = model_params[275]
            lv1486 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv282, lv94_1, lv95), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv96: R.Tensor((5120, 1280), dtype="float32") = model_params[359]
            lv97: R.Tensor((1280,), dtype="float32") = model_params[276]
            lv1487 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1486, lv96, lv97, lv1485), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv98: R.Tensor((1280,), dtype="float32") = model_params[282]
            lv99: R.Tensor((1280,), dtype="float32") = model_params[281]
            lv291 = R.call_tir(cls.layer_norm3, (lv1487, lv98, lv99), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv100: R.Tensor((1280, 1280), dtype="float32") = model_params[360]
            lv101: R.Tensor((1280,), dtype="float32") = model_params[289]
            lv1488 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv291, lv100, lv101), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv102_1: R.Tensor((1280, 1280), dtype="float32") = model_params[361]
            lv103: R.Tensor((1280,), dtype="float32") = model_params[287]
            lv1489 = R.call_tir(cls.fused_matmul29_add42, (lv291, lv102_1, lv103), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1490 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1489,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv104: R.Tensor((1280, 1280), dtype="float32") = model_params[362]
            lv105: R.Tensor((1280,), dtype="float32") = model_params[290]
            lv1491 = R.call_tir(cls.fused_matmul29_add42, (lv291, lv104, lv105), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1492 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1491,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1493 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1488,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv312 = R.call_tir(cls.matmul30, (lv1493, lv1490), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1494 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv312, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv316 = R.call_tir(cls.softmax6, (lv1494,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1495 = R.call_tir(cls.fused_reshape44_reshape45, (lv316,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv319 = R.call_tir(cls.matmul31, (lv1495, lv1492), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1496 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv319,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv106: R.Tensor((1280, 1280), dtype="float32") = model_params[363]
            lv107: R.Tensor((1280,), dtype="float32") = model_params[288]
            lv1497 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1496, lv106, lv107, lv1487), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv108: R.Tensor((1280,), dtype="float32") = model_params[284]
            lv109: R.Tensor((1280,), dtype="float32") = model_params[283]
            lv327 = R.call_tir(cls.layer_norm3, (lv1497, lv108, lv109), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv110: R.Tensor((1280, 5120), dtype="float32") = model_params[364]
            lv111_1: R.Tensor((5120,), dtype="float32") = model_params[285]
            lv1498 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv327, lv110, lv111_1), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv112: R.Tensor((5120, 1280), dtype="float32") = model_params[365]
            lv113: R.Tensor((1280,), dtype="float32") = model_params[286]
            lv1499 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1498, lv112, lv113, lv1497), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv114: R.Tensor((1280,), dtype="float32") = model_params[292]
            lv115: R.Tensor((1280,), dtype="float32") = model_params[291]
            lv336 = R.call_tir(cls.layer_norm3, (lv1499, lv114, lv115), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv116: R.Tensor((1280, 1280), dtype="float32") = model_params[366]
            lv117: R.Tensor((1280,), dtype="float32") = model_params[299]
            lv1500 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv336, lv116, lv117), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv118: R.Tensor((1280, 1280), dtype="float32") = model_params[367]
            lv119: R.Tensor((1280,), dtype="float32") = model_params[297]
            lv1501 = R.call_tir(cls.fused_matmul29_add42, (lv336, lv118, lv119), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1502 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1501,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv120: R.Tensor((1280, 1280), dtype="float32") = model_params[368]
            lv121: R.Tensor((1280,), dtype="float32") = model_params[300]
            lv1503 = R.call_tir(cls.fused_matmul29_add42, (lv336, lv120, lv121), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1504 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1503,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1505 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1500,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv357 = R.call_tir(cls.matmul30, (lv1505, lv1502), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1506 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv357, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv361 = R.call_tir(cls.softmax6, (lv1506,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1507 = R.call_tir(cls.fused_reshape44_reshape45, (lv361,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv364 = R.call_tir(cls.matmul31, (lv1507, lv1504), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1508 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv364,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv122: R.Tensor((1280, 1280), dtype="float32") = model_params[369]
            lv123: R.Tensor((1280,), dtype="float32") = model_params[298]
            lv1509 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1508, lv122, lv123, lv1499), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv124: R.Tensor((1280,), dtype="float32") = model_params[294]
            lv125: R.Tensor((1280,), dtype="float32") = model_params[293]
            lv372 = R.call_tir(cls.layer_norm3, (lv1509, lv124, lv125), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv126: R.Tensor((1280, 5120), dtype="float32") = model_params[370]
            lv127: R.Tensor((5120,), dtype="float32") = model_params[295]
            lv1510 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv372, lv126, lv127), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv128: R.Tensor((5120, 1280), dtype="float32") = model_params[371]
            lv129: R.Tensor((1280,), dtype="float32") = model_params[296]
            lv1511 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1510, lv128, lv129, lv1509), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv130: R.Tensor((1280,), dtype="float32") = model_params[302]
            lv131: R.Tensor((1280,), dtype="float32") = model_params[301]
            lv381 = R.call_tir(cls.layer_norm3, (lv1511, lv130, lv131), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv132_1: R.Tensor((1280, 1280), dtype="float32") = model_params[372]
            lv133: R.Tensor((1280,), dtype="float32") = model_params[309]
            lv1512 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv381, lv132_1, lv133), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv134: R.Tensor((1280, 1280), dtype="float32") = model_params[373]
            lv135: R.Tensor((1280,), dtype="float32") = model_params[307]
            lv1513 = R.call_tir(cls.fused_matmul29_add42, (lv381, lv134, lv135), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1514 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1513,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv136_1: R.Tensor((1280, 1280), dtype="float32") = model_params[374]
            lv137: R.Tensor((1280,), dtype="float32") = model_params[310]
            lv1515 = R.call_tir(cls.fused_matmul29_add42, (lv381, lv136_1, lv137), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1516 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1515,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1517 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1512,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv402 = R.call_tir(cls.matmul30, (lv1517, lv1514), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1518 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv402, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv406 = R.call_tir(cls.softmax6, (lv1518,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1519 = R.call_tir(cls.fused_reshape44_reshape45, (lv406,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv409 = R.call_tir(cls.matmul31, (lv1519, lv1516), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1520 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv409,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv138: R.Tensor((1280, 1280), dtype="float32") = model_params[375]
            lv139_1: R.Tensor((1280,), dtype="float32") = model_params[308]
            lv1521 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1520, lv138, lv139_1, lv1511), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv140: R.Tensor((1280,), dtype="float32") = model_params[304]
            lv141: R.Tensor((1280,), dtype="float32") = model_params[303]
            lv417 = R.call_tir(cls.layer_norm3, (lv1521, lv140, lv141), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv142: R.Tensor((1280, 5120), dtype="float32") = model_params[376]
            lv143: R.Tensor((5120,), dtype="float32") = model_params[305]
            lv1522 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv417, lv142, lv143), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv144: R.Tensor((5120, 1280), dtype="float32") = model_params[377]
            lv145: R.Tensor((1280,), dtype="float32") = model_params[306]
            lv1523 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1522, lv144, lv145, lv1521), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv146: R.Tensor((1280,), dtype="float32") = model_params[312]
            lv147_1: R.Tensor((1280,), dtype="float32") = model_params[311]
            lv426 = R.call_tir(cls.layer_norm3, (lv1523, lv146, lv147_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv148: R.Tensor((1280, 1280), dtype="float32") = model_params[378]
            lv149: R.Tensor((1280,), dtype="float32") = model_params[319]
            lv1524 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv426, lv148, lv149), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv150: R.Tensor((1280, 1280), dtype="float32") = model_params[379]
            lv151: R.Tensor((1280,), dtype="float32") = model_params[317]
            lv1525 = R.call_tir(cls.fused_matmul29_add42, (lv426, lv150, lv151), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1526 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1525,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv152: R.Tensor((1280, 1280), dtype="float32") = model_params[380]
            lv153: R.Tensor((1280,), dtype="float32") = model_params[320]
            lv1527 = R.call_tir(cls.fused_matmul29_add42, (lv426, lv152, lv153), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1528 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1527,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1529 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1524,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv447 = R.call_tir(cls.matmul30, (lv1529, lv1526), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1530 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv447, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv451 = R.call_tir(cls.softmax6, (lv1530,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1531 = R.call_tir(cls.fused_reshape44_reshape45, (lv451,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv454 = R.call_tir(cls.matmul31, (lv1531, lv1528), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1532 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv454,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv154: R.Tensor((1280, 1280), dtype="float32") = model_params[381]
            lv155: R.Tensor((1280,), dtype="float32") = model_params[318]
            lv1533 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1532, lv154, lv155, lv1523), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv156_1: R.Tensor((1280,), dtype="float32") = model_params[314]
            lv157: R.Tensor((1280,), dtype="float32") = model_params[313]
            lv462 = R.call_tir(cls.layer_norm3, (lv1533, lv156_1, lv157), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv158: R.Tensor((1280, 5120), dtype="float32") = model_params[382]
            lv159: R.Tensor((5120,), dtype="float32") = model_params[315]
            lv1534 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv462, lv158, lv159), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv160: R.Tensor((5120, 1280), dtype="float32") = model_params[383]
            lv161: R.Tensor((1280,), dtype="float32") = model_params[316]
            lv1535 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1534, lv160, lv161, lv1533), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv162: R.Tensor((1280,), dtype="float32") = model_params[12]
            lv163: R.Tensor((1280,), dtype="float32") = model_params[11]
            lv471 = R.call_tir(cls.layer_norm3, (lv1535, lv162, lv163), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv164: R.Tensor((1280, 1280), dtype="float32") = model_params[384]
            lv165: R.Tensor((1280,), dtype="float32") = model_params[19]
            lv1536 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv471, lv164, lv165), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv166: R.Tensor((1280, 1280), dtype="float32") = model_params[385]
            lv167: R.Tensor((1280,), dtype="float32") = model_params[17]
            lv1537 = R.call_tir(cls.fused_matmul29_add42, (lv471, lv166, lv167), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1538 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1537,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv168: R.Tensor((1280, 1280), dtype="float32") = model_params[386]
            lv169: R.Tensor((1280,), dtype="float32") = model_params[20]
            lv1539 = R.call_tir(cls.fused_matmul29_add42, (lv471, lv168, lv169), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1540 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1539,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1541 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1536,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv492 = R.call_tir(cls.matmul30, (lv1541, lv1538), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1542 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv492, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv496 = R.call_tir(cls.softmax6, (lv1542,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1543 = R.call_tir(cls.fused_reshape44_reshape45, (lv496,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv499 = R.call_tir(cls.matmul31, (lv1543, lv1540), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1544 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv499,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv170: R.Tensor((1280, 1280), dtype="float32") = model_params[387]
            lv171: R.Tensor((1280,), dtype="float32") = model_params[18]
            lv1545 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1544, lv170, lv171, lv1535), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv172: R.Tensor((1280,), dtype="float32") = model_params[14]
            lv173: R.Tensor((1280,), dtype="float32") = model_params[13]
            lv507 = R.call_tir(cls.layer_norm3, (lv1545, lv172, lv173), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv174: R.Tensor((1280, 5120), dtype="float32") = model_params[388]
            lv175: R.Tensor((5120,), dtype="float32") = model_params[15]
            lv1546 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv507, lv174, lv175), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv176: R.Tensor((5120, 1280), dtype="float32") = model_params[389]
            lv177_1: R.Tensor((1280,), dtype="float32") = model_params[16]
            lv1547 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1546, lv176, lv177_1, lv1545), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv178: R.Tensor((1280,), dtype="float32") = model_params[22]
            lv179: R.Tensor((1280,), dtype="float32") = model_params[21]
            lv516 = R.call_tir(cls.layer_norm3, (lv1547, lv178, lv179), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv180: R.Tensor((1280, 1280), dtype="float32") = model_params[390]
            lv181_1: R.Tensor((1280,), dtype="float32") = model_params[29]
            lv1548 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv516, lv180, lv181_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv182: R.Tensor((1280, 1280), dtype="float32") = model_params[391]
            lv183: R.Tensor((1280,), dtype="float32") = model_params[27]
            lv1549 = R.call_tir(cls.fused_matmul29_add42, (lv516, lv182, lv183), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1550 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1549,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv184_1: R.Tensor((1280, 1280), dtype="float32") = model_params[392]
            lv185: R.Tensor((1280,), dtype="float32") = model_params[30]
            lv1551 = R.call_tir(cls.fused_matmul29_add42, (lv516, lv184_1, lv185), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1552 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1551,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1553 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1548,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv537 = R.call_tir(cls.matmul30, (lv1553, lv1550), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1554 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv537, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv541 = R.call_tir(cls.softmax6, (lv1554,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1555 = R.call_tir(cls.fused_reshape44_reshape45, (lv541,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv544 = R.call_tir(cls.matmul31, (lv1555, lv1552), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1556 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv544,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv186: R.Tensor((1280, 1280), dtype="float32") = model_params[393]
            lv187: R.Tensor((1280,), dtype="float32") = model_params[28]
            lv1557 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1556, lv186, lv187, lv1547), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv188: R.Tensor((1280,), dtype="float32") = model_params[24]
            lv189: R.Tensor((1280,), dtype="float32") = model_params[23]
            lv552 = R.call_tir(cls.layer_norm3, (lv1557, lv188, lv189), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv190: R.Tensor((1280, 5120), dtype="float32") = model_params[394]
            lv191: R.Tensor((5120,), dtype="float32") = model_params[25]
            lv1558 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv552, lv190, lv191), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv192_1: R.Tensor((5120, 1280), dtype="float32") = model_params[395]
            lv193: R.Tensor((1280,), dtype="float32") = model_params[26]
            lv1559 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1558, lv192_1, lv193, lv1557), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv194: R.Tensor((1280,), dtype="float32") = model_params[32]
            lv195: R.Tensor((1280,), dtype="float32") = model_params[31]
            lv561 = R.call_tir(cls.layer_norm3, (lv1559, lv194, lv195), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv196: R.Tensor((1280, 1280), dtype="float32") = model_params[396]
            lv197: R.Tensor((1280,), dtype="float32") = model_params[39]
            lv1560 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv561, lv196, lv197), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv198: R.Tensor((1280, 1280), dtype="float32") = model_params[397]
            lv199: R.Tensor((1280,), dtype="float32") = model_params[37]
            lv1561 = R.call_tir(cls.fused_matmul29_add42, (lv561, lv198, lv199), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1562 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1561,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv200: R.Tensor((1280, 1280), dtype="float32") = model_params[398]
            lv201_1: R.Tensor((1280,), dtype="float32") = model_params[40]
            lv1563 = R.call_tir(cls.fused_matmul29_add42, (lv561, lv200, lv201_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1564 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1563,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1565 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1560,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv582 = R.call_tir(cls.matmul30, (lv1565, lv1562), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1566 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv582, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv586 = R.call_tir(cls.softmax6, (lv1566,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1567 = R.call_tir(cls.fused_reshape44_reshape45, (lv586,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv589 = R.call_tir(cls.matmul31, (lv1567, lv1564), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1568 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv589,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv202: R.Tensor((1280, 1280), dtype="float32") = model_params[399]
            lv203: R.Tensor((1280,), dtype="float32") = model_params[38]
            lv1569 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1568, lv202, lv203, lv1559), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv204: R.Tensor((1280,), dtype="float32") = model_params[34]
            lv205: R.Tensor((1280,), dtype="float32") = model_params[33]
            lv597 = R.call_tir(cls.layer_norm3, (lv1569, lv204, lv205), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv206: R.Tensor((1280, 5120), dtype="float32") = model_params[400]
            lv207: R.Tensor((5120,), dtype="float32") = model_params[35]
            lv1570 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv597, lv206, lv207), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv208: R.Tensor((5120, 1280), dtype="float32") = model_params[401]
            lv209: R.Tensor((1280,), dtype="float32") = model_params[36]
            lv1571 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1570, lv208, lv209, lv1569), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv210: R.Tensor((1280,), dtype="float32") = model_params[42]
            lv211: R.Tensor((1280,), dtype="float32") = model_params[41]
            lv606 = R.call_tir(cls.layer_norm3, (lv1571, lv210, lv211), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv212: R.Tensor((1280, 1280), dtype="float32") = model_params[402]
            lv213: R.Tensor((1280,), dtype="float32") = model_params[49]
            lv1572 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv606, lv212, lv213), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv214: R.Tensor((1280, 1280), dtype="float32") = model_params[403]
            lv215: R.Tensor((1280,), dtype="float32") = model_params[47]
            lv1573 = R.call_tir(cls.fused_matmul29_add42, (lv606, lv214, lv215), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1574 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1573,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv216: R.Tensor((1280, 1280), dtype="float32") = model_params[404]
            lv217: R.Tensor((1280,), dtype="float32") = model_params[50]
            lv1575 = R.call_tir(cls.fused_matmul29_add42, (lv606, lv216, lv217), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1576 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1575,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1577 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1572,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv627 = R.call_tir(cls.matmul30, (lv1577, lv1574), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1578 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv627, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv631 = R.call_tir(cls.softmax6, (lv1578,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1579 = R.call_tir(cls.fused_reshape44_reshape45, (lv631,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv634 = R.call_tir(cls.matmul31, (lv1579, lv1576), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1580 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv634,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv218: R.Tensor((1280, 1280), dtype="float32") = model_params[405]
            lv219: R.Tensor((1280,), dtype="float32") = model_params[48]
            lv1581 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1580, lv218, lv219, lv1571), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv220: R.Tensor((1280,), dtype="float32") = model_params[44]
            lv221: R.Tensor((1280,), dtype="float32") = model_params[43]
            lv642 = R.call_tir(cls.layer_norm3, (lv1581, lv220, lv221), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv222_1: R.Tensor((1280, 5120), dtype="float32") = model_params[406]
            lv223: R.Tensor((5120,), dtype="float32") = model_params[45]
            lv1582 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv642, lv222_1, lv223), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv224: R.Tensor((5120, 1280), dtype="float32") = model_params[407]
            lv225: R.Tensor((1280,), dtype="float32") = model_params[46]
            lv1583 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1582, lv224, lv225, lv1581), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv226_1: R.Tensor((1280,), dtype="float32") = model_params[52]
            lv227: R.Tensor((1280,), dtype="float32") = model_params[51]
            lv651 = R.call_tir(cls.layer_norm3, (lv1583, lv226_1, lv227), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv228: R.Tensor((1280, 1280), dtype="float32") = model_params[408]
            lv229_1: R.Tensor((1280,), dtype="float32") = model_params[59]
            lv1584 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv651, lv228, lv229_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv230: R.Tensor((1280, 1280), dtype="float32") = model_params[409]
            lv231: R.Tensor((1280,), dtype="float32") = model_params[57]
            lv1585 = R.call_tir(cls.fused_matmul29_add42, (lv651, lv230, lv231), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1586 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1585,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv232: R.Tensor((1280, 1280), dtype="float32") = model_params[410]
            lv233: R.Tensor((1280,), dtype="float32") = model_params[60]
            lv1587 = R.call_tir(cls.fused_matmul29_add42, (lv651, lv232, lv233), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1588 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1587,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1589 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1584,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv672 = R.call_tir(cls.matmul30, (lv1589, lv1586), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1590 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv672, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv676 = R.call_tir(cls.softmax6, (lv1590,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1591 = R.call_tir(cls.fused_reshape44_reshape45, (lv676,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv679 = R.call_tir(cls.matmul31, (lv1591, lv1588), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1592 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv679,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv234: R.Tensor((1280, 1280), dtype="float32") = model_params[411]
            lv235: R.Tensor((1280,), dtype="float32") = model_params[58]
            lv1593 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1592, lv234, lv235, lv1583), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv236: R.Tensor((1280,), dtype="float32") = model_params[54]
            lv237_1: R.Tensor((1280,), dtype="float32") = model_params[53]
            lv687 = R.call_tir(cls.layer_norm3, (lv1593, lv236, lv237_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv238: R.Tensor((1280, 5120), dtype="float32") = model_params[412]
            lv239: R.Tensor((5120,), dtype="float32") = model_params[55]
            lv1594 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv687, lv238, lv239), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv240: R.Tensor((5120, 1280), dtype="float32") = model_params[413]
            lv241: R.Tensor((1280,), dtype="float32") = model_params[56]
            lv1595 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1594, lv240, lv241, lv1593), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv242: R.Tensor((1280,), dtype="float32") = model_params[62]
            lv243: R.Tensor((1280,), dtype="float32") = model_params[61]
            lv696 = R.call_tir(cls.layer_norm3, (lv1595, lv242, lv243), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv244: R.Tensor((1280, 1280), dtype="float32") = model_params[414]
            lv245: R.Tensor((1280,), dtype="float32") = model_params[69]
            lv1596 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv696, lv244, lv245), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv246_1: R.Tensor((1280, 1280), dtype="float32") = model_params[415]
            lv247: R.Tensor((1280,), dtype="float32") = model_params[67]
            lv1597 = R.call_tir(cls.fused_matmul29_add42, (lv696, lv246_1, lv247), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1598 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1597,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv248: R.Tensor((1280, 1280), dtype="float32") = model_params[416]
            lv249: R.Tensor((1280,), dtype="float32") = model_params[70]
            lv1599 = R.call_tir(cls.fused_matmul29_add42, (lv696, lv248, lv249), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1600 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1599,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1601 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1596,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv717 = R.call_tir(cls.matmul30, (lv1601, lv1598), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1602 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv717, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv721 = R.call_tir(cls.softmax6, (lv1602,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1603 = R.call_tir(cls.fused_reshape44_reshape45, (lv721,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv724 = R.call_tir(cls.matmul31, (lv1603, lv1600), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1604 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv724,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv250: R.Tensor((1280, 1280), dtype="float32") = model_params[417]
            lv251: R.Tensor((1280,), dtype="float32") = model_params[68]
            lv1605 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1604, lv250, lv251, lv1595), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv252: R.Tensor((1280,), dtype="float32") = model_params[64]
            lv253: R.Tensor((1280,), dtype="float32") = model_params[63]
            lv732 = R.call_tir(cls.layer_norm3, (lv1605, lv252, lv253), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv254: R.Tensor((1280, 5120), dtype="float32") = model_params[418]
            lv255: R.Tensor((5120,), dtype="float32") = model_params[65]
            lv1606 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv732, lv254, lv255), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv256: R.Tensor((5120, 1280), dtype="float32") = model_params[419]
            lv257: R.Tensor((1280,), dtype="float32") = model_params[66]
            lv1607 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1606, lv256, lv257, lv1605), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv258: R.Tensor((1280,), dtype="float32") = model_params[72]
            lv259: R.Tensor((1280,), dtype="float32") = model_params[71]
            lv741 = R.call_tir(cls.layer_norm3, (lv1607, lv258, lv259), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv260: R.Tensor((1280, 1280), dtype="float32") = model_params[420]
            lv261: R.Tensor((1280,), dtype="float32") = model_params[79]
            lv1608 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv741, lv260, lv261), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv262: R.Tensor((1280, 1280), dtype="float32") = model_params[421]
            lv263: R.Tensor((1280,), dtype="float32") = model_params[77]
            lv1609 = R.call_tir(cls.fused_matmul29_add42, (lv741, lv262, lv263), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1610 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1609,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv264: R.Tensor((1280, 1280), dtype="float32") = model_params[422]
            lv265: R.Tensor((1280,), dtype="float32") = model_params[80]
            lv1611 = R.call_tir(cls.fused_matmul29_add42, (lv741, lv264, lv265), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1612 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1611,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1613 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1608,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv762 = R.call_tir(cls.matmul30, (lv1613, lv1610), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1614 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv762, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv766 = R.call_tir(cls.softmax6, (lv1614,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1615 = R.call_tir(cls.fused_reshape44_reshape45, (lv766,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv769 = R.call_tir(cls.matmul31, (lv1615, lv1612), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1616 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv769,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv266: R.Tensor((1280, 1280), dtype="float32") = model_params[423]
            lv267_1: R.Tensor((1280,), dtype="float32") = model_params[78]
            lv1617 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1616, lv266, lv267_1, lv1607), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv268: R.Tensor((1280,), dtype="float32") = model_params[74]
            lv269: R.Tensor((1280,), dtype="float32") = model_params[73]
            lv777 = R.call_tir(cls.layer_norm3, (lv1617, lv268, lv269), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv270: R.Tensor((1280, 5120), dtype="float32") = model_params[424]
            lv271_1: R.Tensor((5120,), dtype="float32") = model_params[75]
            lv1618 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv777, lv270, lv271_1), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv272: R.Tensor((5120, 1280), dtype="float32") = model_params[425]
            lv273: R.Tensor((1280,), dtype="float32") = model_params[76]
            lv1619 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1618, lv272, lv273, lv1617), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv274_1: R.Tensor((1280,), dtype="float32") = model_params[82]
            lv275: R.Tensor((1280,), dtype="float32") = model_params[81]
            lv786 = R.call_tir(cls.layer_norm3, (lv1619, lv274_1, lv275), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv276: R.Tensor((1280, 1280), dtype="float32") = model_params[426]
            lv277: R.Tensor((1280,), dtype="float32") = model_params[89]
            lv1620 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv786, lv276, lv277), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv278: R.Tensor((1280, 1280), dtype="float32") = model_params[427]
            lv279: R.Tensor((1280,), dtype="float32") = model_params[87]
            lv1621 = R.call_tir(cls.fused_matmul29_add42, (lv786, lv278, lv279), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1622 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1621,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv280: R.Tensor((1280, 1280), dtype="float32") = model_params[428]
            lv281: R.Tensor((1280,), dtype="float32") = model_params[90]
            lv1623 = R.call_tir(cls.fused_matmul29_add42, (lv786, lv280, lv281), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1624 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1623,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1625 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1620,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv807 = R.call_tir(cls.matmul30, (lv1625, lv1622), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1626 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv807, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv811 = R.call_tir(cls.softmax6, (lv1626,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1627 = R.call_tir(cls.fused_reshape44_reshape45, (lv811,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv814 = R.call_tir(cls.matmul31, (lv1627, lv1624), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1628 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv814,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv282_1: R.Tensor((1280, 1280), dtype="float32") = model_params[429]
            lv283: R.Tensor((1280,), dtype="float32") = model_params[88]
            lv1629 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1628, lv282_1, lv283, lv1619), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv284: R.Tensor((1280,), dtype="float32") = model_params[84]
            lv285: R.Tensor((1280,), dtype="float32") = model_params[83]
            lv822 = R.call_tir(cls.layer_norm3, (lv1629, lv284, lv285), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv286: R.Tensor((1280, 5120), dtype="float32") = model_params[430]
            lv287: R.Tensor((5120,), dtype="float32") = model_params[85]
            lv1630 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv822, lv286, lv287), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv288: R.Tensor((5120, 1280), dtype="float32") = model_params[431]
            lv289: R.Tensor((1280,), dtype="float32") = model_params[86]
            lv1631 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1630, lv288, lv289, lv1629), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv290: R.Tensor((1280,), dtype="float32") = model_params[92]
            lv291_1: R.Tensor((1280,), dtype="float32") = model_params[91]
            lv831 = R.call_tir(cls.layer_norm3, (lv1631, lv290, lv291_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv292: R.Tensor((1280, 1280), dtype="float32") = model_params[432]
            lv293: R.Tensor((1280,), dtype="float32") = model_params[99]
            lv1632 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv831, lv292, lv293), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv294: R.Tensor((1280, 1280), dtype="float32") = model_params[433]
            lv295: R.Tensor((1280,), dtype="float32") = model_params[97]
            lv1633 = R.call_tir(cls.fused_matmul29_add42, (lv831, lv294, lv295), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1634 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1633,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv296: R.Tensor((1280, 1280), dtype="float32") = model_params[434]
            lv297: R.Tensor((1280,), dtype="float32") = model_params[100]
            lv1635 = R.call_tir(cls.fused_matmul29_add42, (lv831, lv296, lv297), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1636 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1635,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1637 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1632,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv852 = R.call_tir(cls.matmul30, (lv1637, lv1634), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1638 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv852, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv856 = R.call_tir(cls.softmax6, (lv1638,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1639 = R.call_tir(cls.fused_reshape44_reshape45, (lv856,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv859 = R.call_tir(cls.matmul31, (lv1639, lv1636), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1640 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv859,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv298: R.Tensor((1280, 1280), dtype="float32") = model_params[435]
            lv299: R.Tensor((1280,), dtype="float32") = model_params[98]
            lv1641 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1640, lv298, lv299, lv1631), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv300: R.Tensor((1280,), dtype="float32") = model_params[94]
            lv301: R.Tensor((1280,), dtype="float32") = model_params[93]
            lv867 = R.call_tir(cls.layer_norm3, (lv1641, lv300, lv301), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv302: R.Tensor((1280, 5120), dtype="float32") = model_params[436]
            lv303: R.Tensor((5120,), dtype="float32") = model_params[95]
            lv1642 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv867, lv302, lv303), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv304: R.Tensor((5120, 1280), dtype="float32") = model_params[437]
            lv305: R.Tensor((1280,), dtype="float32") = model_params[96]
            lv1643 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1642, lv304, lv305, lv1641), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv306: R.Tensor((1280,), dtype="float32") = model_params[102]
            lv307: R.Tensor((1280,), dtype="float32") = model_params[101]
            lv876 = R.call_tir(cls.layer_norm3, (lv1643, lv306, lv307), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv308: R.Tensor((1280, 1280), dtype="float32") = model_params[438]
            lv309: R.Tensor((1280,), dtype="float32") = model_params[109]
            lv1644 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv876, lv308, lv309), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv310: R.Tensor((1280, 1280), dtype="float32") = model_params[439]
            lv311: R.Tensor((1280,), dtype="float32") = model_params[107]
            lv1645 = R.call_tir(cls.fused_matmul29_add42, (lv876, lv310, lv311), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1646 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1645,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv312_1: R.Tensor((1280, 1280), dtype="float32") = model_params[440]
            lv313: R.Tensor((1280,), dtype="float32") = model_params[110]
            lv1647 = R.call_tir(cls.fused_matmul29_add42, (lv876, lv312_1, lv313), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1648 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1647,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1649 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1644,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv897 = R.call_tir(cls.matmul30, (lv1649, lv1646), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1650 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv897, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv901 = R.call_tir(cls.softmax6, (lv1650,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1651 = R.call_tir(cls.fused_reshape44_reshape45, (lv901,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv904 = R.call_tir(cls.matmul31, (lv1651, lv1648), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1652 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv904,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv314: R.Tensor((1280, 1280), dtype="float32") = model_params[441]
            lv315: R.Tensor((1280,), dtype="float32") = model_params[108]
            lv1653 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1652, lv314, lv315, lv1643), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv316_1: R.Tensor((1280,), dtype="float32") = model_params[104]
            lv317: R.Tensor((1280,), dtype="float32") = model_params[103]
            lv912 = R.call_tir(cls.layer_norm3, (lv1653, lv316_1, lv317), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv318: R.Tensor((1280, 5120), dtype="float32") = model_params[442]
            lv319_1: R.Tensor((5120,), dtype="float32") = model_params[105]
            lv1654 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv912, lv318, lv319_1), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv320: R.Tensor((5120, 1280), dtype="float32") = model_params[443]
            lv321: R.Tensor((1280,), dtype="float32") = model_params[106]
            lv1655 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1654, lv320, lv321, lv1653), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv322: R.Tensor((1280,), dtype="float32") = model_params[122]
            lv323: R.Tensor((1280,), dtype="float32") = model_params[121]
            lv921 = R.call_tir(cls.layer_norm3, (lv1655, lv322, lv323), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv324: R.Tensor((1280, 1280), dtype="float32") = model_params[444]
            lv325: R.Tensor((1280,), dtype="float32") = model_params[129]
            lv1656 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv921, lv324, lv325), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv326: R.Tensor((1280, 1280), dtype="float32") = model_params[445]
            lv327_1: R.Tensor((1280,), dtype="float32") = model_params[127]
            lv1657 = R.call_tir(cls.fused_matmul29_add42, (lv921, lv326, lv327_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1658 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1657,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv328: R.Tensor((1280, 1280), dtype="float32") = model_params[446]
            lv329: R.Tensor((1280,), dtype="float32") = model_params[130]
            lv1659 = R.call_tir(cls.fused_matmul29_add42, (lv921, lv328, lv329), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1660 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1659,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1661 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1656,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv942 = R.call_tir(cls.matmul30, (lv1661, lv1658), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1662 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv942, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv946 = R.call_tir(cls.softmax6, (lv1662,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1663 = R.call_tir(cls.fused_reshape44_reshape45, (lv946,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv949 = R.call_tir(cls.matmul31, (lv1663, lv1660), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1664 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv949,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv330: R.Tensor((1280, 1280), dtype="float32") = model_params[447]
            lv331: R.Tensor((1280,), dtype="float32") = model_params[128]
            lv1665 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1664, lv330, lv331, lv1655), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv332: R.Tensor((1280,), dtype="float32") = model_params[124]
            lv333: R.Tensor((1280,), dtype="float32") = model_params[123]
            lv957 = R.call_tir(cls.layer_norm3, (lv1665, lv332, lv333), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv334: R.Tensor((1280, 5120), dtype="float32") = model_params[448]
            lv335: R.Tensor((5120,), dtype="float32") = model_params[125]
            lv1666 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv957, lv334, lv335), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv336_1: R.Tensor((5120, 1280), dtype="float32") = model_params[449]
            lv337: R.Tensor((1280,), dtype="float32") = model_params[126]
            lv1667 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1666, lv336_1, lv337, lv1665), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv338: R.Tensor((1280,), dtype="float32") = model_params[132]
            lv339: R.Tensor((1280,), dtype="float32") = model_params[131]
            lv966 = R.call_tir(cls.layer_norm3, (lv1667, lv338, lv339), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv340: R.Tensor((1280, 1280), dtype="float32") = model_params[450]
            lv341: R.Tensor((1280,), dtype="float32") = model_params[139]
            lv1668 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv966, lv340, lv341), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv342: R.Tensor((1280, 1280), dtype="float32") = model_params[451]
            lv343: R.Tensor((1280,), dtype="float32") = model_params[137]
            lv1669 = R.call_tir(cls.fused_matmul29_add42, (lv966, lv342, lv343), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1670 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1669,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv344: R.Tensor((1280, 1280), dtype="float32") = model_params[452]
            lv345: R.Tensor((1280,), dtype="float32") = model_params[140]
            lv1671 = R.call_tir(cls.fused_matmul29_add42, (lv966, lv344, lv345), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1672 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1671,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1673 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1668,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv987 = R.call_tir(cls.matmul30, (lv1673, lv1670), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1674 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv987, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv991 = R.call_tir(cls.softmax6, (lv1674,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1675 = R.call_tir(cls.fused_reshape44_reshape45, (lv991,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv994 = R.call_tir(cls.matmul31, (lv1675, lv1672), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1676 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv994,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv346: R.Tensor((1280, 1280), dtype="float32") = model_params[453]
            lv347: R.Tensor((1280,), dtype="float32") = model_params[138]
            lv1677 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1676, lv346, lv347, lv1667), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv348: R.Tensor((1280,), dtype="float32") = model_params[134]
            lv349: R.Tensor((1280,), dtype="float32") = model_params[133]
            lv1002 = R.call_tir(cls.layer_norm3, (lv1677, lv348, lv349), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv350: R.Tensor((1280, 5120), dtype="float32") = model_params[454]
            lv351: R.Tensor((5120,), dtype="float32") = model_params[135]
            lv1678 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1002, lv350, lv351), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv352: R.Tensor((5120, 1280), dtype="float32") = model_params[455]
            lv353: R.Tensor((1280,), dtype="float32") = model_params[136]
            lv1679 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1678, lv352, lv353, lv1677), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv354: R.Tensor((1280,), dtype="float32") = model_params[142]
            lv355: R.Tensor((1280,), dtype="float32") = model_params[141]
            lv1011 = R.call_tir(cls.layer_norm3, (lv1679, lv354, lv355), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv356: R.Tensor((1280, 1280), dtype="float32") = model_params[456]
            lv357_1: R.Tensor((1280,), dtype="float32") = model_params[149]
            lv1680 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1011, lv356, lv357_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv358: R.Tensor((1280, 1280), dtype="float32") = model_params[457]
            lv359: R.Tensor((1280,), dtype="float32") = model_params[147]
            lv1681 = R.call_tir(cls.fused_matmul29_add42, (lv1011, lv358, lv359), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1682 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1681,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv360: R.Tensor((1280, 1280), dtype="float32") = model_params[458]
            lv361_1: R.Tensor((1280,), dtype="float32") = model_params[150]
            lv1683 = R.call_tir(cls.fused_matmul29_add42, (lv1011, lv360, lv361_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1684 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1683,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1685 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1680,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1032 = R.call_tir(cls.matmul30, (lv1685, lv1682), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1686 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1032, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1036 = R.call_tir(cls.softmax6, (lv1686,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1687 = R.call_tir(cls.fused_reshape44_reshape45, (lv1036,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1039 = R.call_tir(cls.matmul31, (lv1687, lv1684), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1688 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1039,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv362: R.Tensor((1280, 1280), dtype="float32") = model_params[459]
            lv363: R.Tensor((1280,), dtype="float32") = model_params[148]
            lv1689 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1688, lv362, lv363, lv1679), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv364_1: R.Tensor((1280,), dtype="float32") = model_params[144]
            lv365: R.Tensor((1280,), dtype="float32") = model_params[143]
            lv1047 = R.call_tir(cls.layer_norm3, (lv1689, lv364_1, lv365), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv366: R.Tensor((1280, 5120), dtype="float32") = model_params[460]
            lv367: R.Tensor((5120,), dtype="float32") = model_params[145]
            lv1690 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1047, lv366, lv367), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv368: R.Tensor((5120, 1280), dtype="float32") = model_params[461]
            lv369: R.Tensor((1280,), dtype="float32") = model_params[146]
            lv1691 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1690, lv368, lv369, lv1689), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv370: R.Tensor((1280,), dtype="float32") = model_params[152]
            lv371: R.Tensor((1280,), dtype="float32") = model_params[151]
            lv1056 = R.call_tir(cls.layer_norm3, (lv1691, lv370, lv371), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv372_1: R.Tensor((1280, 1280), dtype="float32") = model_params[462]
            lv373: R.Tensor((1280,), dtype="float32") = model_params[159]
            lv1692 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1056, lv372_1, lv373), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv374: R.Tensor((1280, 1280), dtype="float32") = model_params[463]
            lv375: R.Tensor((1280,), dtype="float32") = model_params[157]
            lv1693 = R.call_tir(cls.fused_matmul29_add42, (lv1056, lv374, lv375), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1694 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1693,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv376: R.Tensor((1280, 1280), dtype="float32") = model_params[464]
            lv377: R.Tensor((1280,), dtype="float32") = model_params[160]
            lv1695 = R.call_tir(cls.fused_matmul29_add42, (lv1056, lv376, lv377), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1696 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1695,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1697 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1692,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1077 = R.call_tir(cls.matmul30, (lv1697, lv1694), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1698 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1077, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1081 = R.call_tir(cls.softmax6, (lv1698,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1699 = R.call_tir(cls.fused_reshape44_reshape45, (lv1081,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1084 = R.call_tir(cls.matmul31, (lv1699, lv1696), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1700 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1084,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv378: R.Tensor((1280, 1280), dtype="float32") = model_params[465]
            lv379: R.Tensor((1280,), dtype="float32") = model_params[158]
            lv1701 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1700, lv378, lv379, lv1691), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv380: R.Tensor((1280,), dtype="float32") = model_params[154]
            lv381_1: R.Tensor((1280,), dtype="float32") = model_params[153]
            lv1092 = R.call_tir(cls.layer_norm3, (lv1701, lv380, lv381_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv382: R.Tensor((1280, 5120), dtype="float32") = model_params[466]
            lv383: R.Tensor((5120,), dtype="float32") = model_params[155]
            lv1702 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1092, lv382, lv383), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv384: R.Tensor((5120, 1280), dtype="float32") = model_params[467]
            lv385: R.Tensor((1280,), dtype="float32") = model_params[156]
            lv1703 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1702, lv384, lv385, lv1701), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv386: R.Tensor((1280,), dtype="float32") = model_params[162]
            lv387: R.Tensor((1280,), dtype="float32") = model_params[161]
            lv1101 = R.call_tir(cls.layer_norm3, (lv1703, lv386, lv387), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv388: R.Tensor((1280, 1280), dtype="float32") = model_params[468]
            lv389: R.Tensor((1280,), dtype="float32") = model_params[169]
            lv1704 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1101, lv388, lv389), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv390: R.Tensor((1280, 1280), dtype="float32") = model_params[469]
            lv391: R.Tensor((1280,), dtype="float32") = model_params[167]
            lv1705 = R.call_tir(cls.fused_matmul29_add42, (lv1101, lv390, lv391), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1706 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1705,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv392: R.Tensor((1280, 1280), dtype="float32") = model_params[470]
            lv393: R.Tensor((1280,), dtype="float32") = model_params[170]
            lv1707 = R.call_tir(cls.fused_matmul29_add42, (lv1101, lv392, lv393), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1708 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1707,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1709 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1704,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1122 = R.call_tir(cls.matmul30, (lv1709, lv1706), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1710 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1122, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1126 = R.call_tir(cls.softmax6, (lv1710,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1711 = R.call_tir(cls.fused_reshape44_reshape45, (lv1126,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1129 = R.call_tir(cls.matmul31, (lv1711, lv1708), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1712 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1129,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv394: R.Tensor((1280, 1280), dtype="float32") = model_params[471]
            lv395: R.Tensor((1280,), dtype="float32") = model_params[168]
            lv1713 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1712, lv394, lv395, lv1703), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv396: R.Tensor((1280,), dtype="float32") = model_params[164]
            lv397: R.Tensor((1280,), dtype="float32") = model_params[163]
            lv1137 = R.call_tir(cls.layer_norm3, (lv1713, lv396, lv397), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv398: R.Tensor((1280, 5120), dtype="float32") = model_params[472]
            lv399: R.Tensor((5120,), dtype="float32") = model_params[165]
            lv1714 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1137, lv398, lv399), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv400: R.Tensor((5120, 1280), dtype="float32") = model_params[473]
            lv401: R.Tensor((1280,), dtype="float32") = model_params[166]
            lv1715 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1714, lv400, lv401, lv1713), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv402_1: R.Tensor((1280,), dtype="float32") = model_params[172]
            lv403: R.Tensor((1280,), dtype="float32") = model_params[171]
            lv1146 = R.call_tir(cls.layer_norm3, (lv1715, lv402_1, lv403), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv404: R.Tensor((1280, 1280), dtype="float32") = model_params[474]
            lv405: R.Tensor((1280,), dtype="float32") = model_params[179]
            lv1716 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1146, lv404, lv405), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv406_1: R.Tensor((1280, 1280), dtype="float32") = model_params[475]
            lv407: R.Tensor((1280,), dtype="float32") = model_params[177]
            lv1717 = R.call_tir(cls.fused_matmul29_add42, (lv1146, lv406_1, lv407), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1718 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1717,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv408: R.Tensor((1280, 1280), dtype="float32") = model_params[476]
            lv409_1: R.Tensor((1280,), dtype="float32") = model_params[180]
            lv1719 = R.call_tir(cls.fused_matmul29_add42, (lv1146, lv408, lv409_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1720 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1719,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1721 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1716,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1167 = R.call_tir(cls.matmul30, (lv1721, lv1718), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1722 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1167, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1171 = R.call_tir(cls.softmax6, (lv1722,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1723 = R.call_tir(cls.fused_reshape44_reshape45, (lv1171,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1174 = R.call_tir(cls.matmul31, (lv1723, lv1720), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1724 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1174,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv410: R.Tensor((1280, 1280), dtype="float32") = model_params[477]
            lv411: R.Tensor((1280,), dtype="float32") = model_params[178]
            lv1725 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1724, lv410, lv411, lv1715), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv412: R.Tensor((1280,), dtype="float32") = model_params[174]
            lv413: R.Tensor((1280,), dtype="float32") = model_params[173]
            lv1182 = R.call_tir(cls.layer_norm3, (lv1725, lv412, lv413), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv414: R.Tensor((1280, 5120), dtype="float32") = model_params[478]
            lv415: R.Tensor((5120,), dtype="float32") = model_params[175]
            lv1726 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1182, lv414, lv415), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv416: R.Tensor((5120, 1280), dtype="float32") = model_params[479]
            lv417_1: R.Tensor((1280,), dtype="float32") = model_params[176]
            lv1727 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1726, lv416, lv417_1, lv1725), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv418: R.Tensor((1280,), dtype="float32") = model_params[182]
            lv419: R.Tensor((1280,), dtype="float32") = model_params[181]
            lv1191 = R.call_tir(cls.layer_norm3, (lv1727, lv418, lv419), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv420: R.Tensor((1280, 1280), dtype="float32") = model_params[480]
            lv421: R.Tensor((1280,), dtype="float32") = model_params[189]
            lv1728 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1191, lv420, lv421), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv422: R.Tensor((1280, 1280), dtype="float32") = model_params[481]
            lv423: R.Tensor((1280,), dtype="float32") = model_params[187]
            lv1729 = R.call_tir(cls.fused_matmul29_add42, (lv1191, lv422, lv423), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1730 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1729,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv424: R.Tensor((1280, 1280), dtype="float32") = model_params[482]
            lv425: R.Tensor((1280,), dtype="float32") = model_params[190]
            lv1731 = R.call_tir(cls.fused_matmul29_add42, (lv1191, lv424, lv425), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1732 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1731,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1733 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1728,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1212 = R.call_tir(cls.matmul30, (lv1733, lv1730), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1734 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1212, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1216 = R.call_tir(cls.softmax6, (lv1734,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1735 = R.call_tir(cls.fused_reshape44_reshape45, (lv1216,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1219 = R.call_tir(cls.matmul31, (lv1735, lv1732), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1736 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1219,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv426_1: R.Tensor((1280, 1280), dtype="float32") = model_params[483]
            lv427: R.Tensor((1280,), dtype="float32") = model_params[188]
            lv1737 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1736, lv426_1, lv427, lv1727), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv428: R.Tensor((1280,), dtype="float32") = model_params[184]
            lv429: R.Tensor((1280,), dtype="float32") = model_params[183]
            lv1227 = R.call_tir(cls.layer_norm3, (lv1737, lv428, lv429), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv430: R.Tensor((1280, 5120), dtype="float32") = model_params[484]
            lv431: R.Tensor((5120,), dtype="float32") = model_params[185]
            lv1738 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1227, lv430, lv431), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv432: R.Tensor((5120, 1280), dtype="float32") = model_params[485]
            lv433: R.Tensor((1280,), dtype="float32") = model_params[186]
            lv1739 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1738, lv432, lv433, lv1737), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv434: R.Tensor((1280,), dtype="float32") = model_params[192]
            lv435: R.Tensor((1280,), dtype="float32") = model_params[191]
            lv1236 = R.call_tir(cls.layer_norm3, (lv1739, lv434, lv435), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv436: R.Tensor((1280, 1280), dtype="float32") = model_params[486]
            lv437: R.Tensor((1280,), dtype="float32") = model_params[199]
            lv1740 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1236, lv436, lv437), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv438: R.Tensor((1280, 1280), dtype="float32") = model_params[487]
            lv439: R.Tensor((1280,), dtype="float32") = model_params[197]
            lv1741 = R.call_tir(cls.fused_matmul29_add42, (lv1236, lv438, lv439), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1742 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1741,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv440: R.Tensor((1280, 1280), dtype="float32") = model_params[488]
            lv441: R.Tensor((1280,), dtype="float32") = model_params[200]
            lv1743 = R.call_tir(cls.fused_matmul29_add42, (lv1236, lv440, lv441), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1744 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1743,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1745 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1740,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1257 = R.call_tir(cls.matmul30, (lv1745, lv1742), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1746 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1257, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1261 = R.call_tir(cls.softmax6, (lv1746,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1747 = R.call_tir(cls.fused_reshape44_reshape45, (lv1261,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1264 = R.call_tir(cls.matmul31, (lv1747, lv1744), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1748 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1264,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv442: R.Tensor((1280, 1280), dtype="float32") = model_params[489]
            lv443: R.Tensor((1280,), dtype="float32") = model_params[198]
            lv1749 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1748, lv442, lv443, lv1739), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv444: R.Tensor((1280,), dtype="float32") = model_params[194]
            lv445: R.Tensor((1280,), dtype="float32") = model_params[193]
            lv1272 = R.call_tir(cls.layer_norm3, (lv1749, lv444, lv445), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv446: R.Tensor((1280, 5120), dtype="float32") = model_params[490]
            lv447_1: R.Tensor((5120,), dtype="float32") = model_params[195]
            lv1750 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1272, lv446, lv447_1), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv448: R.Tensor((5120, 1280), dtype="float32") = model_params[491]
            lv449: R.Tensor((1280,), dtype="float32") = model_params[196]
            lv1751 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1750, lv448, lv449, lv1749), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv450: R.Tensor((1280,), dtype="float32") = model_params[202]
            lv451_1: R.Tensor((1280,), dtype="float32") = model_params[201]
            lv1281 = R.call_tir(cls.layer_norm3, (lv1751, lv450, lv451_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv452: R.Tensor((1280, 1280), dtype="float32") = model_params[492]
            lv453: R.Tensor((1280,), dtype="float32") = model_params[209]
            lv1752 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1281, lv452, lv453), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv454_1: R.Tensor((1280, 1280), dtype="float32") = model_params[493]
            lv455: R.Tensor((1280,), dtype="float32") = model_params[207]
            lv1753 = R.call_tir(cls.fused_matmul29_add42, (lv1281, lv454_1, lv455), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1754 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1753,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv456: R.Tensor((1280, 1280), dtype="float32") = model_params[494]
            lv457: R.Tensor((1280,), dtype="float32") = model_params[210]
            lv1755 = R.call_tir(cls.fused_matmul29_add42, (lv1281, lv456, lv457), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1756 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1755,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1757 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1752,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1302 = R.call_tir(cls.matmul30, (lv1757, lv1754), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1758 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1302, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1306 = R.call_tir(cls.softmax6, (lv1758,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1759 = R.call_tir(cls.fused_reshape44_reshape45, (lv1306,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1309 = R.call_tir(cls.matmul31, (lv1759, lv1756), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1760 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1309,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv458: R.Tensor((1280, 1280), dtype="float32") = model_params[495]
            lv459: R.Tensor((1280,), dtype="float32") = model_params[208]
            lv1761 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1760, lv458, lv459, lv1751), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv460: R.Tensor((1280,), dtype="float32") = model_params[204]
            lv461: R.Tensor((1280,), dtype="float32") = model_params[203]
            lv1317 = R.call_tir(cls.layer_norm3, (lv1761, lv460, lv461), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv462_1: R.Tensor((1280, 5120), dtype="float32") = model_params[496]
            lv463: R.Tensor((5120,), dtype="float32") = model_params[205]
            lv1762 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1317, lv462_1, lv463), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv464: R.Tensor((5120, 1280), dtype="float32") = model_params[497]
            lv465: R.Tensor((1280,), dtype="float32") = model_params[206]
            lv1763 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1762, lv464, lv465, lv1761), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv466: R.Tensor((1280,), dtype="float32") = model_params[212]
            lv467: R.Tensor((1280,), dtype="float32") = model_params[211]
            lv1326 = R.call_tir(cls.layer_norm3, (lv1763, lv466, lv467), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv468: R.Tensor((1280, 1280), dtype="float32") = model_params[498]
            lv469: R.Tensor((1280,), dtype="float32") = model_params[219]
            lv1764 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1326, lv468, lv469), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv470: R.Tensor((1280, 1280), dtype="float32") = model_params[499]
            lv471_1: R.Tensor((1280,), dtype="float32") = model_params[217]
            lv1765 = R.call_tir(cls.fused_matmul29_add42, (lv1326, lv470, lv471_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1766 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1765,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv472: R.Tensor((1280, 1280), dtype="float32") = model_params[500]
            lv473: R.Tensor((1280,), dtype="float32") = model_params[220]
            lv1767 = R.call_tir(cls.fused_matmul29_add42, (lv1326, lv472, lv473), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1768 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1767,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1769 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1764,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1347 = R.call_tir(cls.matmul30, (lv1769, lv1766), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1770 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1347, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1351 = R.call_tir(cls.softmax6, (lv1770,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1771 = R.call_tir(cls.fused_reshape44_reshape45, (lv1351,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1354 = R.call_tir(cls.matmul31, (lv1771, lv1768), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1772 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1354,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv474: R.Tensor((1280, 1280), dtype="float32") = model_params[501]
            lv475: R.Tensor((1280,), dtype="float32") = model_params[218]
            lv1773 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1772, lv474, lv475, lv1763), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv476: R.Tensor((1280,), dtype="float32") = model_params[214]
            lv477: R.Tensor((1280,), dtype="float32") = model_params[213]
            lv1362 = R.call_tir(cls.layer_norm3, (lv1773, lv476, lv477), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv478: R.Tensor((1280, 5120), dtype="float32") = model_params[502]
            lv479: R.Tensor((5120,), dtype="float32") = model_params[215]
            lv1774 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1362, lv478, lv479), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv480: R.Tensor((5120, 1280), dtype="float32") = model_params[503]
            lv481: R.Tensor((1280,), dtype="float32") = model_params[216]
            lv1775 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1774, lv480, lv481, lv1773), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv482: R.Tensor((1280,), dtype="float32") = model_params[232]
            lv483: R.Tensor((1280,), dtype="float32") = model_params[231]
            lv1371 = R.call_tir(cls.layer_norm3, (lv1775, lv482, lv483), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv484: R.Tensor((1280, 1280), dtype="float32") = model_params[504]
            lv485: R.Tensor((1280,), dtype="float32") = model_params[239]
            lv1776 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1371, lv484, lv485), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv486: R.Tensor((1280, 1280), dtype="float32") = model_params[505]
            lv487: R.Tensor((1280,), dtype="float32") = model_params[237]
            lv1777 = R.call_tir(cls.fused_matmul29_add42, (lv1371, lv486, lv487), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1778 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1777,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv488: R.Tensor((1280, 1280), dtype="float32") = model_params[506]
            lv489: R.Tensor((1280,), dtype="float32") = model_params[240]
            lv1779 = R.call_tir(cls.fused_matmul29_add42, (lv1371, lv488, lv489), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1780 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1779,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1781 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1776,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1392 = R.call_tir(cls.matmul30, (lv1781, lv1778), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1782 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1392, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1396 = R.call_tir(cls.softmax6, (lv1782,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1783 = R.call_tir(cls.fused_reshape44_reshape45, (lv1396,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1399 = R.call_tir(cls.matmul31, (lv1783, lv1780), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1784 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1399,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv490: R.Tensor((1280, 1280), dtype="float32") = model_params[507]
            lv491: R.Tensor((1280,), dtype="float32") = model_params[238]
            lv1785 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1784, lv490, lv491, lv1775), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv492_1: R.Tensor((1280,), dtype="float32") = model_params[234]
            lv493: R.Tensor((1280,), dtype="float32") = model_params[233]
            lv1407 = R.call_tir(cls.layer_norm3, (lv1785, lv492_1, lv493), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv494: R.Tensor((1280, 5120), dtype="float32") = model_params[508]
            lv495: R.Tensor((5120,), dtype="float32") = model_params[235]
            lv1786 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1407, lv494, lv495), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv496_1: R.Tensor((5120, 1280), dtype="float32") = model_params[509]
            lv497: R.Tensor((1280,), dtype="float32") = model_params[236]
            lv1787 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1786, lv496_1, lv497, lv1785), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv498: R.Tensor((1280,), dtype="float32") = model_params[242]
            lv499_1: R.Tensor((1280,), dtype="float32") = model_params[241]
            lv1416_1 = R.call_tir(cls.layer_norm3, (lv1787, lv498, lv499_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv500: R.Tensor((1280, 1280), dtype="float32") = model_params[510]
            lv501: R.Tensor((1280,), dtype="float32") = model_params[249]
            lv1788 = R.call_tir(cls.fused_matmul29_add42_multiply19, (lv1416_1, lv500, lv501), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv502: R.Tensor((1280, 1280), dtype="float32") = model_params[511]
            lv503: R.Tensor((1280,), dtype="float32") = model_params[247]
            lv1789 = R.call_tir(cls.fused_matmul29_add42, (lv1416_1, lv502, lv503), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1790 = R.call_tir(cls.fused_reshape20_transpose21_reshape43_transpose39, (lv1789,), out_sinfo=R.Tensor((20, 64, 77), dtype="float32"))
            lv504: R.Tensor((1280, 1280), dtype="float32") = model_params[512]
            lv505: R.Tensor((1280,), dtype="float32") = model_params[250]
            lv1791 = R.call_tir(cls.fused_matmul29_add42, (lv1416_1, lv504, lv505), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1792 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1791,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1793 = R.call_tir(cls.fused_reshape20_transpose21_reshape43, (lv1788,), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1437_1 = R.call_tir(cls.matmul30, (lv1793, lv1790), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1794 = R.call_tir(cls.fused_reshape44_add43_reshape45, (lv1437_1, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1441_1 = R.call_tir(cls.softmax6, (lv1794,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1795 = R.call_tir(cls.fused_reshape44_reshape45, (lv1441_1,), out_sinfo=R.Tensor((20, 77, 77), dtype="float32"))
            lv1444_1 = R.call_tir(cls.matmul31, (lv1795, lv1792), out_sinfo=R.Tensor((20, 77, 64), dtype="float32"))
            lv1796 = R.call_tir(cls.fused_reshape46_transpose40_reshape47, (lv1444_1,), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv506: R.Tensor((1280, 1280), dtype="float32") = model_params[513]
            lv507_1: R.Tensor((1280,), dtype="float32") = model_params[248]
            lv1797 = R.call_tir(cls.fused_matmul29_add42_add41, (lv1796, lv506, lv507_1, lv1787), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv508: R.Tensor((1280,), dtype="float32") = model_params[244]
            lv509: R.Tensor((1280,), dtype="float32") = model_params[243]
            lv1452_1 = R.call_tir(cls.layer_norm3, (lv1797, lv508, lv509), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv510: R.Tensor((1280, 5120), dtype="float32") = model_params[514]
            lv511: R.Tensor((5120,), dtype="float32") = model_params[245]
            lv1798 = R.call_tir(cls.fused_matmul32_add44_gelu2, (lv1452_1, lv510, lv511), out_sinfo=R.Tensor((1, 77, 5120), dtype="float32"))
            lv512: R.Tensor((5120, 1280), dtype="float32") = model_params[515]
            lv513: R.Tensor((1280,), dtype="float32") = model_params[246]
            lv1799 = R.call_tir(cls.fused_matmul33_add42_add41, (lv1798, lv512, lv513, lv1797), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv514: R.Tensor((1280,), dtype="float32") = model_params[322]
            lv515: R.Tensor((1280,), dtype="float32") = model_params[321]
            lv1461_1 = R.call_tir(cls.layer_norm3, (lv1799, lv514, lv515), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1462_1 = R.call_tir(cls.squeeze, (lv1461_1,), out_sinfo=R.Tensor((77, 1280), dtype="float32"))
            lv1463_1 = R.call_tir(cls.cast3, (lv,), out_sinfo=R.Tensor((1, 77), dtype="int32"))
            lv1464_1 = R.call_tir(cls.argmax, (lv1463_1,), out_sinfo=R.Tensor((1,), dtype="int64"))
            lv1465_1 = R.call_tir(cls.take4, (lv1462_1, lv1464_1), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1800 = R.call_tir(cls.fused_strided_slice7_reshape48, (lv1465_1,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv516_1: R.Tensor((1280, 1280), dtype="float32") = model_params[516]
            lv1469_1 = R.call_tir(cls.matmul1, (lv1800, lv516_1), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 77, 1280), dtype="float32"), R.Tensor((1, 1280), dtype="float32")) = lv1787, lv1469_1
            R.output(gv)
        return gv

    @R.function
    def concat_embeddings(cond_embeddings: R.Tensor((1, 77, 2048), dtype="float32"), uncond_embeddings: R.Tensor((1, 77, 2048), dtype="float32")) -> R.Tensor((2, 77, 2048), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.concatenate11, (cond_embeddings, uncond_embeddings), out_sinfo=R.Tensor((2, 77, 2048), dtype="float32"))
        return gv

    @R.function
    def concat_enocder_outputs(cond_embeddings: R.Tensor((1, 77, 768), dtype="float32"), uncond_embeddings: R.Tensor((1, 77, 1280), dtype="float32")) -> R.Tensor((1, 77, 2048), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.concatenate, (cond_embeddings, uncond_embeddings), out_sinfo=R.Tensor((1, 77, 2048), dtype="float32"))
        return gv

    @R.function
    def concat_pool_embeddings(cond_embeddings: R.Tensor((1, 1280), dtype="float32"), uncond_embeddings: R.Tensor((1, 1280), dtype="float32")) -> R.Tensor((2, 1280), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.concatenate13, (cond_embeddings, uncond_embeddings), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
        return gv

    @R.function
    def euler_ancestral_discrete_scheduler_scale(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), sigma: R.Tensor((), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.power, (sigma,), out_sinfo=R.Tensor((), dtype="float32"))
        gv1 = R.call_tir(cls.add35, (gv,), out_sinfo=R.Tensor((), dtype="float32"))
        gv2 = R.call_tir(cls.power1, (gv1,), out_sinfo=R.Tensor((), dtype="float32"))
        scaled_latent_model_input = R.call_tir(cls.divide11, (sample, gv2), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return scaled_latent_model_input

    @R.function
    def euler_ancestral_discrete_scheduler_step(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sigma: R.Tensor((), dtype="float32"), sigma_1: R.Tensor((), dtype="float32"), noise: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.multiply13, (sigma, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv1 = R.call_tir(cls.subtract, (sample, gv), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv2 = R.call_tir(cls.subtract, (sample, gv1), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv3 = R.call_tir(cls.divide11, (gv2, sigma), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv4 = R.call_tir(cls.power, (sigma_1,), out_sinfo=R.Tensor((), dtype="float32"))
        gv5 = R.call_tir(cls.power, (sigma_1,), out_sinfo=R.Tensor((), dtype="float32"))
        gv6 = R.call_tir(cls.power, (sigma,), out_sinfo=R.Tensor((), dtype="float32"))
        gv7 = R.call_tir(cls.power, (sigma_1,), out_sinfo=R.Tensor((), dtype="float32"))
        gv8 = R.call_tir(cls.subtract1, (gv6, gv7), out_sinfo=R.Tensor((), dtype="float32"))
        gv9 = R.call_tir(cls.multiply14, (gv5, gv8), out_sinfo=R.Tensor((), dtype="float32"))
        gv10 = R.call_tir(cls.power, (sigma,), out_sinfo=R.Tensor((), dtype="float32"))
        gv11 = R.call_tir(cls.divide12, (gv9, gv10), out_sinfo=R.Tensor((), dtype="float32"))
        gv12 = R.call_tir(cls.power1, (gv11,), out_sinfo=R.Tensor((), dtype="float32"))
        gv13 = R.call_tir(cls.power, (gv12,), out_sinfo=R.Tensor((), dtype="float32"))
        gv14 = R.call_tir(cls.subtract1, (gv4, gv13), out_sinfo=R.Tensor((), dtype="float32"))
        gv15 = R.call_tir(cls.power1, (gv14,), out_sinfo=R.Tensor((), dtype="float32"))
        gv16 = R.call_tir(cls.subtract1, (gv15, sigma), out_sinfo=R.Tensor((), dtype="float32"))
        gv17 = R.call_tir(cls.multiply15, (gv3, gv16), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv18 = R.call_tir(cls.add34, (sample, gv17), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv19 = R.call_tir(cls.multiply15, (noise, gv12), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample = R.call_tir(cls.add34, (gv18, gv19), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample

    @R.function
    def image_to_rgba(x: R.Tensor((1, 512, 512, 3), dtype="float32")) -> R.Tensor((512, 512), dtype="uint32"):
        cls = Module
        gv = R.call_tir(cls.tir_image_to_rgba, (x,), out_sinfo=R.Tensor((512, 512), dtype="uint32"))
        return gv

    @R.function
    def unet(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="int32"), inp_2: R.Tensor((1, 77, 2048), dtype="float32"), inp_3: R.Tensor((1, 1280), dtype="float32"), inp_4: R.Tensor((1, 6), dtype="float32"), model_params: R.Tuple(R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320, 4, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((4, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 640, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((10240,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1920, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1920, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 1920, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1920, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 1280, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 960, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 960, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((320, 960, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 960, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2816, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((2048, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 10240), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((2048, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 5120), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"))) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 5})
        cls = Module
        with R.dataflow():
            lv77 = R.call_tir(cls.fused_broadcast_to_strided_slice_reshape_cast_multiply_multiply1_tir_sin_tir_cos_concatenate1_strided_slice1_reshape1_strided_slice2_reshape1_concatenate1_cast1, (inp_1, metadata["relax.expr.Constant"][2]), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv517: R.Tensor((320, 1280), dtype="float32") = model_params[886]
            lv518: R.Tensor((1280,), dtype="float32") = model_params[426]
            lv78 = R.call_tir(cls.fused_matmul_add_silu, (lv77, lv517, lv518), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv79 = R.call_tir(cls.fused_reshape2_strided_slice3_reshape3_cast2_multiply2_multiply3_tir_sin1_tir_cos1_concatenate2_strided_slice4_reshape4_strided_slice5_reshape4_concatenate2_reshape5_concatenate3, (inp_4, metadata["relax.expr.Constant"][3], inp_3), out_sinfo=R.Tensor((1, 2816), dtype="float32"))
            lv519: R.Tensor((2816, 1280), dtype="float32") = model_params[888]
            lv520: R.Tensor((1280,), dtype="float32") = model_params[0]
            lv80 = R.call_tir(cls.fused_matmul2_add_silu, (lv79, lv519, lv520), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv521: R.Tensor((1280, 1280), dtype="float32") = model_params[889]
            lv522: R.Tensor((1280,), dtype="float32") = model_params[1]
            lv81 = R.call_tir(cls.fused_matmul1_add, (lv80, lv521, lv522), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv523: R.Tensor((1280, 1280), dtype="float32") = model_params[887]
            lv524: R.Tensor((1280,), dtype="float32") = model_params[427]
            lv82 = R.call_tir(cls.fused_matmul1_add_add1, (lv78, lv523, lv524, lv81), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv525: R.Tensor((320, 4, 3, 3), dtype="float32") = model_params[2]
            lv526: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[890]
            lv83 = R.call_tir(cls.fused_conv2d_add2, (inp_0, lv525, lv526), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv527: R.Tensor((320,), dtype="float32") = model_params[10]
            lv528: R.Tensor((320,), dtype="float32") = model_params[9]
            lv84 = R.call_tir(cls.fused_group_norm_silu1, (lv83, lv527, lv528), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv54 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv529: R.Tensor((1280, 320), dtype="float32") = model_params[892]
            lv530: R.Tensor((320,), dtype="float32") = model_params[13]
            lv85 = R.call_tir(cls.fused_matmul3_add3_cast1, (lv54, lv529, lv530), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv59 = R.call_tir(cls.reshape7, (lv85,), out_sinfo=R.Tensor((1, 320, 1, 1), dtype="float32"))
            lv531: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[7]
            lv532: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[891]
            lv86 = R.call_tir(cls.fused_conv2d1_add2_add2, (lv84, lv531, lv532, lv59), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv533: R.Tensor((320,), dtype="float32") = model_params[12]
            lv534: R.Tensor((320,), dtype="float32") = model_params[11]
            lv87 = R.call_tir(cls.fused_group_norm_silu1, (lv86, lv533, lv534), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv535: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[8]
            lv536: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[893]
            lv88 = R.call_tir(cls.fused_conv2d1_add2_add4_divide, (lv87, lv535, lv536, lv83), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv537: R.Tensor((320,), dtype="float32") = model_params[17]
            lv538: R.Tensor((320,), dtype="float32") = model_params[16]
            lv89 = R.call_tir(cls.fused_group_norm_silu1, (lv88, lv537, lv538), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv73 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv539: R.Tensor((1280, 320), dtype="float32") = model_params[895]
            lv540: R.Tensor((320,), dtype="float32") = model_params[20]
            lv90 = R.call_tir(cls.fused_matmul3_add3_cast1, (lv73, lv539, lv540), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv78_1 = R.call_tir(cls.reshape7, (lv90,), out_sinfo=R.Tensor((1, 320, 1, 1), dtype="float32"))
            lv541: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[14]
            lv542: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[894]
            lv91 = R.call_tir(cls.fused_conv2d1_add2_add2, (lv89, lv541, lv542, lv78_1), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv543: R.Tensor((320,), dtype="float32") = model_params[19]
            lv544: R.Tensor((320,), dtype="float32") = model_params[18]
            lv92 = R.call_tir(cls.fused_group_norm_silu1, (lv91, lv543, lv544), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv545: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[15]
            lv546: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[896]
            lv93 = R.call_tir(cls.fused_conv2d1_add2_add4_divide, (lv92, lv545, lv546, lv88), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv547: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[6]
            lv548: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[897]
            lv94 = R.call_tir(cls.fused_conv2d2_add5, (lv93, lv547, lv548), out_sinfo=R.Tensor((1, 320, 32, 32), dtype="float32"))
            lv549: R.Tensor((320,), dtype="float32") = model_params[74]
            lv550: R.Tensor((320,), dtype="float32") = model_params[73]
            lv95 = R.call_tir(cls.fused_group_norm1_silu2, (lv94, lv549, lv550), out_sinfo=R.Tensor((1, 320, 32, 32), dtype="float32"))
            lv95_1 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv551: R.Tensor((1280, 640), dtype="float32") = model_params[899]
            lv552: R.Tensor((640,), dtype="float32") = model_params[77]
            lv96 = R.call_tir(cls.fused_matmul4_add7_strided_slice6, (lv95_1, lv551, lv552), out_sinfo=R.Tensor((1, 640), dtype="float32"))
            lv100 = R.call_tir(cls.reshape9, (lv96,), out_sinfo=R.Tensor((1, 640, 1, 1), dtype="float32"))
            lv553: R.Tensor((640, 320, 3, 3), dtype="float32") = model_params[70]
            lv554: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[898]
            lv97 = R.call_tir(cls.fused_conv2d3_add6_add6, (lv95, lv553, lv554, lv100), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv555: R.Tensor((640,), dtype="float32") = model_params[76]
            lv556: R.Tensor((640,), dtype="float32") = model_params[75]
            lv98 = R.call_tir(cls.fused_group_norm2_silu3, (lv97, lv555, lv556), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv557: R.Tensor((640, 320, 1, 1), dtype="float32") = model_params[72]
            lv558: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[901]
            lv99 = R.call_tir(cls.fused_conv2d5_add6, (lv94, lv557, lv558), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv559: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[71]
            lv560: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[900]
            lv100_1 = R.call_tir(cls.fused_conv2d4_add6_add8_divide1, (lv98, lv559, lv560, lv99), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv561: R.Tensor((640,), dtype="float32") = model_params[22]
            lv562: R.Tensor((640,), dtype="float32") = model_params[21]
            lv112 = R.call_tir(cls.group_norm3, (lv100_1, lv561, lv562), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv101 = R.call_tir(cls.fused_transpose5_reshape10, (lv112,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv563: R.Tensor((640, 640), dtype="float32") = model_params[902]
            lv564: R.Tensor((640,), dtype="float32") = model_params[23]
            lv102 = R.call_tir(cls.fused_matmul5_add9, (lv101, lv563, lv564), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv565: R.Tensor((640,), dtype="float32") = model_params[30]
            lv566: R.Tensor((640,), dtype="float32") = model_params[29]
            lv118 = R.call_tir(cls.layer_norm, (lv102, lv565, lv566), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv567: R.Tensor((640, 640), dtype="float32") = model_params[903]
            lv120 = R.call_tir(cls.matmul5, (lv118, lv567), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv568: R.Tensor((640, 640), dtype="float32") = model_params[904]
            lv122 = R.call_tir(cls.matmul5, (lv118, lv568), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv569: R.Tensor((640, 640), dtype="float32") = model_params[905]
            lv124 = R.call_tir(cls.matmul5, (lv118, lv569), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv103 = R.call_tir(cls.fused_reshape11_transpose7, (lv120,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv104 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv122,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv105 = R.call_tir(cls.fused_reshape11_transpose7, (lv124,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv106 = R.call_tir(cls.fused_matmul6_multiply4, (lv103, lv104, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv136 = R.call_tir(cls.softmax, (lv106,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv137 = R.call_tir(cls.matmul7, (lv136, lv105), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv107 = R.call_tir(cls.fused_transpose9_reshape12, (lv137,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv570: R.Tensor((640, 640), dtype="float32") = model_params[906]
            lv571: R.Tensor((640,), dtype="float32") = model_params[25]
            lv108 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv107, lv570, lv571, lv102), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv572: R.Tensor((640,), dtype="float32") = model_params[32]
            lv573: R.Tensor((640,), dtype="float32") = model_params[31]
            lv145 = R.call_tir(cls.layer_norm, (lv108, lv572, lv573), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv574: R.Tensor((640, 640), dtype="float32") = model_params[907]
            lv147 = R.call_tir(cls.matmul5, (lv145, lv574), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv575: R.Tensor((2048, 640), dtype="float32") = model_params[908]
            lv149 = R.call_tir(cls.matmul8, (inp_2, lv575), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv576: R.Tensor((2048, 640), dtype="float32") = model_params[909]
            lv151 = R.call_tir(cls.matmul8, (inp_2, lv576), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv109 = R.call_tir(cls.fused_reshape11_transpose7, (lv147,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv110 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv149,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv111 = R.call_tir(cls.fused_reshape13_transpose11, (lv151,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv112_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv109, lv110, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv163 = R.call_tir(cls.softmax1, (lv112_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv164 = R.call_tir(cls.matmul10, (lv163, lv111), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv113 = R.call_tir(cls.fused_transpose9_reshape12, (lv164,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv577: R.Tensor((640, 640), dtype="float32") = model_params[910]
            lv578: R.Tensor((640,), dtype="float32") = model_params[26]
            lv114 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv113, lv577, lv578, lv108), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv579: R.Tensor((640,), dtype="float32") = model_params[34]
            lv580: R.Tensor((640,), dtype="float32") = model_params[33]
            lv172 = R.call_tir(cls.layer_norm, (lv114, lv579, lv580), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv581: R.Tensor((640, 5120), dtype="float32") = model_params[911]
            lv582: R.Tensor((5120,), dtype="float32") = model_params[27]
            lv115 = R.call_tir(cls.fused_matmul11_add11, (lv172, lv581, lv582), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv116 = R.call_tir(cls.fused_split_gelu_multiply6, (lv115,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv583: R.Tensor((2560, 640), dtype="float32") = model_params[912]
            lv584: R.Tensor((640,), dtype="float32") = model_params[28]
            lv117 = R.call_tir(cls.fused_matmul12_add9_add10, (lv116, lv583, lv584, lv114), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv585: R.Tensor((640,), dtype="float32") = model_params[40]
            lv586: R.Tensor((640,), dtype="float32") = model_params[39]
            lv185 = R.call_tir(cls.layer_norm, (lv117, lv585, lv586), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv587: R.Tensor((640, 640), dtype="float32") = model_params[913]
            lv187 = R.call_tir(cls.matmul5, (lv185, lv587), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv588: R.Tensor((640, 640), dtype="float32") = model_params[914]
            lv189 = R.call_tir(cls.matmul5, (lv185, lv588), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv589: R.Tensor((640, 640), dtype="float32") = model_params[915]
            lv191 = R.call_tir(cls.matmul5, (lv185, lv589), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv118_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv187,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv119 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv189,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv120_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv191,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv121 = R.call_tir(cls.fused_matmul6_multiply4, (lv118_1, lv119, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv203 = R.call_tir(cls.softmax, (lv121,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv204 = R.call_tir(cls.matmul7, (lv203, lv120_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv122_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv204,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv590: R.Tensor((640, 640), dtype="float32") = model_params[916]
            lv591: R.Tensor((640,), dtype="float32") = model_params[35]
            lv123 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv122_1, lv590, lv591, lv117), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv592: R.Tensor((640,), dtype="float32") = model_params[42]
            lv593: R.Tensor((640,), dtype="float32") = model_params[41]
            lv212 = R.call_tir(cls.layer_norm, (lv123, lv592, lv593), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv594: R.Tensor((640, 640), dtype="float32") = model_params[917]
            lv214 = R.call_tir(cls.matmul5, (lv212, lv594), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv595: R.Tensor((2048, 640), dtype="float32") = model_params[918]
            lv216 = R.call_tir(cls.matmul8, (inp_2, lv595), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv596: R.Tensor((2048, 640), dtype="float32") = model_params[919]
            lv218 = R.call_tir(cls.matmul8, (inp_2, lv596), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv124_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv214,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv125 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv216,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv126 = R.call_tir(cls.fused_reshape13_transpose11, (lv218,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv127 = R.call_tir(cls.fused_matmul9_multiply5, (lv124_1, lv125, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv230 = R.call_tir(cls.softmax1, (lv127,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv231 = R.call_tir(cls.matmul10, (lv230, lv126), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv128 = R.call_tir(cls.fused_transpose9_reshape12, (lv231,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv597: R.Tensor((640, 640), dtype="float32") = model_params[920]
            lv598: R.Tensor((640,), dtype="float32") = model_params[36]
            lv129 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv128, lv597, lv598, lv123), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv599: R.Tensor((640,), dtype="float32") = model_params[44]
            lv600: R.Tensor((640,), dtype="float32") = model_params[43]
            lv239 = R.call_tir(cls.layer_norm, (lv129, lv599, lv600), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv601: R.Tensor((640, 5120), dtype="float32") = model_params[921]
            lv602: R.Tensor((5120,), dtype="float32") = model_params[37]
            lv130 = R.call_tir(cls.fused_matmul11_add11, (lv239, lv601, lv602), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv131 = R.call_tir(cls.fused_split_gelu_multiply6, (lv130,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv603: R.Tensor((2560, 640), dtype="float32") = model_params[922]
            lv604: R.Tensor((640,), dtype="float32") = model_params[38]
            lv132 = R.call_tir(cls.fused_matmul12_add9_add10, (lv131, lv603, lv604, lv129), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv605: R.Tensor((640, 640), dtype="float32") = model_params[923]
            lv606: R.Tensor((640,), dtype="float32") = model_params[24]
            lv133 = R.call_tir(cls.fused_matmul5_add9, (lv132, lv605, lv606), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv134 = R.call_tir(cls.fused_reshape14_transpose15_add8, (lv133, lv100_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv607: R.Tensor((640,), dtype="float32") = model_params[81]
            lv608: R.Tensor((640,), dtype="float32") = model_params[80]
            lv135 = R.call_tir(cls.fused_group_norm2_silu3, (lv134, lv607, lv608), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv263 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv609: R.Tensor((1280, 640), dtype="float32") = model_params[925]
            lv610: R.Tensor((640,), dtype="float32") = model_params[84]
            lv136_1 = R.call_tir(cls.fused_matmul4_add7_strided_slice6, (lv263, lv609, lv610), out_sinfo=R.Tensor((1, 640), dtype="float32"))
            lv268 = R.call_tir(cls.reshape9, (lv136_1,), out_sinfo=R.Tensor((1, 640, 1, 1), dtype="float32"))
            lv611: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[78]
            lv612: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[924]
            lv137_1 = R.call_tir(cls.fused_conv2d4_add6_add6, (lv135, lv611, lv612, lv268), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv613: R.Tensor((640,), dtype="float32") = model_params[83]
            lv614: R.Tensor((640,), dtype="float32") = model_params[82]
            lv138 = R.call_tir(cls.fused_group_norm2_silu3, (lv137_1, lv613, lv614), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv615: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[79]
            lv616: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[926]
            lv139 = R.call_tir(cls.fused_conv2d4_add6_add8_divide1, (lv138, lv615, lv616, lv134), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv617: R.Tensor((640,), dtype="float32") = model_params[46]
            lv618: R.Tensor((640,), dtype="float32") = model_params[45]
            lv277 = R.call_tir(cls.group_norm3, (lv139, lv617, lv618), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv140 = R.call_tir(cls.fused_transpose5_reshape10, (lv277,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv619: R.Tensor((640, 640), dtype="float32") = model_params[927]
            lv620: R.Tensor((640,), dtype="float32") = model_params[47]
            lv141 = R.call_tir(cls.fused_matmul5_add9, (lv140, lv619, lv620), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv621: R.Tensor((640,), dtype="float32") = model_params[54]
            lv622: R.Tensor((640,), dtype="float32") = model_params[53]
            lv283 = R.call_tir(cls.layer_norm, (lv141, lv621, lv622), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv623: R.Tensor((640, 640), dtype="float32") = model_params[928]
            lv285 = R.call_tir(cls.matmul5, (lv283, lv623), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv624: R.Tensor((640, 640), dtype="float32") = model_params[929]
            lv287 = R.call_tir(cls.matmul5, (lv283, lv624), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv625: R.Tensor((640, 640), dtype="float32") = model_params[930]
            lv289 = R.call_tir(cls.matmul5, (lv283, lv625), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv142 = R.call_tir(cls.fused_reshape11_transpose7, (lv285,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv143 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv287,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv144 = R.call_tir(cls.fused_reshape11_transpose7, (lv289,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv145_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv142, lv143, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv301 = R.call_tir(cls.softmax, (lv145_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv302 = R.call_tir(cls.matmul7, (lv301, lv144), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv146 = R.call_tir(cls.fused_transpose9_reshape12, (lv302,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv626: R.Tensor((640, 640), dtype="float32") = model_params[931]
            lv627: R.Tensor((640,), dtype="float32") = model_params[49]
            lv147_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv146, lv626, lv627, lv141), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv628: R.Tensor((640,), dtype="float32") = model_params[56]
            lv629: R.Tensor((640,), dtype="float32") = model_params[55]
            lv310 = R.call_tir(cls.layer_norm, (lv147_1, lv628, lv629), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv630: R.Tensor((640, 640), dtype="float32") = model_params[932]
            lv312 = R.call_tir(cls.matmul5, (lv310, lv630), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv631: R.Tensor((2048, 640), dtype="float32") = model_params[933]
            lv314 = R.call_tir(cls.matmul8, (inp_2, lv631), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv632: R.Tensor((2048, 640), dtype="float32") = model_params[934]
            lv316 = R.call_tir(cls.matmul8, (inp_2, lv632), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv148 = R.call_tir(cls.fused_reshape11_transpose7, (lv312,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv149_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv314,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv150 = R.call_tir(cls.fused_reshape13_transpose11, (lv316,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv151_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv148, lv149_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv328 = R.call_tir(cls.softmax1, (lv151_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv329 = R.call_tir(cls.matmul10, (lv328, lv150), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv152 = R.call_tir(cls.fused_transpose9_reshape12, (lv329,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv633: R.Tensor((640, 640), dtype="float32") = model_params[935]
            lv634: R.Tensor((640,), dtype="float32") = model_params[50]
            lv153 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv152, lv633, lv634, lv147_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv635: R.Tensor((640,), dtype="float32") = model_params[58]
            lv636: R.Tensor((640,), dtype="float32") = model_params[57]
            lv337 = R.call_tir(cls.layer_norm, (lv153, lv635, lv636), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv637: R.Tensor((640, 5120), dtype="float32") = model_params[936]
            lv638: R.Tensor((5120,), dtype="float32") = model_params[51]
            lv154 = R.call_tir(cls.fused_matmul11_add11, (lv337, lv637, lv638), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv155 = R.call_tir(cls.fused_split_gelu_multiply6, (lv154,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv639: R.Tensor((2560, 640), dtype="float32") = model_params[937]
            lv640: R.Tensor((640,), dtype="float32") = model_params[52]
            lv156 = R.call_tir(cls.fused_matmul12_add9_add10, (lv155, lv639, lv640, lv153), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv641: R.Tensor((640,), dtype="float32") = model_params[64]
            lv642: R.Tensor((640,), dtype="float32") = model_params[63]
            lv350 = R.call_tir(cls.layer_norm, (lv156, lv641, lv642), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv643: R.Tensor((640, 640), dtype="float32") = model_params[938]
            lv352 = R.call_tir(cls.matmul5, (lv350, lv643), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv644: R.Tensor((640, 640), dtype="float32") = model_params[939]
            lv354 = R.call_tir(cls.matmul5, (lv350, lv644), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv645: R.Tensor((640, 640), dtype="float32") = model_params[940]
            lv356 = R.call_tir(cls.matmul5, (lv350, lv645), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv157 = R.call_tir(cls.fused_reshape11_transpose7, (lv352,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv158 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv354,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv159 = R.call_tir(cls.fused_reshape11_transpose7, (lv356,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv160 = R.call_tir(cls.fused_matmul6_multiply4, (lv157, lv158, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv368 = R.call_tir(cls.softmax, (lv160,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv369 = R.call_tir(cls.matmul7, (lv368, lv159), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv161 = R.call_tir(cls.fused_transpose9_reshape12, (lv369,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv646: R.Tensor((640, 640), dtype="float32") = model_params[941]
            lv647: R.Tensor((640,), dtype="float32") = model_params[59]
            lv162 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv161, lv646, lv647, lv156), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv648: R.Tensor((640,), dtype="float32") = model_params[66]
            lv649: R.Tensor((640,), dtype="float32") = model_params[65]
            lv377 = R.call_tir(cls.layer_norm, (lv162, lv648, lv649), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv650: R.Tensor((640, 640), dtype="float32") = model_params[942]
            lv379 = R.call_tir(cls.matmul5, (lv377, lv650), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv651: R.Tensor((2048, 640), dtype="float32") = model_params[943]
            lv381 = R.call_tir(cls.matmul8, (inp_2, lv651), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv652: R.Tensor((2048, 640), dtype="float32") = model_params[944]
            lv383 = R.call_tir(cls.matmul8, (inp_2, lv652), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv163_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv379,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv164_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv381,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv165 = R.call_tir(cls.fused_reshape13_transpose11, (lv383,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv166 = R.call_tir(cls.fused_matmul9_multiply5, (lv163_1, lv164_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv395 = R.call_tir(cls.softmax1, (lv166,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv396 = R.call_tir(cls.matmul10, (lv395, lv165), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv167 = R.call_tir(cls.fused_transpose9_reshape12, (lv396,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv653: R.Tensor((640, 640), dtype="float32") = model_params[945]
            lv654: R.Tensor((640,), dtype="float32") = model_params[60]
            lv168 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv167, lv653, lv654, lv162), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv655: R.Tensor((640,), dtype="float32") = model_params[68]
            lv656: R.Tensor((640,), dtype="float32") = model_params[67]
            lv404 = R.call_tir(cls.layer_norm, (lv168, lv655, lv656), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv657: R.Tensor((640, 5120), dtype="float32") = model_params[946]
            lv658: R.Tensor((5120,), dtype="float32") = model_params[61]
            lv169 = R.call_tir(cls.fused_matmul11_add11, (lv404, lv657, lv658), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv170 = R.call_tir(cls.fused_split_gelu_multiply6, (lv169,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv659: R.Tensor((2560, 640), dtype="float32") = model_params[947]
            lv660: R.Tensor((640,), dtype="float32") = model_params[62]
            lv171 = R.call_tir(cls.fused_matmul12_add9_add10, (lv170, lv659, lv660, lv168), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv661: R.Tensor((640, 640), dtype="float32") = model_params[948]
            lv662: R.Tensor((640,), dtype="float32") = model_params[48]
            lv172_1 = R.call_tir(cls.fused_matmul5_add9, (lv171, lv661, lv662), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv173 = R.call_tir(cls.fused_reshape14_transpose15_add8, (lv172_1, lv139), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv663: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[69]
            lv664: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[949]
            lv174 = R.call_tir(cls.fused_conv2d6_add12, (lv173, lv663, lv664), out_sinfo=R.Tensor((1, 640, 16, 16), dtype="float32"))
            lv665: R.Tensor((640,), dtype="float32") = model_params[297]
            lv666: R.Tensor((640,), dtype="float32") = model_params[296]
            lv175 = R.call_tir(cls.fused_group_norm4_silu4, (lv174, lv665, lv666), out_sinfo=R.Tensor((1, 640, 16, 16), dtype="float32"))
            lv431 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv667: R.Tensor((1280, 1280), dtype="float32") = model_params[951]
            lv668: R.Tensor((1280,), dtype="float32") = model_params[300]
            lv176 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv431, lv667, lv668), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv436 = R.call_tir(cls.reshape16, (lv176,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv669: R.Tensor((1280, 640, 3, 3), dtype="float32") = model_params[293]
            lv670: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[950]
            lv177 = R.call_tir(cls.fused_conv2d7_add13_add13, (lv175, lv669, lv670, lv436), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv671: R.Tensor((1280,), dtype="float32") = model_params[299]
            lv672: R.Tensor((1280,), dtype="float32") = model_params[298]
            lv178 = R.call_tir(cls.fused_group_norm5_silu5, (lv177, lv671, lv672), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv673: R.Tensor((1280, 640, 1, 1), dtype="float32") = model_params[295]
            lv674: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[953]
            lv179 = R.call_tir(cls.fused_conv2d9_add13, (lv174, lv673, lv674), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv675: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[294]
            lv676: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[952]
            lv180 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv178, lv675, lv676, lv179), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv677: R.Tensor((1280,), dtype="float32") = model_params[86]
            lv678: R.Tensor((1280,), dtype="float32") = model_params[85]
            lv448 = R.call_tir(cls.group_norm6, (lv180, lv677, lv678), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv181 = R.call_tir(cls.fused_transpose16_reshape17, (lv448,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv679: R.Tensor((1280, 1280), dtype="float32") = model_params[954]
            lv680: R.Tensor((1280,), dtype="float32") = model_params[87]
            lv182 = R.call_tir(cls.fused_matmul13_add15, (lv181, lv679, lv680), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv681: R.Tensor((1280,), dtype="float32") = model_params[94]
            lv682: R.Tensor((1280,), dtype="float32") = model_params[93]
            lv454 = R.call_tir(cls.layer_norm1, (lv182, lv681, lv682), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv683: R.Tensor((1280, 1280), dtype="float32") = model_params[955]
            lv456 = R.call_tir(cls.matmul13, (lv454, lv683), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv684: R.Tensor((1280, 1280), dtype="float32") = model_params[956]
            lv458 = R.call_tir(cls.matmul13, (lv454, lv684), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv685: R.Tensor((1280, 1280), dtype="float32") = model_params[957]
            lv460 = R.call_tir(cls.matmul13, (lv454, lv685), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv183 = R.call_tir(cls.fused_reshape18_transpose17, (lv456,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv184 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv458,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv185_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv460,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv186 = R.call_tir(cls.fused_matmul14_multiply7, (lv183, lv184, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv472 = R.call_tir(cls.softmax2, (lv186,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv473 = R.call_tir(cls.matmul15, (lv472, lv185_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv187_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv473,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv686: R.Tensor((1280, 1280), dtype="float32") = model_params[958]
            lv687: R.Tensor((1280,), dtype="float32") = model_params[89]
            lv188 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv187_1, lv686, lv687, lv182), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv688: R.Tensor((1280,), dtype="float32") = model_params[96]
            lv689: R.Tensor((1280,), dtype="float32") = model_params[95]
            lv481 = R.call_tir(cls.layer_norm1, (lv188, lv688, lv689), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv690: R.Tensor((1280, 1280), dtype="float32") = model_params[959]
            lv483 = R.call_tir(cls.matmul13, (lv481, lv690), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv691: R.Tensor((2048, 1280), dtype="float32") = model_params[960]
            lv485 = R.call_tir(cls.matmul16, (inp_2, lv691), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv692: R.Tensor((2048, 1280), dtype="float32") = model_params[961]
            lv487 = R.call_tir(cls.matmul16, (inp_2, lv692), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv189_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv483,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv190 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv485,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv191_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv487,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv192 = R.call_tir(cls.fused_matmul17_multiply8, (lv189_1, lv190, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv499 = R.call_tir(cls.softmax3, (lv192,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv500 = R.call_tir(cls.matmul18, (lv499, lv191_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv193 = R.call_tir(cls.fused_transpose19_reshape19, (lv500,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv693: R.Tensor((1280, 1280), dtype="float32") = model_params[962]
            lv694: R.Tensor((1280,), dtype="float32") = model_params[90]
            lv194 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv193, lv693, lv694, lv188), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv695: R.Tensor((1280,), dtype="float32") = model_params[98]
            lv696: R.Tensor((1280,), dtype="float32") = model_params[97]
            lv508 = R.call_tir(cls.layer_norm1, (lv194, lv695, lv696), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv697: R.Tensor((1280, 10240), dtype="float32") = model_params[963]
            lv698: R.Tensor((10240,), dtype="float32") = model_params[91]
            lv195 = R.call_tir(cls.fused_matmul19_add17, (lv508, lv697, lv698), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv196 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv195,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv699: R.Tensor((5120, 1280), dtype="float32") = model_params[964]
            lv700: R.Tensor((1280,), dtype="float32") = model_params[92]
            lv197 = R.call_tir(cls.fused_matmul20_add15_add16, (lv196, lv699, lv700, lv194), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv701: R.Tensor((1280,), dtype="float32") = model_params[104]
            lv702: R.Tensor((1280,), dtype="float32") = model_params[103]
            lv521_1 = R.call_tir(cls.layer_norm1, (lv197, lv701, lv702), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv703: R.Tensor((1280, 1280), dtype="float32") = model_params[965]
            lv523_1 = R.call_tir(cls.matmul13, (lv521_1, lv703), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv704: R.Tensor((1280, 1280), dtype="float32") = model_params[966]
            lv525_1 = R.call_tir(cls.matmul13, (lv521_1, lv704), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv705: R.Tensor((1280, 1280), dtype="float32") = model_params[967]
            lv527_1 = R.call_tir(cls.matmul13, (lv521_1, lv705), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv198 = R.call_tir(cls.fused_reshape18_transpose17, (lv523_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv199 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv525_1,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv200 = R.call_tir(cls.fused_reshape18_transpose17, (lv527_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv201 = R.call_tir(cls.fused_matmul14_multiply7, (lv198, lv199, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv539_1 = R.call_tir(cls.softmax2, (lv201,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv540_1 = R.call_tir(cls.matmul15, (lv539_1, lv200), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv202 = R.call_tir(cls.fused_transpose19_reshape19, (lv540_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv706: R.Tensor((1280, 1280), dtype="float32") = model_params[968]
            lv707: R.Tensor((1280,), dtype="float32") = model_params[99]
            lv203_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv202, lv706, lv707, lv197), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv708: R.Tensor((1280,), dtype="float32") = model_params[106]
            lv709: R.Tensor((1280,), dtype="float32") = model_params[105]
            lv548_1 = R.call_tir(cls.layer_norm1, (lv203_1, lv708, lv709), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv710: R.Tensor((1280, 1280), dtype="float32") = model_params[969]
            lv550_1 = R.call_tir(cls.matmul13, (lv548_1, lv710), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv711: R.Tensor((2048, 1280), dtype="float32") = model_params[970]
            lv552_1 = R.call_tir(cls.matmul16, (inp_2, lv711), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv712: R.Tensor((2048, 1280), dtype="float32") = model_params[971]
            lv554_1 = R.call_tir(cls.matmul16, (inp_2, lv712), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv204_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv550_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv205 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv552_1,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv206 = R.call_tir(cls.fused_reshape20_transpose21, (lv554_1,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv207 = R.call_tir(cls.fused_matmul17_multiply8, (lv204_1, lv205, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv566_1 = R.call_tir(cls.softmax3, (lv207,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv567_1 = R.call_tir(cls.matmul18, (lv566_1, lv206), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv208 = R.call_tir(cls.fused_transpose19_reshape19, (lv567_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv713: R.Tensor((1280, 1280), dtype="float32") = model_params[972]
            lv714: R.Tensor((1280,), dtype="float32") = model_params[100]
            lv209 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv208, lv713, lv714, lv203_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv715: R.Tensor((1280,), dtype="float32") = model_params[108]
            lv716: R.Tensor((1280,), dtype="float32") = model_params[107]
            lv575_1 = R.call_tir(cls.layer_norm1, (lv209, lv715, lv716), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv717: R.Tensor((1280, 10240), dtype="float32") = model_params[973]
            lv718: R.Tensor((10240,), dtype="float32") = model_params[101]
            lv210 = R.call_tir(cls.fused_matmul19_add17, (lv575_1, lv717, lv718), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv211 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv210,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv719: R.Tensor((5120, 1280), dtype="float32") = model_params[974]
            lv720: R.Tensor((1280,), dtype="float32") = model_params[102]
            lv212_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv211, lv719, lv720, lv209), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv721: R.Tensor((1280,), dtype="float32") = model_params[114]
            lv722: R.Tensor((1280,), dtype="float32") = model_params[113]
            lv588_1 = R.call_tir(cls.layer_norm1, (lv212_1, lv721, lv722), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv723: R.Tensor((1280, 1280), dtype="float32") = model_params[975]
            lv590_1 = R.call_tir(cls.matmul13, (lv588_1, lv723), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv724: R.Tensor((1280, 1280), dtype="float32") = model_params[976]
            lv592_1 = R.call_tir(cls.matmul13, (lv588_1, lv724), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv725: R.Tensor((1280, 1280), dtype="float32") = model_params[977]
            lv594_1 = R.call_tir(cls.matmul13, (lv588_1, lv725), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv213 = R.call_tir(cls.fused_reshape18_transpose17, (lv590_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv214_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv592_1,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv215 = R.call_tir(cls.fused_reshape18_transpose17, (lv594_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv216_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv213, lv214_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv606_1 = R.call_tir(cls.softmax2, (lv216_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv607_1 = R.call_tir(cls.matmul15, (lv606_1, lv215), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv217 = R.call_tir(cls.fused_transpose19_reshape19, (lv607_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv726: R.Tensor((1280, 1280), dtype="float32") = model_params[978]
            lv727: R.Tensor((1280,), dtype="float32") = model_params[109]
            lv218_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv217, lv726, lv727, lv212_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv728: R.Tensor((1280,), dtype="float32") = model_params[116]
            lv729: R.Tensor((1280,), dtype="float32") = model_params[115]
            lv615_1 = R.call_tir(cls.layer_norm1, (lv218_1, lv728, lv729), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv730: R.Tensor((1280, 1280), dtype="float32") = model_params[979]
            lv617_1 = R.call_tir(cls.matmul13, (lv615_1, lv730), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv731: R.Tensor((2048, 1280), dtype="float32") = model_params[980]
            lv619_1 = R.call_tir(cls.matmul16, (inp_2, lv731), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv732: R.Tensor((2048, 1280), dtype="float32") = model_params[981]
            lv621_1 = R.call_tir(cls.matmul16, (inp_2, lv732), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv219 = R.call_tir(cls.fused_reshape18_transpose17, (lv617_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv220 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv619_1,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv221 = R.call_tir(cls.fused_reshape20_transpose21, (lv621_1,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv222 = R.call_tir(cls.fused_matmul17_multiply8, (lv219, lv220, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv633_1 = R.call_tir(cls.softmax3, (lv222,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv634_1 = R.call_tir(cls.matmul18, (lv633_1, lv221), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv223 = R.call_tir(cls.fused_transpose19_reshape19, (lv634_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv733: R.Tensor((1280, 1280), dtype="float32") = model_params[982]
            lv734: R.Tensor((1280,), dtype="float32") = model_params[110]
            lv224 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv223, lv733, lv734, lv218_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv735: R.Tensor((1280,), dtype="float32") = model_params[118]
            lv736: R.Tensor((1280,), dtype="float32") = model_params[117]
            lv642_1 = R.call_tir(cls.layer_norm1, (lv224, lv735, lv736), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv737: R.Tensor((1280, 10240), dtype="float32") = model_params[983]
            lv738: R.Tensor((10240,), dtype="float32") = model_params[111]
            lv225 = R.call_tir(cls.fused_matmul19_add17, (lv642_1, lv737, lv738), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv226 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv225,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv739: R.Tensor((5120, 1280), dtype="float32") = model_params[984]
            lv740: R.Tensor((1280,), dtype="float32") = model_params[112]
            lv227 = R.call_tir(cls.fused_matmul20_add15_add16, (lv226, lv739, lv740, lv224), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv741: R.Tensor((1280,), dtype="float32") = model_params[124]
            lv742: R.Tensor((1280,), dtype="float32") = model_params[123]
            lv655_1 = R.call_tir(cls.layer_norm1, (lv227, lv741, lv742), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv743: R.Tensor((1280, 1280), dtype="float32") = model_params[985]
            lv657_1 = R.call_tir(cls.matmul13, (lv655_1, lv743), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv744: R.Tensor((1280, 1280), dtype="float32") = model_params[986]
            lv659_1 = R.call_tir(cls.matmul13, (lv655_1, lv744), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv745: R.Tensor((1280, 1280), dtype="float32") = model_params[987]
            lv661_1 = R.call_tir(cls.matmul13, (lv655_1, lv745), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv228 = R.call_tir(cls.fused_reshape18_transpose17, (lv657_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv229 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv659_1,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv230_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv661_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv231_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv228, lv229, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv673_1 = R.call_tir(cls.softmax2, (lv231_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv674_1 = R.call_tir(cls.matmul15, (lv673_1, lv230_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv232 = R.call_tir(cls.fused_transpose19_reshape19, (lv674_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv746: R.Tensor((1280, 1280), dtype="float32") = model_params[988]
            lv747: R.Tensor((1280,), dtype="float32") = model_params[119]
            lv233 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv232, lv746, lv747, lv227), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv748: R.Tensor((1280,), dtype="float32") = model_params[126]
            lv749: R.Tensor((1280,), dtype="float32") = model_params[125]
            lv682_1 = R.call_tir(cls.layer_norm1, (lv233, lv748, lv749), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv750: R.Tensor((1280, 1280), dtype="float32") = model_params[989]
            lv684_1 = R.call_tir(cls.matmul13, (lv682_1, lv750), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv751: R.Tensor((2048, 1280), dtype="float32") = model_params[990]
            lv686_1 = R.call_tir(cls.matmul16, (inp_2, lv751), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv752: R.Tensor((2048, 1280), dtype="float32") = model_params[991]
            lv688_1 = R.call_tir(cls.matmul16, (inp_2, lv752), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv234 = R.call_tir(cls.fused_reshape18_transpose17, (lv684_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv235 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv686_1,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv236 = R.call_tir(cls.fused_reshape20_transpose21, (lv688_1,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv237 = R.call_tir(cls.fused_matmul17_multiply8, (lv234, lv235, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv700_1 = R.call_tir(cls.softmax3, (lv237,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv701_1 = R.call_tir(cls.matmul18, (lv700_1, lv236), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv238 = R.call_tir(cls.fused_transpose19_reshape19, (lv701_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv753: R.Tensor((1280, 1280), dtype="float32") = model_params[992]
            lv754: R.Tensor((1280,), dtype="float32") = model_params[120]
            lv239_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv238, lv753, lv754, lv233), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv755: R.Tensor((1280,), dtype="float32") = model_params[128]
            lv756: R.Tensor((1280,), dtype="float32") = model_params[127]
            lv709_1 = R.call_tir(cls.layer_norm1, (lv239_1, lv755, lv756), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv757: R.Tensor((1280, 10240), dtype="float32") = model_params[993]
            lv758: R.Tensor((10240,), dtype="float32") = model_params[121]
            lv240 = R.call_tir(cls.fused_matmul19_add17, (lv709_1, lv757, lv758), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv241 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv240,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv759: R.Tensor((5120, 1280), dtype="float32") = model_params[994]
            lv760: R.Tensor((1280,), dtype="float32") = model_params[122]
            lv242 = R.call_tir(cls.fused_matmul20_add15_add16, (lv241, lv759, lv760, lv239_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv761: R.Tensor((1280,), dtype="float32") = model_params[134]
            lv762: R.Tensor((1280,), dtype="float32") = model_params[133]
            lv722_1 = R.call_tir(cls.layer_norm1, (lv242, lv761, lv762), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv763: R.Tensor((1280, 1280), dtype="float32") = model_params[995]
            lv724_1 = R.call_tir(cls.matmul13, (lv722_1, lv763), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv764: R.Tensor((1280, 1280), dtype="float32") = model_params[996]
            lv726_1 = R.call_tir(cls.matmul13, (lv722_1, lv764), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv765: R.Tensor((1280, 1280), dtype="float32") = model_params[997]
            lv728_1 = R.call_tir(cls.matmul13, (lv722_1, lv765), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv243 = R.call_tir(cls.fused_reshape18_transpose17, (lv724_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv244 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv726_1,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv245 = R.call_tir(cls.fused_reshape18_transpose17, (lv728_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv246 = R.call_tir(cls.fused_matmul14_multiply7, (lv243, lv244, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv740_1 = R.call_tir(cls.softmax2, (lv246,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv741_1 = R.call_tir(cls.matmul15, (lv740_1, lv245), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv247 = R.call_tir(cls.fused_transpose19_reshape19, (lv741_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv766: R.Tensor((1280, 1280), dtype="float32") = model_params[998]
            lv767: R.Tensor((1280,), dtype="float32") = model_params[129]
            lv248 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv247, lv766, lv767, lv242), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv768: R.Tensor((1280,), dtype="float32") = model_params[136]
            lv769: R.Tensor((1280,), dtype="float32") = model_params[135]
            lv749_1 = R.call_tir(cls.layer_norm1, (lv248, lv768, lv769), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv770: R.Tensor((1280, 1280), dtype="float32") = model_params[999]
            lv751_1 = R.call_tir(cls.matmul13, (lv749_1, lv770), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv771: R.Tensor((2048, 1280), dtype="float32") = model_params[1000]
            lv753_1 = R.call_tir(cls.matmul16, (inp_2, lv771), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv772: R.Tensor((2048, 1280), dtype="float32") = model_params[1001]
            lv755_1 = R.call_tir(cls.matmul16, (inp_2, lv772), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv249 = R.call_tir(cls.fused_reshape18_transpose17, (lv751_1,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv250 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv753_1,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv251 = R.call_tir(cls.fused_reshape20_transpose21, (lv755_1,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv252 = R.call_tir(cls.fused_matmul17_multiply8, (lv249, lv250, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv767_1 = R.call_tir(cls.softmax3, (lv252,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv768_1 = R.call_tir(cls.matmul18, (lv767_1, lv251), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv253 = R.call_tir(cls.fused_transpose19_reshape19, (lv768_1,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv773: R.Tensor((1280, 1280), dtype="float32") = model_params[1002]
            lv774: R.Tensor((1280,), dtype="float32") = model_params[130]
            lv254 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv253, lv773, lv774, lv248), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv775: R.Tensor((1280,), dtype="float32") = model_params[138]
            lv776: R.Tensor((1280,), dtype="float32") = model_params[137]
            lv776_1 = R.call_tir(cls.layer_norm1, (lv254, lv775, lv776), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv777: R.Tensor((1280, 10240), dtype="float32") = model_params[1003]
            lv778: R.Tensor((10240,), dtype="float32") = model_params[131]
            lv255 = R.call_tir(cls.fused_matmul19_add17, (lv776_1, lv777, lv778), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv256 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv255,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv779: R.Tensor((5120, 1280), dtype="float32") = model_params[1004]
            lv780: R.Tensor((1280,), dtype="float32") = model_params[132]
            lv257 = R.call_tir(cls.fused_matmul20_add15_add16, (lv256, lv779, lv780, lv254), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv781: R.Tensor((1280,), dtype="float32") = model_params[144]
            lv782: R.Tensor((1280,), dtype="float32") = model_params[143]
            lv789 = R.call_tir(cls.layer_norm1, (lv257, lv781, lv782), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv783: R.Tensor((1280, 1280), dtype="float32") = model_params[1005]
            lv791 = R.call_tir(cls.matmul13, (lv789, lv783), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv784: R.Tensor((1280, 1280), dtype="float32") = model_params[1006]
            lv793 = R.call_tir(cls.matmul13, (lv789, lv784), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv785: R.Tensor((1280, 1280), dtype="float32") = model_params[1007]
            lv795 = R.call_tir(cls.matmul13, (lv789, lv785), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv258 = R.call_tir(cls.fused_reshape18_transpose17, (lv791,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv259 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv793,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv260 = R.call_tir(cls.fused_reshape18_transpose17, (lv795,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv261 = R.call_tir(cls.fused_matmul14_multiply7, (lv258, lv259, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv807 = R.call_tir(cls.softmax2, (lv261,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv808 = R.call_tir(cls.matmul15, (lv807, lv260), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv262 = R.call_tir(cls.fused_transpose19_reshape19, (lv808,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv786: R.Tensor((1280, 1280), dtype="float32") = model_params[1008]
            lv787: R.Tensor((1280,), dtype="float32") = model_params[139]
            lv263_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv262, lv786, lv787, lv257), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv788: R.Tensor((1280,), dtype="float32") = model_params[146]
            lv789_1: R.Tensor((1280,), dtype="float32") = model_params[145]
            lv816 = R.call_tir(cls.layer_norm1, (lv263_1, lv788, lv789_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv790: R.Tensor((1280, 1280), dtype="float32") = model_params[1009]
            lv818 = R.call_tir(cls.matmul13, (lv816, lv790), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv791_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1010]
            lv820 = R.call_tir(cls.matmul16, (inp_2, lv791_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv792: R.Tensor((2048, 1280), dtype="float32") = model_params[1011]
            lv822 = R.call_tir(cls.matmul16, (inp_2, lv792), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv264 = R.call_tir(cls.fused_reshape18_transpose17, (lv818,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv265 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv820,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv266 = R.call_tir(cls.fused_reshape20_transpose21, (lv822,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv267 = R.call_tir(cls.fused_matmul17_multiply8, (lv264, lv265, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv834 = R.call_tir(cls.softmax3, (lv267,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv835 = R.call_tir(cls.matmul18, (lv834, lv266), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv268_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv835,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv793_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1012]
            lv794: R.Tensor((1280,), dtype="float32") = model_params[140]
            lv269 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv268_1, lv793_1, lv794, lv263_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv795_1: R.Tensor((1280,), dtype="float32") = model_params[148]
            lv796: R.Tensor((1280,), dtype="float32") = model_params[147]
            lv843 = R.call_tir(cls.layer_norm1, (lv269, lv795_1, lv796), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv797: R.Tensor((1280, 10240), dtype="float32") = model_params[1013]
            lv798: R.Tensor((10240,), dtype="float32") = model_params[141]
            lv270 = R.call_tir(cls.fused_matmul19_add17, (lv843, lv797, lv798), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv271 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv270,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv799: R.Tensor((5120, 1280), dtype="float32") = model_params[1014]
            lv800: R.Tensor((1280,), dtype="float32") = model_params[142]
            lv272 = R.call_tir(cls.fused_matmul20_add15_add16, (lv271, lv799, lv800, lv269), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv801: R.Tensor((1280,), dtype="float32") = model_params[154]
            lv802: R.Tensor((1280,), dtype="float32") = model_params[153]
            lv856 = R.call_tir(cls.layer_norm1, (lv272, lv801, lv802), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv803: R.Tensor((1280, 1280), dtype="float32") = model_params[1015]
            lv858 = R.call_tir(cls.matmul13, (lv856, lv803), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv804: R.Tensor((1280, 1280), dtype="float32") = model_params[1016]
            lv860 = R.call_tir(cls.matmul13, (lv856, lv804), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv805: R.Tensor((1280, 1280), dtype="float32") = model_params[1017]
            lv862 = R.call_tir(cls.matmul13, (lv856, lv805), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv273 = R.call_tir(cls.fused_reshape18_transpose17, (lv858,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv274 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv860,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv275 = R.call_tir(cls.fused_reshape18_transpose17, (lv862,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv276 = R.call_tir(cls.fused_matmul14_multiply7, (lv273, lv274, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv874 = R.call_tir(cls.softmax2, (lv276,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv875 = R.call_tir(cls.matmul15, (lv874, lv275), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv277_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv875,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv806: R.Tensor((1280, 1280), dtype="float32") = model_params[1018]
            lv807_1: R.Tensor((1280,), dtype="float32") = model_params[149]
            lv278 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv277_1, lv806, lv807_1, lv272), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv808_1: R.Tensor((1280,), dtype="float32") = model_params[156]
            lv809: R.Tensor((1280,), dtype="float32") = model_params[155]
            lv883 = R.call_tir(cls.layer_norm1, (lv278, lv808_1, lv809), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv810: R.Tensor((1280, 1280), dtype="float32") = model_params[1019]
            lv885 = R.call_tir(cls.matmul13, (lv883, lv810), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv811: R.Tensor((2048, 1280), dtype="float32") = model_params[1020]
            lv887 = R.call_tir(cls.matmul16, (inp_2, lv811), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv812: R.Tensor((2048, 1280), dtype="float32") = model_params[1021]
            lv889 = R.call_tir(cls.matmul16, (inp_2, lv812), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv279 = R.call_tir(cls.fused_reshape18_transpose17, (lv885,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv280 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv887,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv281 = R.call_tir(cls.fused_reshape20_transpose21, (lv889,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv282 = R.call_tir(cls.fused_matmul17_multiply8, (lv279, lv280, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv901 = R.call_tir(cls.softmax3, (lv282,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv902 = R.call_tir(cls.matmul18, (lv901, lv281), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv283_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv902,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv813: R.Tensor((1280, 1280), dtype="float32") = model_params[1022]
            lv814: R.Tensor((1280,), dtype="float32") = model_params[150]
            lv284 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv283_1, lv813, lv814, lv278), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv815: R.Tensor((1280,), dtype="float32") = model_params[158]
            lv816_1: R.Tensor((1280,), dtype="float32") = model_params[157]
            lv910 = R.call_tir(cls.layer_norm1, (lv284, lv815, lv816_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv817: R.Tensor((1280, 10240), dtype="float32") = model_params[1023]
            lv818_1: R.Tensor((10240,), dtype="float32") = model_params[151]
            lv285_1 = R.call_tir(cls.fused_matmul19_add17, (lv910, lv817, lv818_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv286 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv285_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv819: R.Tensor((5120, 1280), dtype="float32") = model_params[1024]
            lv820_1: R.Tensor((1280,), dtype="float32") = model_params[152]
            lv287_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv286, lv819, lv820_1, lv284), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv821: R.Tensor((1280,), dtype="float32") = model_params[164]
            lv822_1: R.Tensor((1280,), dtype="float32") = model_params[163]
            lv923 = R.call_tir(cls.layer_norm1, (lv287_1, lv821, lv822_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv823: R.Tensor((1280, 1280), dtype="float32") = model_params[1025]
            lv925 = R.call_tir(cls.matmul13, (lv923, lv823), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv824: R.Tensor((1280, 1280), dtype="float32") = model_params[1026]
            lv927 = R.call_tir(cls.matmul13, (lv923, lv824), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv825: R.Tensor((1280, 1280), dtype="float32") = model_params[1027]
            lv929 = R.call_tir(cls.matmul13, (lv923, lv825), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv288 = R.call_tir(cls.fused_reshape18_transpose17, (lv925,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv289_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv927,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv290 = R.call_tir(cls.fused_reshape18_transpose17, (lv929,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv291 = R.call_tir(cls.fused_matmul14_multiply7, (lv288, lv289_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv941 = R.call_tir(cls.softmax2, (lv291,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv942 = R.call_tir(cls.matmul15, (lv941, lv290), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv292 = R.call_tir(cls.fused_transpose19_reshape19, (lv942,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv826: R.Tensor((1280, 1280), dtype="float32") = model_params[1028]
            lv827: R.Tensor((1280,), dtype="float32") = model_params[159]
            lv293 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv292, lv826, lv827, lv287_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv828: R.Tensor((1280,), dtype="float32") = model_params[166]
            lv829: R.Tensor((1280,), dtype="float32") = model_params[165]
            lv950 = R.call_tir(cls.layer_norm1, (lv293, lv828, lv829), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv830: R.Tensor((1280, 1280), dtype="float32") = model_params[1029]
            lv952 = R.call_tir(cls.matmul13, (lv950, lv830), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv831: R.Tensor((2048, 1280), dtype="float32") = model_params[1030]
            lv954 = R.call_tir(cls.matmul16, (inp_2, lv831), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv832: R.Tensor((2048, 1280), dtype="float32") = model_params[1031]
            lv956 = R.call_tir(cls.matmul16, (inp_2, lv832), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv294 = R.call_tir(cls.fused_reshape18_transpose17, (lv952,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv295 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv954,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv296 = R.call_tir(cls.fused_reshape20_transpose21, (lv956,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv297 = R.call_tir(cls.fused_matmul17_multiply8, (lv294, lv295, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv968 = R.call_tir(cls.softmax3, (lv297,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv969 = R.call_tir(cls.matmul18, (lv968, lv296), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv298 = R.call_tir(cls.fused_transpose19_reshape19, (lv969,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv833: R.Tensor((1280, 1280), dtype="float32") = model_params[1032]
            lv834_1: R.Tensor((1280,), dtype="float32") = model_params[160]
            lv299 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv298, lv833, lv834_1, lv293), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv835_1: R.Tensor((1280,), dtype="float32") = model_params[168]
            lv836: R.Tensor((1280,), dtype="float32") = model_params[167]
            lv977 = R.call_tir(cls.layer_norm1, (lv299, lv835_1, lv836), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv837: R.Tensor((1280, 10240), dtype="float32") = model_params[1033]
            lv838: R.Tensor((10240,), dtype="float32") = model_params[161]
            lv300 = R.call_tir(cls.fused_matmul19_add17, (lv977, lv837, lv838), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv301_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv300,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv839: R.Tensor((5120, 1280), dtype="float32") = model_params[1034]
            lv840: R.Tensor((1280,), dtype="float32") = model_params[162]
            lv302_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv301_1, lv839, lv840, lv299), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv841: R.Tensor((1280,), dtype="float32") = model_params[174]
            lv842: R.Tensor((1280,), dtype="float32") = model_params[173]
            lv990 = R.call_tir(cls.layer_norm1, (lv302_1, lv841, lv842), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv843_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1035]
            lv992 = R.call_tir(cls.matmul13, (lv990, lv843_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv844: R.Tensor((1280, 1280), dtype="float32") = model_params[1036]
            lv994 = R.call_tir(cls.matmul13, (lv990, lv844), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv845: R.Tensor((1280, 1280), dtype="float32") = model_params[1037]
            lv996 = R.call_tir(cls.matmul13, (lv990, lv845), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv303 = R.call_tir(cls.fused_reshape18_transpose17, (lv992,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv304 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv994,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv305 = R.call_tir(cls.fused_reshape18_transpose17, (lv996,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv306 = R.call_tir(cls.fused_matmul14_multiply7, (lv303, lv304, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1008 = R.call_tir(cls.softmax2, (lv306,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1009 = R.call_tir(cls.matmul15, (lv1008, lv305), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv307 = R.call_tir(cls.fused_transpose19_reshape19, (lv1009,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv846: R.Tensor((1280, 1280), dtype="float32") = model_params[1038]
            lv847: R.Tensor((1280,), dtype="float32") = model_params[169]
            lv308 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv307, lv846, lv847, lv302_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv848: R.Tensor((1280,), dtype="float32") = model_params[176]
            lv849: R.Tensor((1280,), dtype="float32") = model_params[175]
            lv1017 = R.call_tir(cls.layer_norm1, (lv308, lv848, lv849), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv850: R.Tensor((1280, 1280), dtype="float32") = model_params[1039]
            lv1019 = R.call_tir(cls.matmul13, (lv1017, lv850), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv851: R.Tensor((2048, 1280), dtype="float32") = model_params[1040]
            lv1021 = R.call_tir(cls.matmul16, (inp_2, lv851), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv852: R.Tensor((2048, 1280), dtype="float32") = model_params[1041]
            lv1023 = R.call_tir(cls.matmul16, (inp_2, lv852), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv309 = R.call_tir(cls.fused_reshape18_transpose17, (lv1019,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv310_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1021,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv311 = R.call_tir(cls.fused_reshape20_transpose21, (lv1023,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv312_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv309, lv310_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1035 = R.call_tir(cls.softmax3, (lv312_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1036 = R.call_tir(cls.matmul18, (lv1035, lv311), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv313 = R.call_tir(cls.fused_transpose19_reshape19, (lv1036,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv853: R.Tensor((1280, 1280), dtype="float32") = model_params[1042]
            lv854: R.Tensor((1280,), dtype="float32") = model_params[170]
            lv314_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv313, lv853, lv854, lv308), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv855: R.Tensor((1280,), dtype="float32") = model_params[178]
            lv856_1: R.Tensor((1280,), dtype="float32") = model_params[177]
            lv1044 = R.call_tir(cls.layer_norm1, (lv314_1, lv855, lv856_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv857: R.Tensor((1280, 10240), dtype="float32") = model_params[1043]
            lv858_1: R.Tensor((10240,), dtype="float32") = model_params[171]
            lv315 = R.call_tir(cls.fused_matmul19_add17, (lv1044, lv857, lv858_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv316_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv315,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv859: R.Tensor((5120, 1280), dtype="float32") = model_params[1044]
            lv860_1: R.Tensor((1280,), dtype="float32") = model_params[172]
            lv317 = R.call_tir(cls.fused_matmul20_add15_add16, (lv316_1, lv859, lv860_1, lv314_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv861: R.Tensor((1280,), dtype="float32") = model_params[184]
            lv862_1: R.Tensor((1280,), dtype="float32") = model_params[183]
            lv1057 = R.call_tir(cls.layer_norm1, (lv317, lv861, lv862_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv863: R.Tensor((1280, 1280), dtype="float32") = model_params[1045]
            lv1059 = R.call_tir(cls.matmul13, (lv1057, lv863), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv864: R.Tensor((1280, 1280), dtype="float32") = model_params[1046]
            lv1061 = R.call_tir(cls.matmul13, (lv1057, lv864), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv865: R.Tensor((1280, 1280), dtype="float32") = model_params[1047]
            lv1063 = R.call_tir(cls.matmul13, (lv1057, lv865), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv318 = R.call_tir(cls.fused_reshape18_transpose17, (lv1059,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv319 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1061,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv320 = R.call_tir(cls.fused_reshape18_transpose17, (lv1063,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv321 = R.call_tir(cls.fused_matmul14_multiply7, (lv318, lv319, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1075 = R.call_tir(cls.softmax2, (lv321,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1076 = R.call_tir(cls.matmul15, (lv1075, lv320), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv322 = R.call_tir(cls.fused_transpose19_reshape19, (lv1076,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv866: R.Tensor((1280, 1280), dtype="float32") = model_params[1048]
            lv867: R.Tensor((1280,), dtype="float32") = model_params[179]
            lv323 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv322, lv866, lv867, lv317), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv868: R.Tensor((1280,), dtype="float32") = model_params[186]
            lv869: R.Tensor((1280,), dtype="float32") = model_params[185]
            lv1084 = R.call_tir(cls.layer_norm1, (lv323, lv868, lv869), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv870: R.Tensor((1280, 1280), dtype="float32") = model_params[1049]
            lv1086 = R.call_tir(cls.matmul13, (lv1084, lv870), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv871: R.Tensor((2048, 1280), dtype="float32") = model_params[1050]
            lv1088 = R.call_tir(cls.matmul16, (inp_2, lv871), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv872: R.Tensor((2048, 1280), dtype="float32") = model_params[1051]
            lv1090 = R.call_tir(cls.matmul16, (inp_2, lv872), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv324 = R.call_tir(cls.fused_reshape18_transpose17, (lv1086,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv325 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1088,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv326 = R.call_tir(cls.fused_reshape20_transpose21, (lv1090,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv327 = R.call_tir(cls.fused_matmul17_multiply8, (lv324, lv325, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1102 = R.call_tir(cls.softmax3, (lv327,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1103 = R.call_tir(cls.matmul18, (lv1102, lv326), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv328_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1103,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv873: R.Tensor((1280, 1280), dtype="float32") = model_params[1052]
            lv874_1: R.Tensor((1280,), dtype="float32") = model_params[180]
            lv329_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv328_1, lv873, lv874_1, lv323), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv875_1: R.Tensor((1280,), dtype="float32") = model_params[188]
            lv876: R.Tensor((1280,), dtype="float32") = model_params[187]
            lv1111 = R.call_tir(cls.layer_norm1, (lv329_1, lv875_1, lv876), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv877: R.Tensor((1280, 10240), dtype="float32") = model_params[1053]
            lv878: R.Tensor((10240,), dtype="float32") = model_params[181]
            lv330 = R.call_tir(cls.fused_matmul19_add17, (lv1111, lv877, lv878), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv331 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv330,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv879: R.Tensor((5120, 1280), dtype="float32") = model_params[1054]
            lv880: R.Tensor((1280,), dtype="float32") = model_params[182]
            lv332 = R.call_tir(cls.fused_matmul20_add15_add16, (lv331, lv879, lv880, lv329_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv881: R.Tensor((1280, 1280), dtype="float32") = model_params[1055]
            lv882: R.Tensor((1280,), dtype="float32") = model_params[88]
            lv333 = R.call_tir(cls.fused_matmul13_add15, (lv332, lv881, lv882), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv334 = R.call_tir(cls.fused_reshape21_transpose25_add14, (lv333, lv180), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv883_1: R.Tensor((1280,), dtype="float32") = model_params[304]
            lv884: R.Tensor((1280,), dtype="float32") = model_params[303]
            lv335 = R.call_tir(cls.fused_group_norm5_silu5, (lv334, lv883_1, lv884), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1135 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv885_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1057]
            lv886: R.Tensor((1280,), dtype="float32") = model_params[307]
            lv336 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv1135, lv885_1, lv886), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1140 = R.call_tir(cls.reshape16, (lv336,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv887_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[301]
            lv888: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1056]
            lv337_1 = R.call_tir(cls.fused_conv2d8_add13_add13, (lv335, lv887_1, lv888, lv1140), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv889_1: R.Tensor((1280,), dtype="float32") = model_params[306]
            lv890: R.Tensor((1280,), dtype="float32") = model_params[305]
            lv338 = R.call_tir(cls.fused_group_norm5_silu5, (lv337_1, lv889_1, lv890), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv891: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[302]
            lv892: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1058]
            lv339 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv338, lv891, lv892, lv334), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv893: R.Tensor((1280,), dtype="float32") = model_params[190]
            lv894: R.Tensor((1280,), dtype="float32") = model_params[189]
            lv1149 = R.call_tir(cls.group_norm6, (lv339, lv893, lv894), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv340 = R.call_tir(cls.fused_transpose16_reshape17, (lv1149,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv895: R.Tensor((1280, 1280), dtype="float32") = model_params[1059]
            lv896: R.Tensor((1280,), dtype="float32") = model_params[191]
            lv341 = R.call_tir(cls.fused_matmul13_add15, (lv340, lv895, lv896), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv897: R.Tensor((1280,), dtype="float32") = model_params[198]
            lv898: R.Tensor((1280,), dtype="float32") = model_params[197]
            lv1155 = R.call_tir(cls.layer_norm1, (lv341, lv897, lv898), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv899: R.Tensor((1280, 1280), dtype="float32") = model_params[1060]
            lv1157 = R.call_tir(cls.matmul13, (lv1155, lv899), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv900: R.Tensor((1280, 1280), dtype="float32") = model_params[1061]
            lv1159 = R.call_tir(cls.matmul13, (lv1155, lv900), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv901_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1062]
            lv1161 = R.call_tir(cls.matmul13, (lv1155, lv901_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv342 = R.call_tir(cls.fused_reshape18_transpose17, (lv1157,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv343 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1159,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv344 = R.call_tir(cls.fused_reshape18_transpose17, (lv1161,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv345 = R.call_tir(cls.fused_matmul14_multiply7, (lv342, lv343, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1173 = R.call_tir(cls.softmax2, (lv345,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1174 = R.call_tir(cls.matmul15, (lv1173, lv344), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv346 = R.call_tir(cls.fused_transpose19_reshape19, (lv1174,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv902_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1063]
            lv903: R.Tensor((1280,), dtype="float32") = model_params[193]
            lv347 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv346, lv902_1, lv903, lv341), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv904: R.Tensor((1280,), dtype="float32") = model_params[200]
            lv905: R.Tensor((1280,), dtype="float32") = model_params[199]
            lv1182 = R.call_tir(cls.layer_norm1, (lv347, lv904, lv905), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv906: R.Tensor((1280, 1280), dtype="float32") = model_params[1064]
            lv1184 = R.call_tir(cls.matmul13, (lv1182, lv906), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv907: R.Tensor((2048, 1280), dtype="float32") = model_params[1065]
            lv1186 = R.call_tir(cls.matmul16, (inp_2, lv907), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv908: R.Tensor((2048, 1280), dtype="float32") = model_params[1066]
            lv1188 = R.call_tir(cls.matmul16, (inp_2, lv908), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv348 = R.call_tir(cls.fused_reshape18_transpose17, (lv1184,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv349 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1186,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv350_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv1188,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv351 = R.call_tir(cls.fused_matmul17_multiply8, (lv348, lv349, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1200 = R.call_tir(cls.softmax3, (lv351,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1201 = R.call_tir(cls.matmul18, (lv1200, lv350_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv352_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1201,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv909: R.Tensor((1280, 1280), dtype="float32") = model_params[1067]
            lv910_1: R.Tensor((1280,), dtype="float32") = model_params[194]
            lv353 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv352_1, lv909, lv910_1, lv347), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv911: R.Tensor((1280,), dtype="float32") = model_params[202]
            lv912: R.Tensor((1280,), dtype="float32") = model_params[201]
            lv1209 = R.call_tir(cls.layer_norm1, (lv353, lv911, lv912), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv913: R.Tensor((1280, 10240), dtype="float32") = model_params[1068]
            lv914: R.Tensor((10240,), dtype="float32") = model_params[195]
            lv354_1 = R.call_tir(cls.fused_matmul19_add17, (lv1209, lv913, lv914), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv355 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv354_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv915: R.Tensor((5120, 1280), dtype="float32") = model_params[1069]
            lv916: R.Tensor((1280,), dtype="float32") = model_params[196]
            lv356_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv355, lv915, lv916, lv353), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv917: R.Tensor((1280,), dtype="float32") = model_params[208]
            lv918: R.Tensor((1280,), dtype="float32") = model_params[207]
            lv1222 = R.call_tir(cls.layer_norm1, (lv356_1, lv917, lv918), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv919: R.Tensor((1280, 1280), dtype="float32") = model_params[1070]
            lv1224 = R.call_tir(cls.matmul13, (lv1222, lv919), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv920: R.Tensor((1280, 1280), dtype="float32") = model_params[1071]
            lv1226 = R.call_tir(cls.matmul13, (lv1222, lv920), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv921: R.Tensor((1280, 1280), dtype="float32") = model_params[1072]
            lv1228 = R.call_tir(cls.matmul13, (lv1222, lv921), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv357 = R.call_tir(cls.fused_reshape18_transpose17, (lv1224,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv358 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1226,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv359 = R.call_tir(cls.fused_reshape18_transpose17, (lv1228,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv360 = R.call_tir(cls.fused_matmul14_multiply7, (lv357, lv358, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1240 = R.call_tir(cls.softmax2, (lv360,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1241 = R.call_tir(cls.matmul15, (lv1240, lv359), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv361 = R.call_tir(cls.fused_transpose19_reshape19, (lv1241,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv922: R.Tensor((1280, 1280), dtype="float32") = model_params[1073]
            lv923_1: R.Tensor((1280,), dtype="float32") = model_params[203]
            lv362 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv361, lv922, lv923_1, lv356_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv924: R.Tensor((1280,), dtype="float32") = model_params[210]
            lv925_1: R.Tensor((1280,), dtype="float32") = model_params[209]
            lv1249 = R.call_tir(cls.layer_norm1, (lv362, lv924, lv925_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv926: R.Tensor((1280, 1280), dtype="float32") = model_params[1074]
            lv1251 = R.call_tir(cls.matmul13, (lv1249, lv926), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv927_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1075]
            lv1253 = R.call_tir(cls.matmul16, (inp_2, lv927_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv928: R.Tensor((2048, 1280), dtype="float32") = model_params[1076]
            lv1255 = R.call_tir(cls.matmul16, (inp_2, lv928), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv363 = R.call_tir(cls.fused_reshape18_transpose17, (lv1251,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv364 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1253,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv365 = R.call_tir(cls.fused_reshape20_transpose21, (lv1255,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv366 = R.call_tir(cls.fused_matmul17_multiply8, (lv363, lv364, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1267 = R.call_tir(cls.softmax3, (lv366,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1268 = R.call_tir(cls.matmul18, (lv1267, lv365), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv367 = R.call_tir(cls.fused_transpose19_reshape19, (lv1268,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv929_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1077]
            lv930: R.Tensor((1280,), dtype="float32") = model_params[204]
            lv368_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv367, lv929_1, lv930, lv362), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv931: R.Tensor((1280,), dtype="float32") = model_params[212]
            lv932: R.Tensor((1280,), dtype="float32") = model_params[211]
            lv1276 = R.call_tir(cls.layer_norm1, (lv368_1, lv931, lv932), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv933: R.Tensor((1280, 10240), dtype="float32") = model_params[1078]
            lv934: R.Tensor((10240,), dtype="float32") = model_params[205]
            lv369_1 = R.call_tir(cls.fused_matmul19_add17, (lv1276, lv933, lv934), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv370 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv369_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv935: R.Tensor((5120, 1280), dtype="float32") = model_params[1079]
            lv936: R.Tensor((1280,), dtype="float32") = model_params[206]
            lv371 = R.call_tir(cls.fused_matmul20_add15_add16, (lv370, lv935, lv936, lv368_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv937: R.Tensor((1280,), dtype="float32") = model_params[218]
            lv938: R.Tensor((1280,), dtype="float32") = model_params[217]
            lv1289 = R.call_tir(cls.layer_norm1, (lv371, lv937, lv938), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv939: R.Tensor((1280, 1280), dtype="float32") = model_params[1080]
            lv1291 = R.call_tir(cls.matmul13, (lv1289, lv939), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv940: R.Tensor((1280, 1280), dtype="float32") = model_params[1081]
            lv1293 = R.call_tir(cls.matmul13, (lv1289, lv940), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv941_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1082]
            lv1295 = R.call_tir(cls.matmul13, (lv1289, lv941_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv372 = R.call_tir(cls.fused_reshape18_transpose17, (lv1291,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv373 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1293,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv374 = R.call_tir(cls.fused_reshape18_transpose17, (lv1295,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv375 = R.call_tir(cls.fused_matmul14_multiply7, (lv372, lv373, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1307 = R.call_tir(cls.softmax2, (lv375,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1308 = R.call_tir(cls.matmul15, (lv1307, lv374), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv376 = R.call_tir(cls.fused_transpose19_reshape19, (lv1308,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv942_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1083]
            lv943: R.Tensor((1280,), dtype="float32") = model_params[213]
            lv377_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv376, lv942_1, lv943, lv371), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv944: R.Tensor((1280,), dtype="float32") = model_params[220]
            lv945: R.Tensor((1280,), dtype="float32") = model_params[219]
            lv1316 = R.call_tir(cls.layer_norm1, (lv377_1, lv944, lv945), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv946: R.Tensor((1280, 1280), dtype="float32") = model_params[1084]
            lv1318 = R.call_tir(cls.matmul13, (lv1316, lv946), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv947: R.Tensor((2048, 1280), dtype="float32") = model_params[1085]
            lv1320 = R.call_tir(cls.matmul16, (inp_2, lv947), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv948: R.Tensor((2048, 1280), dtype="float32") = model_params[1086]
            lv1322 = R.call_tir(cls.matmul16, (inp_2, lv948), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv378 = R.call_tir(cls.fused_reshape18_transpose17, (lv1318,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv379_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1320,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv380 = R.call_tir(cls.fused_reshape20_transpose21, (lv1322,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv381_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv378, lv379_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1334 = R.call_tir(cls.softmax3, (lv381_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1335 = R.call_tir(cls.matmul18, (lv1334, lv380), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv382 = R.call_tir(cls.fused_transpose19_reshape19, (lv1335,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv949: R.Tensor((1280, 1280), dtype="float32") = model_params[1087]
            lv950_1: R.Tensor((1280,), dtype="float32") = model_params[214]
            lv383_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv382, lv949, lv950_1, lv377_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv951: R.Tensor((1280,), dtype="float32") = model_params[222]
            lv952_1: R.Tensor((1280,), dtype="float32") = model_params[221]
            lv1343 = R.call_tir(cls.layer_norm1, (lv383_1, lv951, lv952_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv953: R.Tensor((1280, 10240), dtype="float32") = model_params[1088]
            lv954_1: R.Tensor((10240,), dtype="float32") = model_params[215]
            lv384 = R.call_tir(cls.fused_matmul19_add17, (lv1343, lv953, lv954_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv385 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv384,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv955: R.Tensor((5120, 1280), dtype="float32") = model_params[1089]
            lv956_1: R.Tensor((1280,), dtype="float32") = model_params[216]
            lv386 = R.call_tir(cls.fused_matmul20_add15_add16, (lv385, lv955, lv956_1, lv383_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv957: R.Tensor((1280,), dtype="float32") = model_params[228]
            lv958: R.Tensor((1280,), dtype="float32") = model_params[227]
            lv1356 = R.call_tir(cls.layer_norm1, (lv386, lv957, lv958), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv959: R.Tensor((1280, 1280), dtype="float32") = model_params[1090]
            lv1358 = R.call_tir(cls.matmul13, (lv1356, lv959), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv960: R.Tensor((1280, 1280), dtype="float32") = model_params[1091]
            lv1360 = R.call_tir(cls.matmul13, (lv1356, lv960), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv961: R.Tensor((1280, 1280), dtype="float32") = model_params[1092]
            lv1362 = R.call_tir(cls.matmul13, (lv1356, lv961), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv387 = R.call_tir(cls.fused_reshape18_transpose17, (lv1358,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv388 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1360,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv389 = R.call_tir(cls.fused_reshape18_transpose17, (lv1362,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv390 = R.call_tir(cls.fused_matmul14_multiply7, (lv387, lv388, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1374 = R.call_tir(cls.softmax2, (lv390,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1375 = R.call_tir(cls.matmul15, (lv1374, lv389), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv391 = R.call_tir(cls.fused_transpose19_reshape19, (lv1375,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv962: R.Tensor((1280, 1280), dtype="float32") = model_params[1093]
            lv963: R.Tensor((1280,), dtype="float32") = model_params[223]
            lv392 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv391, lv962, lv963, lv386), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv964: R.Tensor((1280,), dtype="float32") = model_params[230]
            lv965: R.Tensor((1280,), dtype="float32") = model_params[229]
            lv1383 = R.call_tir(cls.layer_norm1, (lv392, lv964, lv965), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv966: R.Tensor((1280, 1280), dtype="float32") = model_params[1094]
            lv1385 = R.call_tir(cls.matmul13, (lv1383, lv966), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv967: R.Tensor((2048, 1280), dtype="float32") = model_params[1095]
            lv1387 = R.call_tir(cls.matmul16, (inp_2, lv967), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv968_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1096]
            lv1389 = R.call_tir(cls.matmul16, (inp_2, lv968_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv393 = R.call_tir(cls.fused_reshape18_transpose17, (lv1385,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv394 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1387,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv395_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv1389,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv396_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv393, lv394, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1401 = R.call_tir(cls.softmax3, (lv396_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1402 = R.call_tir(cls.matmul18, (lv1401, lv395_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv397 = R.call_tir(cls.fused_transpose19_reshape19, (lv1402,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv969_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1097]
            lv970: R.Tensor((1280,), dtype="float32") = model_params[224]
            lv398 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv397, lv969_1, lv970, lv392), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv971: R.Tensor((1280,), dtype="float32") = model_params[232]
            lv972: R.Tensor((1280,), dtype="float32") = model_params[231]
            lv1410 = R.call_tir(cls.layer_norm1, (lv398, lv971, lv972), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv973: R.Tensor((1280, 10240), dtype="float32") = model_params[1098]
            lv974: R.Tensor((10240,), dtype="float32") = model_params[225]
            lv399 = R.call_tir(cls.fused_matmul19_add17, (lv1410, lv973, lv974), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv400 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv399,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv975: R.Tensor((5120, 1280), dtype="float32") = model_params[1099]
            lv976: R.Tensor((1280,), dtype="float32") = model_params[226]
            lv401 = R.call_tir(cls.fused_matmul20_add15_add16, (lv400, lv975, lv976, lv398), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv977_1: R.Tensor((1280,), dtype="float32") = model_params[238]
            lv978: R.Tensor((1280,), dtype="float32") = model_params[237]
            lv1423 = R.call_tir(cls.layer_norm1, (lv401, lv977_1, lv978), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv979: R.Tensor((1280, 1280), dtype="float32") = model_params[1100]
            lv1425 = R.call_tir(cls.matmul13, (lv1423, lv979), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv980: R.Tensor((1280, 1280), dtype="float32") = model_params[1101]
            lv1427 = R.call_tir(cls.matmul13, (lv1423, lv980), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv981: R.Tensor((1280, 1280), dtype="float32") = model_params[1102]
            lv1429 = R.call_tir(cls.matmul13, (lv1423, lv981), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv402 = R.call_tir(cls.fused_reshape18_transpose17, (lv1425,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv403 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1427,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv404_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1429,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv405 = R.call_tir(cls.fused_matmul14_multiply7, (lv402, lv403, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1441 = R.call_tir(cls.softmax2, (lv405,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1442 = R.call_tir(cls.matmul15, (lv1441, lv404_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv406 = R.call_tir(cls.fused_transpose19_reshape19, (lv1442,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv982: R.Tensor((1280, 1280), dtype="float32") = model_params[1103]
            lv983: R.Tensor((1280,), dtype="float32") = model_params[233]
            lv407 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv406, lv982, lv983, lv401), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv984: R.Tensor((1280,), dtype="float32") = model_params[240]
            lv985: R.Tensor((1280,), dtype="float32") = model_params[239]
            lv1450 = R.call_tir(cls.layer_norm1, (lv407, lv984, lv985), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv986: R.Tensor((1280, 1280), dtype="float32") = model_params[1104]
            lv1452 = R.call_tir(cls.matmul13, (lv1450, lv986), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv987: R.Tensor((2048, 1280), dtype="float32") = model_params[1105]
            lv1454 = R.call_tir(cls.matmul16, (inp_2, lv987), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv988: R.Tensor((2048, 1280), dtype="float32") = model_params[1106]
            lv1456 = R.call_tir(cls.matmul16, (inp_2, lv988), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv408 = R.call_tir(cls.fused_reshape18_transpose17, (lv1452,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv409 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1454,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv410 = R.call_tir(cls.fused_reshape20_transpose21, (lv1456,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv411 = R.call_tir(cls.fused_matmul17_multiply8, (lv408, lv409, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1468 = R.call_tir(cls.softmax3, (lv411,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1469 = R.call_tir(cls.matmul18, (lv1468, lv410), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv412 = R.call_tir(cls.fused_transpose19_reshape19, (lv1469,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv989: R.Tensor((1280, 1280), dtype="float32") = model_params[1107]
            lv990_1: R.Tensor((1280,), dtype="float32") = model_params[234]
            lv413 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv412, lv989, lv990_1, lv407), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv991: R.Tensor((1280,), dtype="float32") = model_params[242]
            lv992_1: R.Tensor((1280,), dtype="float32") = model_params[241]
            lv1477 = R.call_tir(cls.layer_norm1, (lv413, lv991, lv992_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv993: R.Tensor((1280, 10240), dtype="float32") = model_params[1108]
            lv994_1: R.Tensor((10240,), dtype="float32") = model_params[235]
            lv414 = R.call_tir(cls.fused_matmul19_add17, (lv1477, lv993, lv994_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv415 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv414,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv995: R.Tensor((5120, 1280), dtype="float32") = model_params[1109]
            lv996_1: R.Tensor((1280,), dtype="float32") = model_params[236]
            lv416 = R.call_tir(cls.fused_matmul20_add15_add16, (lv415, lv995, lv996_1, lv413), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv997: R.Tensor((1280,), dtype="float32") = model_params[248]
            lv998: R.Tensor((1280,), dtype="float32") = model_params[247]
            lv1490 = R.call_tir(cls.layer_norm1, (lv416, lv997, lv998), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv999: R.Tensor((1280, 1280), dtype="float32") = model_params[1110]
            lv1492 = R.call_tir(cls.matmul13, (lv1490, lv999), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1000: R.Tensor((1280, 1280), dtype="float32") = model_params[1111]
            lv1494 = R.call_tir(cls.matmul13, (lv1490, lv1000), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1001: R.Tensor((1280, 1280), dtype="float32") = model_params[1112]
            lv1496 = R.call_tir(cls.matmul13, (lv1490, lv1001), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv417 = R.call_tir(cls.fused_reshape18_transpose17, (lv1492,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv418 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1494,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv419 = R.call_tir(cls.fused_reshape18_transpose17, (lv1496,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv420 = R.call_tir(cls.fused_matmul14_multiply7, (lv417, lv418, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1508 = R.call_tir(cls.softmax2, (lv420,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1509 = R.call_tir(cls.matmul15, (lv1508, lv419), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv421 = R.call_tir(cls.fused_transpose19_reshape19, (lv1509,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1002: R.Tensor((1280, 1280), dtype="float32") = model_params[1113]
            lv1003: R.Tensor((1280,), dtype="float32") = model_params[243]
            lv422 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv421, lv1002, lv1003, lv416), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1004: R.Tensor((1280,), dtype="float32") = model_params[250]
            lv1005: R.Tensor((1280,), dtype="float32") = model_params[249]
            lv1517 = R.call_tir(cls.layer_norm1, (lv422, lv1004, lv1005), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1006: R.Tensor((1280, 1280), dtype="float32") = model_params[1114]
            lv1519 = R.call_tir(cls.matmul13, (lv1517, lv1006), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1007: R.Tensor((2048, 1280), dtype="float32") = model_params[1115]
            lv1521 = R.call_tir(cls.matmul16, (inp_2, lv1007), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1008_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1116]
            lv1523 = R.call_tir(cls.matmul16, (inp_2, lv1008_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv423 = R.call_tir(cls.fused_reshape18_transpose17, (lv1519,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv424 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1521,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv425 = R.call_tir(cls.fused_reshape20_transpose21, (lv1523,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv426 = R.call_tir(cls.fused_matmul17_multiply8, (lv423, lv424, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1535 = R.call_tir(cls.softmax3, (lv426,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1536 = R.call_tir(cls.matmul18, (lv1535, lv425), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv427 = R.call_tir(cls.fused_transpose19_reshape19, (lv1536,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1009_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1117]
            lv1010: R.Tensor((1280,), dtype="float32") = model_params[244]
            lv428 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv427, lv1009_1, lv1010, lv422), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1011: R.Tensor((1280,), dtype="float32") = model_params[252]
            lv1012: R.Tensor((1280,), dtype="float32") = model_params[251]
            lv1544 = R.call_tir(cls.layer_norm1, (lv428, lv1011, lv1012), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1013: R.Tensor((1280, 10240), dtype="float32") = model_params[1118]
            lv1014: R.Tensor((10240,), dtype="float32") = model_params[245]
            lv429 = R.call_tir(cls.fused_matmul19_add17, (lv1544, lv1013, lv1014), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv430 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv429,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1015: R.Tensor((5120, 1280), dtype="float32") = model_params[1119]
            lv1016: R.Tensor((1280,), dtype="float32") = model_params[246]
            lv431_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv430, lv1015, lv1016, lv428), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1017_1: R.Tensor((1280,), dtype="float32") = model_params[258]
            lv1018: R.Tensor((1280,), dtype="float32") = model_params[257]
            lv1557 = R.call_tir(cls.layer_norm1, (lv431_1, lv1017_1, lv1018), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1019_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1120]
            lv1559 = R.call_tir(cls.matmul13, (lv1557, lv1019_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1020: R.Tensor((1280, 1280), dtype="float32") = model_params[1121]
            lv1561 = R.call_tir(cls.matmul13, (lv1557, lv1020), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1021_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1122]
            lv1563 = R.call_tir(cls.matmul13, (lv1557, lv1021_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv432 = R.call_tir(cls.fused_reshape18_transpose17, (lv1559,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv433 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1561,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv434 = R.call_tir(cls.fused_reshape18_transpose17, (lv1563,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv435 = R.call_tir(cls.fused_matmul14_multiply7, (lv432, lv433, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1575 = R.call_tir(cls.softmax2, (lv435,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1576 = R.call_tir(cls.matmul15, (lv1575, lv434), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv436_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1576,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1022: R.Tensor((1280, 1280), dtype="float32") = model_params[1123]
            lv1023_1: R.Tensor((1280,), dtype="float32") = model_params[253]
            lv437 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv436_1, lv1022, lv1023_1, lv431_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1024: R.Tensor((1280,), dtype="float32") = model_params[260]
            lv1025: R.Tensor((1280,), dtype="float32") = model_params[259]
            lv1584 = R.call_tir(cls.layer_norm1, (lv437, lv1024, lv1025), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1026: R.Tensor((1280, 1280), dtype="float32") = model_params[1124]
            lv1586 = R.call_tir(cls.matmul13, (lv1584, lv1026), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1027: R.Tensor((2048, 1280), dtype="float32") = model_params[1125]
            lv1588 = R.call_tir(cls.matmul16, (inp_2, lv1027), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1028: R.Tensor((2048, 1280), dtype="float32") = model_params[1126]
            lv1590 = R.call_tir(cls.matmul16, (inp_2, lv1028), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv438 = R.call_tir(cls.fused_reshape18_transpose17, (lv1586,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv439 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1588,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv440 = R.call_tir(cls.fused_reshape20_transpose21, (lv1590,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv441 = R.call_tir(cls.fused_matmul17_multiply8, (lv438, lv439, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1602 = R.call_tir(cls.softmax3, (lv441,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1603 = R.call_tir(cls.matmul18, (lv1602, lv440), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv442 = R.call_tir(cls.fused_transpose19_reshape19, (lv1603,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1029: R.Tensor((1280, 1280), dtype="float32") = model_params[1127]
            lv1030: R.Tensor((1280,), dtype="float32") = model_params[254]
            lv443 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv442, lv1029, lv1030, lv437), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1031: R.Tensor((1280,), dtype="float32") = model_params[262]
            lv1032: R.Tensor((1280,), dtype="float32") = model_params[261]
            lv1611 = R.call_tir(cls.layer_norm1, (lv443, lv1031, lv1032), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1033: R.Tensor((1280, 10240), dtype="float32") = model_params[1128]
            lv1034: R.Tensor((10240,), dtype="float32") = model_params[255]
            lv444 = R.call_tir(cls.fused_matmul19_add17, (lv1611, lv1033, lv1034), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv445 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv444,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1035_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1129]
            lv1036_1: R.Tensor((1280,), dtype="float32") = model_params[256]
            lv446 = R.call_tir(cls.fused_matmul20_add15_add16, (lv445, lv1035_1, lv1036_1, lv443), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1037: R.Tensor((1280,), dtype="float32") = model_params[268]
            lv1038: R.Tensor((1280,), dtype="float32") = model_params[267]
            lv1624 = R.call_tir(cls.layer_norm1, (lv446, lv1037, lv1038), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1039: R.Tensor((1280, 1280), dtype="float32") = model_params[1130]
            lv1626 = R.call_tir(cls.matmul13, (lv1624, lv1039), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1040: R.Tensor((1280, 1280), dtype="float32") = model_params[1131]
            lv1628 = R.call_tir(cls.matmul13, (lv1624, lv1040), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1041: R.Tensor((1280, 1280), dtype="float32") = model_params[1132]
            lv1630 = R.call_tir(cls.matmul13, (lv1624, lv1041), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv447 = R.call_tir(cls.fused_reshape18_transpose17, (lv1626,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv448_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1628,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv449 = R.call_tir(cls.fused_reshape18_transpose17, (lv1630,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv450 = R.call_tir(cls.fused_matmul14_multiply7, (lv447, lv448_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1642 = R.call_tir(cls.softmax2, (lv450,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1643 = R.call_tir(cls.matmul15, (lv1642, lv449), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv451 = R.call_tir(cls.fused_transpose19_reshape19, (lv1643,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1042: R.Tensor((1280, 1280), dtype="float32") = model_params[1133]
            lv1043: R.Tensor((1280,), dtype="float32") = model_params[263]
            lv452 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv451, lv1042, lv1043, lv446), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1044_1: R.Tensor((1280,), dtype="float32") = model_params[270]
            lv1045: R.Tensor((1280,), dtype="float32") = model_params[269]
            lv1651 = R.call_tir(cls.layer_norm1, (lv452, lv1044_1, lv1045), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1046: R.Tensor((1280, 1280), dtype="float32") = model_params[1134]
            lv1653 = R.call_tir(cls.matmul13, (lv1651, lv1046), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1047: R.Tensor((2048, 1280), dtype="float32") = model_params[1135]
            lv1655 = R.call_tir(cls.matmul16, (inp_2, lv1047), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1048: R.Tensor((2048, 1280), dtype="float32") = model_params[1136]
            lv1657 = R.call_tir(cls.matmul16, (inp_2, lv1048), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv453 = R.call_tir(cls.fused_reshape18_transpose17, (lv1653,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv454_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1655,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv455 = R.call_tir(cls.fused_reshape20_transpose21, (lv1657,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv456_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv453, lv454_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1669 = R.call_tir(cls.softmax3, (lv456_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1670 = R.call_tir(cls.matmul18, (lv1669, lv455), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv457 = R.call_tir(cls.fused_transpose19_reshape19, (lv1670,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1049: R.Tensor((1280, 1280), dtype="float32") = model_params[1137]
            lv1050: R.Tensor((1280,), dtype="float32") = model_params[264]
            lv458_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv457, lv1049, lv1050, lv452), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1051: R.Tensor((1280,), dtype="float32") = model_params[272]
            lv1052: R.Tensor((1280,), dtype="float32") = model_params[271]
            lv1678 = R.call_tir(cls.layer_norm1, (lv458_1, lv1051, lv1052), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1053: R.Tensor((1280, 10240), dtype="float32") = model_params[1138]
            lv1054: R.Tensor((10240,), dtype="float32") = model_params[265]
            lv459 = R.call_tir(cls.fused_matmul19_add17, (lv1678, lv1053, lv1054), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv460_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv459,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1055: R.Tensor((5120, 1280), dtype="float32") = model_params[1139]
            lv1056: R.Tensor((1280,), dtype="float32") = model_params[266]
            lv461 = R.call_tir(cls.fused_matmul20_add15_add16, (lv460_1, lv1055, lv1056, lv458_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1057_1: R.Tensor((1280,), dtype="float32") = model_params[278]
            lv1058: R.Tensor((1280,), dtype="float32") = model_params[277]
            lv1691 = R.call_tir(cls.layer_norm1, (lv461, lv1057_1, lv1058), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1059_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1140]
            lv1693 = R.call_tir(cls.matmul13, (lv1691, lv1059_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1060: R.Tensor((1280, 1280), dtype="float32") = model_params[1141]
            lv1695 = R.call_tir(cls.matmul13, (lv1691, lv1060), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1061_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1142]
            lv1697 = R.call_tir(cls.matmul13, (lv1691, lv1061_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv462 = R.call_tir(cls.fused_reshape18_transpose17, (lv1693,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv463 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1695,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv464 = R.call_tir(cls.fused_reshape18_transpose17, (lv1697,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv465 = R.call_tir(cls.fused_matmul14_multiply7, (lv462, lv463, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1709 = R.call_tir(cls.softmax2, (lv465,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1710 = R.call_tir(cls.matmul15, (lv1709, lv464), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv466 = R.call_tir(cls.fused_transpose19_reshape19, (lv1710,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1062: R.Tensor((1280, 1280), dtype="float32") = model_params[1143]
            lv1063_1: R.Tensor((1280,), dtype="float32") = model_params[273]
            lv467 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv466, lv1062, lv1063_1, lv461), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1064: R.Tensor((1280,), dtype="float32") = model_params[280]
            lv1065: R.Tensor((1280,), dtype="float32") = model_params[279]
            lv1718 = R.call_tir(cls.layer_norm1, (lv467, lv1064, lv1065), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1066: R.Tensor((1280, 1280), dtype="float32") = model_params[1144]
            lv1720 = R.call_tir(cls.matmul13, (lv1718, lv1066), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1067: R.Tensor((2048, 1280), dtype="float32") = model_params[1145]
            lv1722 = R.call_tir(cls.matmul16, (inp_2, lv1067), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1068: R.Tensor((2048, 1280), dtype="float32") = model_params[1146]
            lv1724 = R.call_tir(cls.matmul16, (inp_2, lv1068), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv468 = R.call_tir(cls.fused_reshape18_transpose17, (lv1720,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv469 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1722,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv470 = R.call_tir(cls.fused_reshape20_transpose21, (lv1724,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv471 = R.call_tir(cls.fused_matmul17_multiply8, (lv468, lv469, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1736 = R.call_tir(cls.softmax3, (lv471,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1737 = R.call_tir(cls.matmul18, (lv1736, lv470), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv472_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1737,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1069: R.Tensor((1280, 1280), dtype="float32") = model_params[1147]
            lv1070: R.Tensor((1280,), dtype="float32") = model_params[274]
            lv473_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv472_1, lv1069, lv1070, lv467), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1071: R.Tensor((1280,), dtype="float32") = model_params[282]
            lv1072: R.Tensor((1280,), dtype="float32") = model_params[281]
            lv1745 = R.call_tir(cls.layer_norm1, (lv473_1, lv1071, lv1072), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1073: R.Tensor((1280, 10240), dtype="float32") = model_params[1148]
            lv1074: R.Tensor((10240,), dtype="float32") = model_params[275]
            lv474 = R.call_tir(cls.fused_matmul19_add17, (lv1745, lv1073, lv1074), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv475 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv474,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1075_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1149]
            lv1076_1: R.Tensor((1280,), dtype="float32") = model_params[276]
            lv476 = R.call_tir(cls.fused_matmul20_add15_add16, (lv475, lv1075_1, lv1076_1, lv473_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1077: R.Tensor((1280,), dtype="float32") = model_params[288]
            lv1078: R.Tensor((1280,), dtype="float32") = model_params[287]
            lv1758 = R.call_tir(cls.layer_norm1, (lv476, lv1077, lv1078), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1079: R.Tensor((1280, 1280), dtype="float32") = model_params[1150]
            lv1760 = R.call_tir(cls.matmul13, (lv1758, lv1079), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1080: R.Tensor((1280, 1280), dtype="float32") = model_params[1151]
            lv1762 = R.call_tir(cls.matmul13, (lv1758, lv1080), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1081: R.Tensor((1280, 1280), dtype="float32") = model_params[1152]
            lv1764 = R.call_tir(cls.matmul13, (lv1758, lv1081), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv477 = R.call_tir(cls.fused_reshape18_transpose17, (lv1760,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv478 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1762,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv479 = R.call_tir(cls.fused_reshape18_transpose17, (lv1764,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv480 = R.call_tir(cls.fused_matmul14_multiply7, (lv477, lv478, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1776 = R.call_tir(cls.softmax2, (lv480,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1777 = R.call_tir(cls.matmul15, (lv1776, lv479), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv481_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1777,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1082: R.Tensor((1280, 1280), dtype="float32") = model_params[1153]
            lv1083: R.Tensor((1280,), dtype="float32") = model_params[283]
            lv482 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv481_1, lv1082, lv1083, lv476), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1084_1: R.Tensor((1280,), dtype="float32") = model_params[290]
            lv1085: R.Tensor((1280,), dtype="float32") = model_params[289]
            lv1785 = R.call_tir(cls.layer_norm1, (lv482, lv1084_1, lv1085), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1086_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1154]
            lv1787 = R.call_tir(cls.matmul13, (lv1785, lv1086_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1087: R.Tensor((2048, 1280), dtype="float32") = model_params[1155]
            lv1789 = R.call_tir(cls.matmul16, (inp_2, lv1087), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1088_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1156]
            lv1791 = R.call_tir(cls.matmul16, (inp_2, lv1088_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv483_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1787,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv484 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1789,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv485_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv1791,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv486 = R.call_tir(cls.fused_matmul17_multiply8, (lv483_1, lv484, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1803 = R.call_tir(cls.softmax3, (lv486,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1804 = R.call_tir(cls.matmul18, (lv1803, lv485_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv487_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1804,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1089: R.Tensor((1280, 1280), dtype="float32") = model_params[1157]
            lv1090_1: R.Tensor((1280,), dtype="float32") = model_params[284]
            lv488 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv487_1, lv1089, lv1090_1, lv482), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1091: R.Tensor((1280,), dtype="float32") = model_params[292]
            lv1092: R.Tensor((1280,), dtype="float32") = model_params[291]
            lv1812 = R.call_tir(cls.layer_norm1, (lv488, lv1091, lv1092), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1093: R.Tensor((1280, 10240), dtype="float32") = model_params[1158]
            lv1094: R.Tensor((10240,), dtype="float32") = model_params[285]
            lv489 = R.call_tir(cls.fused_matmul19_add17, (lv1812, lv1093, lv1094), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv490 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv489,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1095: R.Tensor((5120, 1280), dtype="float32") = model_params[1159]
            lv1096: R.Tensor((1280,), dtype="float32") = model_params[286]
            lv491 = R.call_tir(cls.fused_matmul20_add15_add16, (lv490, lv1095, lv1096, lv488), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1097: R.Tensor((1280, 1280), dtype="float32") = model_params[1160]
            lv1098: R.Tensor((1280,), dtype="float32") = model_params[192]
            lv492 = R.call_tir(cls.fused_matmul13_add15, (lv491, lv1097, lv1098), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv493 = R.call_tir(cls.fused_reshape21_transpose25_add14, (lv492, lv339), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1099: R.Tensor((1280,), dtype="float32") = model_params[415]
            lv1100: R.Tensor((1280,), dtype="float32") = model_params[414]
            lv494 = R.call_tir(cls.fused_group_norm5_silu5, (lv493, lv1099, lv1100), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1836 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1101: R.Tensor((1280, 1280), dtype="float32") = model_params[1162]
            lv1102_1: R.Tensor((1280,), dtype="float32") = model_params[418]
            lv495 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv1836, lv1101, lv1102_1), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1841 = R.call_tir(cls.reshape16, (lv495,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv1103_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[412]
            lv1104: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1161]
            lv496 = R.call_tir(cls.fused_conv2d8_add13_add13, (lv494, lv1103_1, lv1104, lv1841), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1105: R.Tensor((1280,), dtype="float32") = model_params[417]
            lv1106: R.Tensor((1280,), dtype="float32") = model_params[416]
            lv497 = R.call_tir(cls.fused_group_norm5_silu5, (lv496, lv1105, lv1106), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1107: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[413]
            lv1108: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1163]
            lv498 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv497, lv1107, lv1108, lv493), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1109: R.Tensor((1280,), dtype="float32") = model_params[309]
            lv1110: R.Tensor((1280,), dtype="float32") = model_params[308]
            lv1850 = R.call_tir(cls.group_norm6, (lv498, lv1109, lv1110), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv499_1 = R.call_tir(cls.fused_transpose16_reshape17, (lv1850,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1111_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1164]
            lv1112: R.Tensor((1280,), dtype="float32") = model_params[310]
            lv500_1 = R.call_tir(cls.fused_matmul13_add15, (lv499_1, lv1111_1, lv1112), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1113: R.Tensor((1280,), dtype="float32") = model_params[317]
            lv1114: R.Tensor((1280,), dtype="float32") = model_params[316]
            lv1856 = R.call_tir(cls.layer_norm1, (lv500_1, lv1113, lv1114), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1115: R.Tensor((1280, 1280), dtype="float32") = model_params[1165]
            lv1858 = R.call_tir(cls.matmul13, (lv1856, lv1115), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1116: R.Tensor((1280, 1280), dtype="float32") = model_params[1166]
            lv1860 = R.call_tir(cls.matmul13, (lv1856, lv1116), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1117: R.Tensor((1280, 1280), dtype="float32") = model_params[1167]
            lv1862 = R.call_tir(cls.matmul13, (lv1856, lv1117), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv501 = R.call_tir(cls.fused_reshape18_transpose17, (lv1858,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv502 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1860,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv503 = R.call_tir(cls.fused_reshape18_transpose17, (lv1862,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv504 = R.call_tir(cls.fused_matmul14_multiply7, (lv501, lv502, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1874 = R.call_tir(cls.softmax2, (lv504,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1875 = R.call_tir(cls.matmul15, (lv1874, lv503), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv505 = R.call_tir(cls.fused_transpose19_reshape19, (lv1875,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1118: R.Tensor((1280, 1280), dtype="float32") = model_params[1168]
            lv1119: R.Tensor((1280,), dtype="float32") = model_params[312]
            lv506 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv505, lv1118, lv1119, lv500_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1120: R.Tensor((1280,), dtype="float32") = model_params[319]
            lv1121: R.Tensor((1280,), dtype="float32") = model_params[318]
            lv1883 = R.call_tir(cls.layer_norm1, (lv506, lv1120, lv1121), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1122: R.Tensor((1280, 1280), dtype="float32") = model_params[1169]
            lv1885 = R.call_tir(cls.matmul13, (lv1883, lv1122), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1123: R.Tensor((2048, 1280), dtype="float32") = model_params[1170]
            lv1887 = R.call_tir(cls.matmul16, (inp_2, lv1123), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1124: R.Tensor((2048, 1280), dtype="float32") = model_params[1171]
            lv1889 = R.call_tir(cls.matmul16, (inp_2, lv1124), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv507 = R.call_tir(cls.fused_reshape18_transpose17, (lv1885,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv508_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1887,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv509 = R.call_tir(cls.fused_reshape20_transpose21, (lv1889,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv510 = R.call_tir(cls.fused_matmul17_multiply8, (lv507, lv508_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1901 = R.call_tir(cls.softmax3, (lv510,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1902 = R.call_tir(cls.matmul18, (lv1901, lv509), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv511 = R.call_tir(cls.fused_transpose19_reshape19, (lv1902,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1125: R.Tensor((1280, 1280), dtype="float32") = model_params[1172]
            lv1126: R.Tensor((1280,), dtype="float32") = model_params[313]
            lv512 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv511, lv1125, lv1126, lv506), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1127: R.Tensor((1280,), dtype="float32") = model_params[321]
            lv1128: R.Tensor((1280,), dtype="float32") = model_params[320]
            lv1910 = R.call_tir(cls.layer_norm1, (lv512, lv1127, lv1128), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1129: R.Tensor((1280, 10240), dtype="float32") = model_params[1173]
            lv1130: R.Tensor((10240,), dtype="float32") = model_params[314]
            lv513 = R.call_tir(cls.fused_matmul19_add17, (lv1910, lv1129, lv1130), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv514 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv513,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1131: R.Tensor((5120, 1280), dtype="float32") = model_params[1174]
            lv1132: R.Tensor((1280,), dtype="float32") = model_params[315]
            lv515 = R.call_tir(cls.fused_matmul20_add15_add16, (lv514, lv1131, lv1132, lv512), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1133: R.Tensor((1280,), dtype="float32") = model_params[327]
            lv1134: R.Tensor((1280,), dtype="float32") = model_params[326]
            lv1923 = R.call_tir(cls.layer_norm1, (lv515, lv1133, lv1134), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1135_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1175]
            lv1925 = R.call_tir(cls.matmul13, (lv1923, lv1135_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1136: R.Tensor((1280, 1280), dtype="float32") = model_params[1176]
            lv1927 = R.call_tir(cls.matmul13, (lv1923, lv1136), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1137: R.Tensor((1280, 1280), dtype="float32") = model_params[1177]
            lv1929 = R.call_tir(cls.matmul13, (lv1923, lv1137), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv516 = R.call_tir(cls.fused_reshape18_transpose17, (lv1925,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv517_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1927,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv518_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1929,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv519_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv516, lv517_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1941 = R.call_tir(cls.softmax2, (lv519_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv1942 = R.call_tir(cls.matmul15, (lv1941, lv518_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv520_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1942,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1138: R.Tensor((1280, 1280), dtype="float32") = model_params[1178]
            lv1139: R.Tensor((1280,), dtype="float32") = model_params[322]
            lv521_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv520_1, lv1138, lv1139, lv515), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1140_1: R.Tensor((1280,), dtype="float32") = model_params[329]
            lv1141: R.Tensor((1280,), dtype="float32") = model_params[328]
            lv1950 = R.call_tir(cls.layer_norm1, (lv521_2, lv1140_1, lv1141), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1142: R.Tensor((1280, 1280), dtype="float32") = model_params[1179]
            lv1952 = R.call_tir(cls.matmul13, (lv1950, lv1142), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1143: R.Tensor((2048, 1280), dtype="float32") = model_params[1180]
            lv1954 = R.call_tir(cls.matmul16, (inp_2, lv1143), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1144: R.Tensor((2048, 1280), dtype="float32") = model_params[1181]
            lv1956 = R.call_tir(cls.matmul16, (inp_2, lv1144), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv522_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1952,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv523_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv1954,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv524_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv1956,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv525_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv522_1, lv523_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1968 = R.call_tir(cls.softmax3, (lv525_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv1969 = R.call_tir(cls.matmul18, (lv1968, lv524_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv526_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv1969,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1145: R.Tensor((1280, 1280), dtype="float32") = model_params[1182]
            lv1146: R.Tensor((1280,), dtype="float32") = model_params[323]
            lv527_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv526_1, lv1145, lv1146, lv521_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1147: R.Tensor((1280,), dtype="float32") = model_params[331]
            lv1148: R.Tensor((1280,), dtype="float32") = model_params[330]
            lv1977 = R.call_tir(cls.layer_norm1, (lv527_2, lv1147, lv1148), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1149_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1183]
            lv1150: R.Tensor((10240,), dtype="float32") = model_params[324]
            lv528_1 = R.call_tir(cls.fused_matmul19_add17, (lv1977, lv1149_1, lv1150), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv529_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv528_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1151: R.Tensor((5120, 1280), dtype="float32") = model_params[1184]
            lv1152: R.Tensor((1280,), dtype="float32") = model_params[325]
            lv530_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv529_1, lv1151, lv1152, lv527_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1153: R.Tensor((1280,), dtype="float32") = model_params[337]
            lv1154: R.Tensor((1280,), dtype="float32") = model_params[336]
            lv1990 = R.call_tir(cls.layer_norm1, (lv530_1, lv1153, lv1154), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1155_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1185]
            lv1992 = R.call_tir(cls.matmul13, (lv1990, lv1155_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1156: R.Tensor((1280, 1280), dtype="float32") = model_params[1186]
            lv1994 = R.call_tir(cls.matmul13, (lv1990, lv1156), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1157_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1187]
            lv1996 = R.call_tir(cls.matmul13, (lv1990, lv1157_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv531_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1992,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv532_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv1994,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv533_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv1996,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv534_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv531_1, lv532_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2008 = R.call_tir(cls.softmax2, (lv534_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2009 = R.call_tir(cls.matmul15, (lv2008, lv533_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv535_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2009,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1158: R.Tensor((1280, 1280), dtype="float32") = model_params[1188]
            lv1159_1: R.Tensor((1280,), dtype="float32") = model_params[332]
            lv536_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv535_1, lv1158, lv1159_1, lv530_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1160: R.Tensor((1280,), dtype="float32") = model_params[339]
            lv1161_1: R.Tensor((1280,), dtype="float32") = model_params[338]
            lv2017 = R.call_tir(cls.layer_norm1, (lv536_1, lv1160, lv1161_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1162: R.Tensor((1280, 1280), dtype="float32") = model_params[1189]
            lv2019 = R.call_tir(cls.matmul13, (lv2017, lv1162), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1163: R.Tensor((2048, 1280), dtype="float32") = model_params[1190]
            lv2021 = R.call_tir(cls.matmul16, (inp_2, lv1163), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1164: R.Tensor((2048, 1280), dtype="float32") = model_params[1191]
            lv2023 = R.call_tir(cls.matmul16, (inp_2, lv1164), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv537_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2019,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv538_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2021,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv539_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv2023,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv540_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv537_1, lv538_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2035 = R.call_tir(cls.softmax3, (lv540_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2036 = R.call_tir(cls.matmul18, (lv2035, lv539_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv541_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2036,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1165: R.Tensor((1280, 1280), dtype="float32") = model_params[1192]
            lv1166: R.Tensor((1280,), dtype="float32") = model_params[333]
            lv542_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv541_1, lv1165, lv1166, lv536_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1167: R.Tensor((1280,), dtype="float32") = model_params[341]
            lv1168: R.Tensor((1280,), dtype="float32") = model_params[340]
            lv2044 = R.call_tir(cls.layer_norm1, (lv542_1, lv1167, lv1168), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1169: R.Tensor((1280, 10240), dtype="float32") = model_params[1193]
            lv1170: R.Tensor((10240,), dtype="float32") = model_params[334]
            lv543_1 = R.call_tir(cls.fused_matmul19_add17, (lv2044, lv1169, lv1170), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv544_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv543_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1171: R.Tensor((5120, 1280), dtype="float32") = model_params[1194]
            lv1172: R.Tensor((1280,), dtype="float32") = model_params[335]
            lv545_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv544_1, lv1171, lv1172, lv542_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1173_1: R.Tensor((1280,), dtype="float32") = model_params[347]
            lv1174_1: R.Tensor((1280,), dtype="float32") = model_params[346]
            lv2057 = R.call_tir(cls.layer_norm1, (lv545_1, lv1173_1, lv1174_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1175: R.Tensor((1280, 1280), dtype="float32") = model_params[1195]
            lv2059 = R.call_tir(cls.matmul13, (lv2057, lv1175), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1176: R.Tensor((1280, 1280), dtype="float32") = model_params[1196]
            lv2061 = R.call_tir(cls.matmul13, (lv2057, lv1176), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1177: R.Tensor((1280, 1280), dtype="float32") = model_params[1197]
            lv2063 = R.call_tir(cls.matmul13, (lv2057, lv1177), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv546_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2059,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv547_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2061,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv548_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2063,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv549_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv546_1, lv547_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2075 = R.call_tir(cls.softmax2, (lv549_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2076 = R.call_tir(cls.matmul15, (lv2075, lv548_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv550_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv2076,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1178: R.Tensor((1280, 1280), dtype="float32") = model_params[1198]
            lv1179: R.Tensor((1280,), dtype="float32") = model_params[342]
            lv551_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv550_2, lv1178, lv1179, lv545_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1180: R.Tensor((1280,), dtype="float32") = model_params[349]
            lv1181: R.Tensor((1280,), dtype="float32") = model_params[348]
            lv2084 = R.call_tir(cls.layer_norm1, (lv551_1, lv1180, lv1181), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1182_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1199]
            lv2086 = R.call_tir(cls.matmul13, (lv2084, lv1182_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1183: R.Tensor((2048, 1280), dtype="float32") = model_params[1200]
            lv2088 = R.call_tir(cls.matmul16, (inp_2, lv1183), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1184_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1201]
            lv2090 = R.call_tir(cls.matmul16, (inp_2, lv1184_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv552_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2086,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv553_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2088,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv554_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv2090,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv555_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv552_2, lv553_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2102 = R.call_tir(cls.softmax3, (lv555_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2103 = R.call_tir(cls.matmul18, (lv2102, lv554_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv556_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2103,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1185: R.Tensor((1280, 1280), dtype="float32") = model_params[1202]
            lv1186_1: R.Tensor((1280,), dtype="float32") = model_params[343]
            lv557_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv556_1, lv1185, lv1186_1, lv551_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1187: R.Tensor((1280,), dtype="float32") = model_params[351]
            lv1188_1: R.Tensor((1280,), dtype="float32") = model_params[350]
            lv2111 = R.call_tir(cls.layer_norm1, (lv557_1, lv1187, lv1188_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1189: R.Tensor((1280, 10240), dtype="float32") = model_params[1203]
            lv1190: R.Tensor((10240,), dtype="float32") = model_params[344]
            lv558_1 = R.call_tir(cls.fused_matmul19_add17, (lv2111, lv1189, lv1190), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv559_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv558_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1191: R.Tensor((5120, 1280), dtype="float32") = model_params[1204]
            lv1192: R.Tensor((1280,), dtype="float32") = model_params[345]
            lv560_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv559_1, lv1191, lv1192, lv557_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1193: R.Tensor((1280,), dtype="float32") = model_params[357]
            lv1194: R.Tensor((1280,), dtype="float32") = model_params[356]
            lv2124 = R.call_tir(cls.layer_norm1, (lv560_1, lv1193, lv1194), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1195: R.Tensor((1280, 1280), dtype="float32") = model_params[1205]
            lv2126 = R.call_tir(cls.matmul13, (lv2124, lv1195), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1196: R.Tensor((1280, 1280), dtype="float32") = model_params[1206]
            lv2128 = R.call_tir(cls.matmul13, (lv2124, lv1196), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1197: R.Tensor((1280, 1280), dtype="float32") = model_params[1207]
            lv2130 = R.call_tir(cls.matmul13, (lv2124, lv1197), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv561_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2126,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv562_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2128,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv563_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2130,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv564_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv561_1, lv562_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2142 = R.call_tir(cls.softmax2, (lv564_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2143 = R.call_tir(cls.matmul15, (lv2142, lv563_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv565_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2143,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1198: R.Tensor((1280, 1280), dtype="float32") = model_params[1208]
            lv1199: R.Tensor((1280,), dtype="float32") = model_params[352]
            lv566_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv565_1, lv1198, lv1199, lv560_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1200_1: R.Tensor((1280,), dtype="float32") = model_params[359]
            lv1201_1: R.Tensor((1280,), dtype="float32") = model_params[358]
            lv2151 = R.call_tir(cls.layer_norm1, (lv566_2, lv1200_1, lv1201_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1202: R.Tensor((1280, 1280), dtype="float32") = model_params[1209]
            lv2153 = R.call_tir(cls.matmul13, (lv2151, lv1202), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1203: R.Tensor((2048, 1280), dtype="float32") = model_params[1210]
            lv2155 = R.call_tir(cls.matmul16, (inp_2, lv1203), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1204: R.Tensor((2048, 1280), dtype="float32") = model_params[1211]
            lv2157 = R.call_tir(cls.matmul16, (inp_2, lv1204), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv567_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2153,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv568_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2155,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv569_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2157,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv570_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv567_2, lv568_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2169 = R.call_tir(cls.softmax3, (lv570_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2170 = R.call_tir(cls.matmul18, (lv2169, lv569_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv571_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2170,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1205: R.Tensor((1280, 1280), dtype="float32") = model_params[1212]
            lv1206: R.Tensor((1280,), dtype="float32") = model_params[353]
            lv572_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv571_1, lv1205, lv1206, lv566_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1207: R.Tensor((1280,), dtype="float32") = model_params[361]
            lv1208: R.Tensor((1280,), dtype="float32") = model_params[360]
            lv2178 = R.call_tir(cls.layer_norm1, (lv572_1, lv1207, lv1208), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1209_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1213]
            lv1210: R.Tensor((10240,), dtype="float32") = model_params[354]
            lv573_1 = R.call_tir(cls.fused_matmul19_add17, (lv2178, lv1209_1, lv1210), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv574_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv573_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1211: R.Tensor((5120, 1280), dtype="float32") = model_params[1214]
            lv1212: R.Tensor((1280,), dtype="float32") = model_params[355]
            lv575_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv574_1, lv1211, lv1212, lv572_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1213: R.Tensor((1280,), dtype="float32") = model_params[367]
            lv1214: R.Tensor((1280,), dtype="float32") = model_params[366]
            lv2191 = R.call_tir(cls.layer_norm1, (lv575_2, lv1213, lv1214), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1215: R.Tensor((1280, 1280), dtype="float32") = model_params[1215]
            lv2193 = R.call_tir(cls.matmul13, (lv2191, lv1215), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1216: R.Tensor((1280, 1280), dtype="float32") = model_params[1216]
            lv2195 = R.call_tir(cls.matmul13, (lv2191, lv1216), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1217: R.Tensor((1280, 1280), dtype="float32") = model_params[1217]
            lv2197 = R.call_tir(cls.matmul13, (lv2191, lv1217), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv576_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2193,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv577_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2195,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv578_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2197,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv579_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv576_1, lv577_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2209 = R.call_tir(cls.softmax2, (lv579_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2210 = R.call_tir(cls.matmul15, (lv2209, lv578_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv580_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2210,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1218: R.Tensor((1280, 1280), dtype="float32") = model_params[1218]
            lv1219: R.Tensor((1280,), dtype="float32") = model_params[362]
            lv581_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv580_1, lv1218, lv1219, lv575_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1220: R.Tensor((1280,), dtype="float32") = model_params[369]
            lv1221: R.Tensor((1280,), dtype="float32") = model_params[368]
            lv2218 = R.call_tir(cls.layer_norm1, (lv581_1, lv1220, lv1221), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1222_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1219]
            lv2220 = R.call_tir(cls.matmul13, (lv2218, lv1222_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1223: R.Tensor((2048, 1280), dtype="float32") = model_params[1220]
            lv2222 = R.call_tir(cls.matmul16, (inp_2, lv1223), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1224_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1221]
            lv2224 = R.call_tir(cls.matmul16, (inp_2, lv1224_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv582_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2220,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv583_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2222,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv584_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2224,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv585_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv582_1, lv583_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2236 = R.call_tir(cls.softmax3, (lv585_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2237 = R.call_tir(cls.matmul18, (lv2236, lv584_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv586_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2237,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1225: R.Tensor((1280, 1280), dtype="float32") = model_params[1222]
            lv1226_1: R.Tensor((1280,), dtype="float32") = model_params[363]
            lv587_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv586_1, lv1225, lv1226_1, lv581_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1227: R.Tensor((1280,), dtype="float32") = model_params[371]
            lv1228_1: R.Tensor((1280,), dtype="float32") = model_params[370]
            lv2245 = R.call_tir(cls.layer_norm1, (lv587_1, lv1227, lv1228_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1229: R.Tensor((1280, 10240), dtype="float32") = model_params[1223]
            lv1230: R.Tensor((10240,), dtype="float32") = model_params[364]
            lv588_2 = R.call_tir(cls.fused_matmul19_add17, (lv2245, lv1229, lv1230), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv589_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv588_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1231: R.Tensor((5120, 1280), dtype="float32") = model_params[1224]
            lv1232: R.Tensor((1280,), dtype="float32") = model_params[365]
            lv590_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv589_1, lv1231, lv1232, lv587_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1233: R.Tensor((1280,), dtype="float32") = model_params[377]
            lv1234: R.Tensor((1280,), dtype="float32") = model_params[376]
            lv2258 = R.call_tir(cls.layer_norm1, (lv590_2, lv1233, lv1234), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1235: R.Tensor((1280, 1280), dtype="float32") = model_params[1225]
            lv2260 = R.call_tir(cls.matmul13, (lv2258, lv1235), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1236: R.Tensor((1280, 1280), dtype="float32") = model_params[1226]
            lv2262 = R.call_tir(cls.matmul13, (lv2258, lv1236), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1237: R.Tensor((1280, 1280), dtype="float32") = model_params[1227]
            lv2264 = R.call_tir(cls.matmul13, (lv2258, lv1237), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv591_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2260,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv592_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2262,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv593_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2264,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv594_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv591_1, lv592_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2276 = R.call_tir(cls.softmax2, (lv594_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2277 = R.call_tir(cls.matmul15, (lv2276, lv593_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv595_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2277,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1238: R.Tensor((1280, 1280), dtype="float32") = model_params[1228]
            lv1239: R.Tensor((1280,), dtype="float32") = model_params[372]
            lv596_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv595_1, lv1238, lv1239, lv590_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1240_1: R.Tensor((1280,), dtype="float32") = model_params[379]
            lv1241_1: R.Tensor((1280,), dtype="float32") = model_params[378]
            lv2285 = R.call_tir(cls.layer_norm1, (lv596_1, lv1240_1, lv1241_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1242: R.Tensor((1280, 1280), dtype="float32") = model_params[1229]
            lv2287 = R.call_tir(cls.matmul13, (lv2285, lv1242), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1243: R.Tensor((2048, 1280), dtype="float32") = model_params[1230]
            lv2289 = R.call_tir(cls.matmul16, (inp_2, lv1243), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1244: R.Tensor((2048, 1280), dtype="float32") = model_params[1231]
            lv2291 = R.call_tir(cls.matmul16, (inp_2, lv1244), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv597_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2287,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv598_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2289,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv599_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2291,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv600_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv597_1, lv598_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2303 = R.call_tir(cls.softmax3, (lv600_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2304 = R.call_tir(cls.matmul18, (lv2303, lv599_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv601_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2304,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1245: R.Tensor((1280, 1280), dtype="float32") = model_params[1232]
            lv1246: R.Tensor((1280,), dtype="float32") = model_params[373]
            lv602_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv601_1, lv1245, lv1246, lv596_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1247: R.Tensor((1280,), dtype="float32") = model_params[381]
            lv1248: R.Tensor((1280,), dtype="float32") = model_params[380]
            lv2312 = R.call_tir(cls.layer_norm1, (lv602_1, lv1247, lv1248), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1249_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1233]
            lv1250: R.Tensor((10240,), dtype="float32") = model_params[374]
            lv603_1 = R.call_tir(cls.fused_matmul19_add17, (lv2312, lv1249_1, lv1250), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv604_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv603_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1251_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1234]
            lv1252: R.Tensor((1280,), dtype="float32") = model_params[375]
            lv605_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv604_1, lv1251_1, lv1252, lv602_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1253_1: R.Tensor((1280,), dtype="float32") = model_params[387]
            lv1254: R.Tensor((1280,), dtype="float32") = model_params[386]
            lv2325 = R.call_tir(cls.layer_norm1, (lv605_1, lv1253_1, lv1254), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1255_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1235]
            lv2327 = R.call_tir(cls.matmul13, (lv2325, lv1255_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1256: R.Tensor((1280, 1280), dtype="float32") = model_params[1236]
            lv2329 = R.call_tir(cls.matmul13, (lv2325, lv1256), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1257: R.Tensor((1280, 1280), dtype="float32") = model_params[1237]
            lv2331 = R.call_tir(cls.matmul13, (lv2325, lv1257), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv606_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2327,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv607_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2329,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv608_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2331,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv609_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv606_2, lv607_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2343 = R.call_tir(cls.softmax2, (lv609_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2344 = R.call_tir(cls.matmul15, (lv2343, lv608_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv610_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2344,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1258: R.Tensor((1280, 1280), dtype="float32") = model_params[1238]
            lv1259: R.Tensor((1280,), dtype="float32") = model_params[382]
            lv611_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv610_1, lv1258, lv1259, lv605_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1260: R.Tensor((1280,), dtype="float32") = model_params[389]
            lv1261: R.Tensor((1280,), dtype="float32") = model_params[388]
            lv2352 = R.call_tir(cls.layer_norm1, (lv611_1, lv1260, lv1261), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1262: R.Tensor((1280, 1280), dtype="float32") = model_params[1239]
            lv2354 = R.call_tir(cls.matmul13, (lv2352, lv1262), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1263: R.Tensor((2048, 1280), dtype="float32") = model_params[1240]
            lv2356 = R.call_tir(cls.matmul16, (inp_2, lv1263), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1264: R.Tensor((2048, 1280), dtype="float32") = model_params[1241]
            lv2358 = R.call_tir(cls.matmul16, (inp_2, lv1264), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv612_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2354,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv613_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2356,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv614_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2358,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv615_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv612_1, lv613_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2370 = R.call_tir(cls.softmax3, (lv615_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2371 = R.call_tir(cls.matmul18, (lv2370, lv614_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv616_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2371,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1265: R.Tensor((1280, 1280), dtype="float32") = model_params[1242]
            lv1266: R.Tensor((1280,), dtype="float32") = model_params[383]
            lv617_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv616_1, lv1265, lv1266, lv611_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1267_1: R.Tensor((1280,), dtype="float32") = model_params[391]
            lv1268_1: R.Tensor((1280,), dtype="float32") = model_params[390]
            lv2379 = R.call_tir(cls.layer_norm1, (lv617_2, lv1267_1, lv1268_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1269: R.Tensor((1280, 10240), dtype="float32") = model_params[1243]
            lv1270: R.Tensor((10240,), dtype="float32") = model_params[384]
            lv618_1 = R.call_tir(cls.fused_matmul19_add17, (lv2379, lv1269, lv1270), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv619_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv618_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1271: R.Tensor((5120, 1280), dtype="float32") = model_params[1244]
            lv1272: R.Tensor((1280,), dtype="float32") = model_params[385]
            lv620_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv619_2, lv1271, lv1272, lv617_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1273: R.Tensor((1280,), dtype="float32") = model_params[397]
            lv1274: R.Tensor((1280,), dtype="float32") = model_params[396]
            lv2392 = R.call_tir(cls.layer_norm1, (lv620_1, lv1273, lv1274), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1275: R.Tensor((1280, 1280), dtype="float32") = model_params[1245]
            lv2394 = R.call_tir(cls.matmul13, (lv2392, lv1275), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1276_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1246]
            lv2396 = R.call_tir(cls.matmul13, (lv2392, lv1276_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1277: R.Tensor((1280, 1280), dtype="float32") = model_params[1247]
            lv2398 = R.call_tir(cls.matmul13, (lv2392, lv1277), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv621_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2394,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv622_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2396,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv623_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2398,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv624_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv621_2, lv622_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2410 = R.call_tir(cls.softmax2, (lv624_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2411 = R.call_tir(cls.matmul15, (lv2410, lv623_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv625_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2411,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1278: R.Tensor((1280, 1280), dtype="float32") = model_params[1248]
            lv1279: R.Tensor((1280,), dtype="float32") = model_params[392]
            lv626_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv625_1, lv1278, lv1279, lv620_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1280: R.Tensor((1280,), dtype="float32") = model_params[399]
            lv1281: R.Tensor((1280,), dtype="float32") = model_params[398]
            lv2419 = R.call_tir(cls.layer_norm1, (lv626_1, lv1280, lv1281), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1282: R.Tensor((1280, 1280), dtype="float32") = model_params[1249]
            lv2421 = R.call_tir(cls.matmul13, (lv2419, lv1282), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1283: R.Tensor((2048, 1280), dtype="float32") = model_params[1250]
            lv2423 = R.call_tir(cls.matmul16, (inp_2, lv1283), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1284: R.Tensor((2048, 1280), dtype="float32") = model_params[1251]
            lv2425 = R.call_tir(cls.matmul16, (inp_2, lv1284), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv627_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2421,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv628_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2423,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv629_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2425,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv630_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv627_1, lv628_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2437 = R.call_tir(cls.softmax3, (lv630_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2438 = R.call_tir(cls.matmul18, (lv2437, lv629_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv631_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2438,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1285: R.Tensor((1280, 1280), dtype="float32") = model_params[1252]
            lv1286: R.Tensor((1280,), dtype="float32") = model_params[393]
            lv632_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv631_1, lv1285, lv1286, lv626_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1287: R.Tensor((1280,), dtype="float32") = model_params[401]
            lv1288: R.Tensor((1280,), dtype="float32") = model_params[400]
            lv2446 = R.call_tir(cls.layer_norm1, (lv632_1, lv1287, lv1288), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1289_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1253]
            lv1290: R.Tensor((10240,), dtype="float32") = model_params[394]
            lv633_2 = R.call_tir(cls.fused_matmul19_add17, (lv2446, lv1289_1, lv1290), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv634_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv633_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1291_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1254]
            lv1292: R.Tensor((1280,), dtype="float32") = model_params[395]
            lv635_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv634_2, lv1291_1, lv1292, lv632_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1293_1: R.Tensor((1280,), dtype="float32") = model_params[407]
            lv1294: R.Tensor((1280,), dtype="float32") = model_params[406]
            lv2459 = R.call_tir(cls.layer_norm1, (lv635_1, lv1293_1, lv1294), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1295_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1255]
            lv2461 = R.call_tir(cls.matmul13, (lv2459, lv1295_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1296: R.Tensor((1280, 1280), dtype="float32") = model_params[1256]
            lv2463 = R.call_tir(cls.matmul13, (lv2459, lv1296), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1297: R.Tensor((1280, 1280), dtype="float32") = model_params[1257]
            lv2465 = R.call_tir(cls.matmul13, (lv2459, lv1297), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv636_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2461,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv637_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2463,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv638_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2465,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv639_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv636_1, lv637_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2477 = R.call_tir(cls.softmax2, (lv639_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2478 = R.call_tir(cls.matmul15, (lv2477, lv638_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv640_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2478,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1298: R.Tensor((1280, 1280), dtype="float32") = model_params[1258]
            lv1299: R.Tensor((1280,), dtype="float32") = model_params[402]
            lv641_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv640_1, lv1298, lv1299, lv635_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1300: R.Tensor((1280,), dtype="float32") = model_params[409]
            lv1301: R.Tensor((1280,), dtype="float32") = model_params[408]
            lv2486 = R.call_tir(cls.layer_norm1, (lv641_1, lv1300, lv1301), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1302: R.Tensor((1280, 1280), dtype="float32") = model_params[1259]
            lv2488 = R.call_tir(cls.matmul13, (lv2486, lv1302), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1303: R.Tensor((2048, 1280), dtype="float32") = model_params[1260]
            lv2490 = R.call_tir(cls.matmul16, (inp_2, lv1303), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1304: R.Tensor((2048, 1280), dtype="float32") = model_params[1261]
            lv2492 = R.call_tir(cls.matmul16, (inp_2, lv1304), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv642_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2488,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv643_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2490,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv644_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2492,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv645_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv642_2, lv643_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2504 = R.call_tir(cls.softmax3, (lv645_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2505 = R.call_tir(cls.matmul18, (lv2504, lv644_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv646_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2505,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1305: R.Tensor((1280, 1280), dtype="float32") = model_params[1262]
            lv1306: R.Tensor((1280,), dtype="float32") = model_params[403]
            lv647_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv646_1, lv1305, lv1306, lv641_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1307_1: R.Tensor((1280,), dtype="float32") = model_params[411]
            lv1308_1: R.Tensor((1280,), dtype="float32") = model_params[410]
            lv2513 = R.call_tir(cls.layer_norm1, (lv647_1, lv1307_1, lv1308_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1309: R.Tensor((1280, 10240), dtype="float32") = model_params[1263]
            lv1310: R.Tensor((10240,), dtype="float32") = model_params[404]
            lv648_1 = R.call_tir(cls.fused_matmul19_add17, (lv2513, lv1309, lv1310), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv649_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv648_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1311: R.Tensor((5120, 1280), dtype="float32") = model_params[1264]
            lv1312: R.Tensor((1280,), dtype="float32") = model_params[405]
            lv650_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv649_1, lv1311, lv1312, lv647_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1313: R.Tensor((1280, 1280), dtype="float32") = model_params[1265]
            lv1314: R.Tensor((1280,), dtype="float32") = model_params[311]
            lv651_1 = R.call_tir(cls.fused_matmul13_add15, (lv650_1, lv1313, lv1314), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv652_1 = R.call_tir(cls.fused_reshape21_transpose25_add14, (lv651_1, lv498), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1315: R.Tensor((1280,), dtype="float32") = model_params[422]
            lv1316_1: R.Tensor((1280,), dtype="float32") = model_params[421]
            lv653_1 = R.call_tir(cls.fused_group_norm5_silu5, (lv652_1, lv1315, lv1316_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv2537 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1317: R.Tensor((1280, 1280), dtype="float32") = model_params[1267]
            lv1318_1: R.Tensor((1280,), dtype="float32") = model_params[425]
            lv654_1 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv2537, lv1317, lv1318_1), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2542 = R.call_tir(cls.reshape16, (lv654_1,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv1319: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[419]
            lv1320_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1266]
            lv655_2 = R.call_tir(cls.fused_conv2d8_add13_add13, (lv653_1, lv1319, lv1320_1, lv2542), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1321: R.Tensor((1280,), dtype="float32") = model_params[424]
            lv1322_1: R.Tensor((1280,), dtype="float32") = model_params[423]
            lv656_1 = R.call_tir(cls.fused_group_norm5_silu5, (lv655_2, lv1321, lv1322_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1323: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[420]
            lv1324: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1268]
            lv657_2 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv656_1, lv1323, lv1324, lv652_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv2551 = R.call_tir(cls.concatenate4, (lv657_2, lv493), out_sinfo=R.Tensor((1, 2560, 16, 16), dtype="float32"))
            lv1325: R.Tensor((2560,), dtype="float32") = model_params[744]
            lv1326: R.Tensor((2560,), dtype="float32") = model_params[743]
            lv658_1 = R.call_tir(cls.fused_group_norm7_silu6, (lv2551, lv1325, lv1326), out_sinfo=R.Tensor((1, 2560, 16, 16), dtype="float32"))
            lv2557 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1327: R.Tensor((1280, 1280), dtype="float32") = model_params[1270]
            lv1328: R.Tensor((1280,), dtype="float32") = model_params[747]
            lv659_2 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv2557, lv1327, lv1328), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2562 = R.call_tir(cls.reshape16, (lv659_2,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv1329: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[740]
            lv1330: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1269]
            lv660_1 = R.call_tir(cls.fused_conv2d10_add13_add13, (lv658_1, lv1329, lv1330, lv2562), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1331: R.Tensor((1280,), dtype="float32") = model_params[746]
            lv1332: R.Tensor((1280,), dtype="float32") = model_params[745]
            lv661_2 = R.call_tir(cls.fused_group_norm5_silu5, (lv660_1, lv1331, lv1332), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1333: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[742]
            lv1334_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1272]
            lv662_1 = R.call_tir(cls.fused_conv2d11_add13, (lv2551, lv1333, lv1334_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1335_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[741]
            lv1336: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1271]
            lv663_1 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv661_2, lv1335_1, lv1336, lv662_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1337: R.Tensor((1280,), dtype="float32") = model_params[429]
            lv1338: R.Tensor((1280,), dtype="float32") = model_params[428]
            lv2574 = R.call_tir(cls.group_norm6, (lv663_1, lv1337, lv1338), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv664_1 = R.call_tir(cls.fused_transpose16_reshape17, (lv2574,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1339: R.Tensor((1280, 1280), dtype="float32") = model_params[1273]
            lv1340: R.Tensor((1280,), dtype="float32") = model_params[430]
            lv665_1 = R.call_tir(cls.fused_matmul13_add15, (lv664_1, lv1339, lv1340), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1341: R.Tensor((1280,), dtype="float32") = model_params[437]
            lv1342: R.Tensor((1280,), dtype="float32") = model_params[436]
            lv2580 = R.call_tir(cls.layer_norm1, (lv665_1, lv1341, lv1342), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1343_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1274]
            lv2582 = R.call_tir(cls.matmul13, (lv2580, lv1343_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1344: R.Tensor((1280, 1280), dtype="float32") = model_params[1275]
            lv2584 = R.call_tir(cls.matmul13, (lv2580, lv1344), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1345: R.Tensor((1280, 1280), dtype="float32") = model_params[1276]
            lv2586 = R.call_tir(cls.matmul13, (lv2580, lv1345), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv666_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2582,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv667_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2584,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv668_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2586,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv669_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv666_1, lv667_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2598 = R.call_tir(cls.softmax2, (lv669_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2599 = R.call_tir(cls.matmul15, (lv2598, lv668_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv670_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2599,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1346: R.Tensor((1280, 1280), dtype="float32") = model_params[1277]
            lv1347: R.Tensor((1280,), dtype="float32") = model_params[432]
            lv671_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv670_1, lv1346, lv1347, lv665_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1348: R.Tensor((1280,), dtype="float32") = model_params[439]
            lv1349: R.Tensor((1280,), dtype="float32") = model_params[438]
            lv2607 = R.call_tir(cls.layer_norm1, (lv671_1, lv1348, lv1349), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1350: R.Tensor((1280, 1280), dtype="float32") = model_params[1278]
            lv2609 = R.call_tir(cls.matmul13, (lv2607, lv1350), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1351: R.Tensor((2048, 1280), dtype="float32") = model_params[1279]
            lv2611 = R.call_tir(cls.matmul16, (inp_2, lv1351), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1352: R.Tensor((2048, 1280), dtype="float32") = model_params[1280]
            lv2613 = R.call_tir(cls.matmul16, (inp_2, lv1352), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv672_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2609,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv673_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2611,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv674_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv2613,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv675_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv672_1, lv673_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2625 = R.call_tir(cls.softmax3, (lv675_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2626 = R.call_tir(cls.matmul18, (lv2625, lv674_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv676_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2626,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1353: R.Tensor((1280, 1280), dtype="float32") = model_params[1281]
            lv1354: R.Tensor((1280,), dtype="float32") = model_params[433]
            lv677_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv676_1, lv1353, lv1354, lv671_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1355: R.Tensor((1280,), dtype="float32") = model_params[441]
            lv1356_1: R.Tensor((1280,), dtype="float32") = model_params[440]
            lv2634 = R.call_tir(cls.layer_norm1, (lv677_1, lv1355, lv1356_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1357: R.Tensor((1280, 10240), dtype="float32") = model_params[1282]
            lv1358_1: R.Tensor((10240,), dtype="float32") = model_params[434]
            lv678_1 = R.call_tir(cls.fused_matmul19_add17, (lv2634, lv1357, lv1358_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv679_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv678_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1359: R.Tensor((5120, 1280), dtype="float32") = model_params[1283]
            lv1360_1: R.Tensor((1280,), dtype="float32") = model_params[435]
            lv680_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv679_1, lv1359, lv1360_1, lv677_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1361: R.Tensor((1280,), dtype="float32") = model_params[447]
            lv1362_1: R.Tensor((1280,), dtype="float32") = model_params[446]
            lv2647 = R.call_tir(cls.layer_norm1, (lv680_1, lv1361, lv1362_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1363: R.Tensor((1280, 1280), dtype="float32") = model_params[1284]
            lv2649 = R.call_tir(cls.matmul13, (lv2647, lv1363), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1364: R.Tensor((1280, 1280), dtype="float32") = model_params[1285]
            lv2651 = R.call_tir(cls.matmul13, (lv2647, lv1364), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1365: R.Tensor((1280, 1280), dtype="float32") = model_params[1286]
            lv2653 = R.call_tir(cls.matmul13, (lv2647, lv1365), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv681_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2649,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv682_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2651,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv683_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2653,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv684_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv681_1, lv682_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2665 = R.call_tir(cls.softmax2, (lv684_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2666 = R.call_tir(cls.matmul15, (lv2665, lv683_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv685_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2666,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1366: R.Tensor((1280, 1280), dtype="float32") = model_params[1287]
            lv1367: R.Tensor((1280,), dtype="float32") = model_params[442]
            lv686_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv685_1, lv1366, lv1367, lv680_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1368: R.Tensor((1280,), dtype="float32") = model_params[449]
            lv1369: R.Tensor((1280,), dtype="float32") = model_params[448]
            lv2674 = R.call_tir(cls.layer_norm1, (lv686_2, lv1368, lv1369), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1370: R.Tensor((1280, 1280), dtype="float32") = model_params[1288]
            lv2676 = R.call_tir(cls.matmul13, (lv2674, lv1370), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1371: R.Tensor((2048, 1280), dtype="float32") = model_params[1289]
            lv2678 = R.call_tir(cls.matmul16, (inp_2, lv1371), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1372: R.Tensor((2048, 1280), dtype="float32") = model_params[1290]
            lv2680 = R.call_tir(cls.matmul16, (inp_2, lv1372), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv687_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2676,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv688_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2678,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv689_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2680,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv690_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv687_1, lv688_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2692 = R.call_tir(cls.softmax3, (lv690_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2693 = R.call_tir(cls.matmul18, (lv2692, lv689_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv691_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2693,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1373: R.Tensor((1280, 1280), dtype="float32") = model_params[1291]
            lv1374_1: R.Tensor((1280,), dtype="float32") = model_params[443]
            lv692_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv691_1, lv1373, lv1374_1, lv686_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1375_1: R.Tensor((1280,), dtype="float32") = model_params[451]
            lv1376: R.Tensor((1280,), dtype="float32") = model_params[450]
            lv2701 = R.call_tir(cls.layer_norm1, (lv692_1, lv1375_1, lv1376), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1377: R.Tensor((1280, 10240), dtype="float32") = model_params[1292]
            lv1378: R.Tensor((10240,), dtype="float32") = model_params[444]
            lv693_1 = R.call_tir(cls.fused_matmul19_add17, (lv2701, lv1377, lv1378), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv694_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv693_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1379: R.Tensor((5120, 1280), dtype="float32") = model_params[1293]
            lv1380: R.Tensor((1280,), dtype="float32") = model_params[445]
            lv695_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv694_1, lv1379, lv1380, lv692_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1381: R.Tensor((1280,), dtype="float32") = model_params[457]
            lv1382: R.Tensor((1280,), dtype="float32") = model_params[456]
            lv2714 = R.call_tir(cls.layer_norm1, (lv695_1, lv1381, lv1382), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1383_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1294]
            lv2716 = R.call_tir(cls.matmul13, (lv2714, lv1383_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1384: R.Tensor((1280, 1280), dtype="float32") = model_params[1295]
            lv2718 = R.call_tir(cls.matmul13, (lv2714, lv1384), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1385_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1296]
            lv2720 = R.call_tir(cls.matmul13, (lv2714, lv1385_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv696_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2716,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv697_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2718,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv698_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2720,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv699_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv696_1, lv697_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2732 = R.call_tir(cls.softmax2, (lv699_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2733 = R.call_tir(cls.matmul15, (lv2732, lv698_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv700_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv2733,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1386: R.Tensor((1280, 1280), dtype="float32") = model_params[1297]
            lv1387_1: R.Tensor((1280,), dtype="float32") = model_params[452]
            lv701_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv700_2, lv1386, lv1387_1, lv695_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1388: R.Tensor((1280,), dtype="float32") = model_params[459]
            lv1389_1: R.Tensor((1280,), dtype="float32") = model_params[458]
            lv2741 = R.call_tir(cls.layer_norm1, (lv701_2, lv1388, lv1389_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1390: R.Tensor((1280, 1280), dtype="float32") = model_params[1298]
            lv2743 = R.call_tir(cls.matmul13, (lv2741, lv1390), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1391: R.Tensor((2048, 1280), dtype="float32") = model_params[1299]
            lv2745 = R.call_tir(cls.matmul16, (inp_2, lv1391), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1392: R.Tensor((2048, 1280), dtype="float32") = model_params[1300]
            lv2747 = R.call_tir(cls.matmul16, (inp_2, lv1392), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv702_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2743,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv703_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2745,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv704_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2747,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv705_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv702_1, lv703_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2759 = R.call_tir(cls.softmax3, (lv705_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2760 = R.call_tir(cls.matmul18, (lv2759, lv704_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv706_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2760,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1393: R.Tensor((1280, 1280), dtype="float32") = model_params[1301]
            lv1394: R.Tensor((1280,), dtype="float32") = model_params[453]
            lv707_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv706_1, lv1393, lv1394, lv701_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1395: R.Tensor((1280,), dtype="float32") = model_params[461]
            lv1396: R.Tensor((1280,), dtype="float32") = model_params[460]
            lv2768 = R.call_tir(cls.layer_norm1, (lv707_1, lv1395, lv1396), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1397: R.Tensor((1280, 10240), dtype="float32") = model_params[1302]
            lv1398: R.Tensor((10240,), dtype="float32") = model_params[454]
            lv708_1 = R.call_tir(cls.fused_matmul19_add17, (lv2768, lv1397, lv1398), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv709_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv708_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1399: R.Tensor((5120, 1280), dtype="float32") = model_params[1303]
            lv1400: R.Tensor((1280,), dtype="float32") = model_params[455]
            lv710_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv709_2, lv1399, lv1400, lv707_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1401_1: R.Tensor((1280,), dtype="float32") = model_params[467]
            lv1402_1: R.Tensor((1280,), dtype="float32") = model_params[466]
            lv2781 = R.call_tir(cls.layer_norm1, (lv710_1, lv1401_1, lv1402_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1403: R.Tensor((1280, 1280), dtype="float32") = model_params[1304]
            lv2783 = R.call_tir(cls.matmul13, (lv2781, lv1403), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1404: R.Tensor((1280, 1280), dtype="float32") = model_params[1305]
            lv2785 = R.call_tir(cls.matmul13, (lv2781, lv1404), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1405: R.Tensor((1280, 1280), dtype="float32") = model_params[1306]
            lv2787 = R.call_tir(cls.matmul13, (lv2781, lv1405), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv711_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2783,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv712_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2785,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv713_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2787,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv714_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv711_1, lv712_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2799 = R.call_tir(cls.softmax2, (lv714_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2800 = R.call_tir(cls.matmul15, (lv2799, lv713_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv715_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2800,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1406: R.Tensor((1280, 1280), dtype="float32") = model_params[1307]
            lv1407: R.Tensor((1280,), dtype="float32") = model_params[462]
            lv716_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv715_1, lv1406, lv1407, lv710_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1408: R.Tensor((1280,), dtype="float32") = model_params[469]
            lv1409: R.Tensor((1280,), dtype="float32") = model_params[468]
            lv2808 = R.call_tir(cls.layer_norm1, (lv716_1, lv1408, lv1409), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1410_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1308]
            lv2810 = R.call_tir(cls.matmul13, (lv2808, lv1410_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1411: R.Tensor((2048, 1280), dtype="float32") = model_params[1309]
            lv2812 = R.call_tir(cls.matmul16, (inp_2, lv1411), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1412: R.Tensor((2048, 1280), dtype="float32") = model_params[1310]
            lv2814 = R.call_tir(cls.matmul16, (inp_2, lv1412), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv717_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2810,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv718_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2812,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv719_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2814,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv720_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv717_1, lv718_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2826 = R.call_tir(cls.softmax3, (lv720_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2827 = R.call_tir(cls.matmul18, (lv2826, lv719_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv721_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2827,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1413: R.Tensor((1280, 1280), dtype="float32") = model_params[1311]
            lv1414: R.Tensor((1280,), dtype="float32") = model_params[463]
            lv722_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv721_1, lv1413, lv1414, lv716_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1415: R.Tensor((1280,), dtype="float32") = model_params[471]
            lv1416: R.Tensor((1280,), dtype="float32") = model_params[470]
            lv2835 = R.call_tir(cls.layer_norm1, (lv722_2, lv1415, lv1416), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1417: R.Tensor((1280, 10240), dtype="float32") = model_params[1312]
            lv1418: R.Tensor((10240,), dtype="float32") = model_params[464]
            lv723_1 = R.call_tir(cls.fused_matmul19_add17, (lv2835, lv1417, lv1418), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv724_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv723_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1419: R.Tensor((5120, 1280), dtype="float32") = model_params[1313]
            lv1420: R.Tensor((1280,), dtype="float32") = model_params[465]
            lv725_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv724_2, lv1419, lv1420, lv722_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1421: R.Tensor((1280,), dtype="float32") = model_params[477]
            lv1422: R.Tensor((1280,), dtype="float32") = model_params[476]
            lv2848 = R.call_tir(cls.layer_norm1, (lv725_1, lv1421, lv1422), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1423_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1314]
            lv2850 = R.call_tir(cls.matmul13, (lv2848, lv1423_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1424: R.Tensor((1280, 1280), dtype="float32") = model_params[1315]
            lv2852 = R.call_tir(cls.matmul13, (lv2848, lv1424), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1425_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1316]
            lv2854 = R.call_tir(cls.matmul13, (lv2848, lv1425_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv726_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2850,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv727_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2852,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv728_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2854,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv729_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv726_2, lv727_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2866 = R.call_tir(cls.softmax2, (lv729_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2867 = R.call_tir(cls.matmul15, (lv2866, lv728_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv730_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2867,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1426: R.Tensor((1280, 1280), dtype="float32") = model_params[1317]
            lv1427_1: R.Tensor((1280,), dtype="float32") = model_params[472]
            lv731_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv730_1, lv1426, lv1427_1, lv725_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1428: R.Tensor((1280,), dtype="float32") = model_params[479]
            lv1429_1: R.Tensor((1280,), dtype="float32") = model_params[478]
            lv2875 = R.call_tir(cls.layer_norm1, (lv731_1, lv1428, lv1429_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1430: R.Tensor((1280, 1280), dtype="float32") = model_params[1318]
            lv2877 = R.call_tir(cls.matmul13, (lv2875, lv1430), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1431: R.Tensor((2048, 1280), dtype="float32") = model_params[1319]
            lv2879 = R.call_tir(cls.matmul16, (inp_2, lv1431), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1432: R.Tensor((2048, 1280), dtype="float32") = model_params[1320]
            lv2881 = R.call_tir(cls.matmul16, (inp_2, lv1432), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv732_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2877,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv733_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2879,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv734_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv2881,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv735_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv732_1, lv733_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2893 = R.call_tir(cls.softmax3, (lv735_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2894 = R.call_tir(cls.matmul18, (lv2893, lv734_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv736_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2894,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1433: R.Tensor((1280, 1280), dtype="float32") = model_params[1321]
            lv1434: R.Tensor((1280,), dtype="float32") = model_params[473]
            lv737_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv736_1, lv1433, lv1434, lv731_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1435: R.Tensor((1280,), dtype="float32") = model_params[481]
            lv1436: R.Tensor((1280,), dtype="float32") = model_params[480]
            lv2902 = R.call_tir(cls.layer_norm1, (lv737_1, lv1435, lv1436), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1437: R.Tensor((1280, 10240), dtype="float32") = model_params[1322]
            lv1438: R.Tensor((10240,), dtype="float32") = model_params[474]
            lv738_1 = R.call_tir(cls.fused_matmul19_add17, (lv2902, lv1437, lv1438), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv739_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv738_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1439: R.Tensor((5120, 1280), dtype="float32") = model_params[1323]
            lv1440: R.Tensor((1280,), dtype="float32") = model_params[475]
            lv740_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv739_1, lv1439, lv1440, lv737_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1441_1: R.Tensor((1280,), dtype="float32") = model_params[487]
            lv1442_1: R.Tensor((1280,), dtype="float32") = model_params[486]
            lv2915 = R.call_tir(cls.layer_norm1, (lv740_2, lv1441_1, lv1442_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1443: R.Tensor((1280, 1280), dtype="float32") = model_params[1324]
            lv2917 = R.call_tir(cls.matmul13, (lv2915, lv1443), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1444: R.Tensor((1280, 1280), dtype="float32") = model_params[1325]
            lv2919 = R.call_tir(cls.matmul13, (lv2915, lv1444), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1445: R.Tensor((1280, 1280), dtype="float32") = model_params[1326]
            lv2921 = R.call_tir(cls.matmul13, (lv2915, lv1445), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv741_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv2917,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv742_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2919,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv743_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2921,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv744_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv741_2, lv742_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2933 = R.call_tir(cls.softmax2, (lv744_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv2934 = R.call_tir(cls.matmul15, (lv2933, lv743_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv745_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv2934,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1446: R.Tensor((1280, 1280), dtype="float32") = model_params[1327]
            lv1447: R.Tensor((1280,), dtype="float32") = model_params[482]
            lv746_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv745_1, lv1446, lv1447, lv740_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1448: R.Tensor((1280,), dtype="float32") = model_params[489]
            lv1449: R.Tensor((1280,), dtype="float32") = model_params[488]
            lv2942 = R.call_tir(cls.layer_norm1, (lv746_1, lv1448, lv1449), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1450_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1328]
            lv2944 = R.call_tir(cls.matmul13, (lv2942, lv1450_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1451: R.Tensor((2048, 1280), dtype="float32") = model_params[1329]
            lv2946 = R.call_tir(cls.matmul16, (inp_2, lv1451), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1452_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1330]
            lv2948 = R.call_tir(cls.matmul16, (inp_2, lv1452_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv747_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2944,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv748_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv2946,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv749_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv2948,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv750_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv747_1, lv748_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2960 = R.call_tir(cls.softmax3, (lv750_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv2961 = R.call_tir(cls.matmul18, (lv2960, lv749_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv751_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv2961,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1453: R.Tensor((1280, 1280), dtype="float32") = model_params[1331]
            lv1454_1: R.Tensor((1280,), dtype="float32") = model_params[483]
            lv752_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv751_2, lv1453, lv1454_1, lv746_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1455: R.Tensor((1280,), dtype="float32") = model_params[491]
            lv1456_1: R.Tensor((1280,), dtype="float32") = model_params[490]
            lv2969 = R.call_tir(cls.layer_norm1, (lv752_1, lv1455, lv1456_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1457: R.Tensor((1280, 10240), dtype="float32") = model_params[1332]
            lv1458: R.Tensor((10240,), dtype="float32") = model_params[484]
            lv753_2 = R.call_tir(cls.fused_matmul19_add17, (lv2969, lv1457, lv1458), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv754_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv753_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1459: R.Tensor((5120, 1280), dtype="float32") = model_params[1333]
            lv1460: R.Tensor((1280,), dtype="float32") = model_params[485]
            lv755_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv754_1, lv1459, lv1460, lv752_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1461: R.Tensor((1280,), dtype="float32") = model_params[497]
            lv1462: R.Tensor((1280,), dtype="float32") = model_params[496]
            lv2982 = R.call_tir(cls.layer_norm1, (lv755_2, lv1461, lv1462), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1463: R.Tensor((1280, 1280), dtype="float32") = model_params[1334]
            lv2984 = R.call_tir(cls.matmul13, (lv2982, lv1463), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1464: R.Tensor((1280, 1280), dtype="float32") = model_params[1335]
            lv2986 = R.call_tir(cls.matmul13, (lv2982, lv1464), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1465: R.Tensor((1280, 1280), dtype="float32") = model_params[1336]
            lv2988 = R.call_tir(cls.matmul13, (lv2982, lv1465), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv756_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2984,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv757_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv2986,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv758_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv2988,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv759_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv756_1, lv757_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3000 = R.call_tir(cls.softmax2, (lv759_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3001 = R.call_tir(cls.matmul15, (lv3000, lv758_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv760_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3001,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1466: R.Tensor((1280, 1280), dtype="float32") = model_params[1337]
            lv1467: R.Tensor((1280,), dtype="float32") = model_params[492]
            lv761_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv760_1, lv1466, lv1467, lv755_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1468_1: R.Tensor((1280,), dtype="float32") = model_params[499]
            lv1469_1: R.Tensor((1280,), dtype="float32") = model_params[498]
            lv3009 = R.call_tir(cls.layer_norm1, (lv761_1, lv1468_1, lv1469_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1470: R.Tensor((1280, 1280), dtype="float32") = model_params[1338]
            lv3011 = R.call_tir(cls.matmul13, (lv3009, lv1470), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1471: R.Tensor((2048, 1280), dtype="float32") = model_params[1339]
            lv3013 = R.call_tir(cls.matmul16, (inp_2, lv1471), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1472: R.Tensor((2048, 1280), dtype="float32") = model_params[1340]
            lv3015 = R.call_tir(cls.matmul16, (inp_2, lv1472), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv762_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3011,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv763_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3013,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv764_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3015,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv765_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv762_1, lv763_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3027 = R.call_tir(cls.softmax3, (lv765_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3028 = R.call_tir(cls.matmul18, (lv3027, lv764_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv766_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3028,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1473: R.Tensor((1280, 1280), dtype="float32") = model_params[1341]
            lv1474: R.Tensor((1280,), dtype="float32") = model_params[493]
            lv767_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv766_1, lv1473, lv1474, lv761_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1475: R.Tensor((1280,), dtype="float32") = model_params[501]
            lv1476: R.Tensor((1280,), dtype="float32") = model_params[500]
            lv3036 = R.call_tir(cls.layer_norm1, (lv767_2, lv1475, lv1476), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1477_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1342]
            lv1478: R.Tensor((10240,), dtype="float32") = model_params[494]
            lv768_2 = R.call_tir(cls.fused_matmul19_add17, (lv3036, lv1477_1, lv1478), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv769_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv768_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1479: R.Tensor((5120, 1280), dtype="float32") = model_params[1343]
            lv1480: R.Tensor((1280,), dtype="float32") = model_params[495]
            lv770_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv769_1, lv1479, lv1480, lv767_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1481: R.Tensor((1280,), dtype="float32") = model_params[507]
            lv1482: R.Tensor((1280,), dtype="float32") = model_params[506]
            lv3049 = R.call_tir(cls.layer_norm1, (lv770_1, lv1481, lv1482), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1483: R.Tensor((1280, 1280), dtype="float32") = model_params[1344]
            lv3051 = R.call_tir(cls.matmul13, (lv3049, lv1483), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1484: R.Tensor((1280, 1280), dtype="float32") = model_params[1345]
            lv3053 = R.call_tir(cls.matmul13, (lv3049, lv1484), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1485: R.Tensor((1280, 1280), dtype="float32") = model_params[1346]
            lv3055 = R.call_tir(cls.matmul13, (lv3049, lv1485), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv771_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3051,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv772_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3053,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv773_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3055,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv774_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv771_1, lv772_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3067 = R.call_tir(cls.softmax2, (lv774_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3068 = R.call_tir(cls.matmul15, (lv3067, lv773_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv775_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3068,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1486: R.Tensor((1280, 1280), dtype="float32") = model_params[1347]
            lv1487: R.Tensor((1280,), dtype="float32") = model_params[502]
            lv776_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv775_1, lv1486, lv1487, lv770_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1488: R.Tensor((1280,), dtype="float32") = model_params[509]
            lv1489: R.Tensor((1280,), dtype="float32") = model_params[508]
            lv3076 = R.call_tir(cls.layer_norm1, (lv776_2, lv1488, lv1489), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1490_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1348]
            lv3078 = R.call_tir(cls.matmul13, (lv3076, lv1490_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1491: R.Tensor((2048, 1280), dtype="float32") = model_params[1349]
            lv3080 = R.call_tir(cls.matmul16, (inp_2, lv1491), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1492_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1350]
            lv3082 = R.call_tir(cls.matmul16, (inp_2, lv1492_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv777_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3078,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv778_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3080,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv779_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3082,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv780_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv777_1, lv778_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3094 = R.call_tir(cls.softmax3, (lv780_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3095 = R.call_tir(cls.matmul18, (lv3094, lv779_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv781_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3095,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1493: R.Tensor((1280, 1280), dtype="float32") = model_params[1351]
            lv1494_1: R.Tensor((1280,), dtype="float32") = model_params[503]
            lv782_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv781_1, lv1493, lv1494_1, lv776_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1495: R.Tensor((1280,), dtype="float32") = model_params[511]
            lv1496_1: R.Tensor((1280,), dtype="float32") = model_params[510]
            lv3103 = R.call_tir(cls.layer_norm1, (lv782_1, lv1495, lv1496_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1497: R.Tensor((1280, 10240), dtype="float32") = model_params[1352]
            lv1498: R.Tensor((10240,), dtype="float32") = model_params[504]
            lv783_1 = R.call_tir(cls.fused_matmul19_add17, (lv3103, lv1497, lv1498), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv784_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv783_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1499: R.Tensor((5120, 1280), dtype="float32") = model_params[1353]
            lv1500: R.Tensor((1280,), dtype="float32") = model_params[505]
            lv785_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv784_1, lv1499, lv1500, lv782_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1501: R.Tensor((1280,), dtype="float32") = model_params[517]
            lv1502: R.Tensor((1280,), dtype="float32") = model_params[516]
            lv3116 = R.call_tir(cls.layer_norm1, (lv785_1, lv1501, lv1502), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1503: R.Tensor((1280, 1280), dtype="float32") = model_params[1354]
            lv3118 = R.call_tir(cls.matmul13, (lv3116, lv1503), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1504: R.Tensor((1280, 1280), dtype="float32") = model_params[1355]
            lv3120 = R.call_tir(cls.matmul13, (lv3116, lv1504), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1505: R.Tensor((1280, 1280), dtype="float32") = model_params[1356]
            lv3122 = R.call_tir(cls.matmul13, (lv3116, lv1505), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv786_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3118,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv787_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3120,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv788_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3122,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv789_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv786_1, lv787_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3134 = R.call_tir(cls.softmax2, (lv789_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3135 = R.call_tir(cls.matmul15, (lv3134, lv788_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv790_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3135,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1506: R.Tensor((1280, 1280), dtype="float32") = model_params[1357]
            lv1507: R.Tensor((1280,), dtype="float32") = model_params[512]
            lv791_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv790_1, lv1506, lv1507, lv785_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1508_1: R.Tensor((1280,), dtype="float32") = model_params[519]
            lv1509_1: R.Tensor((1280,), dtype="float32") = model_params[518]
            lv3143 = R.call_tir(cls.layer_norm1, (lv791_2, lv1508_1, lv1509_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1510: R.Tensor((1280, 1280), dtype="float32") = model_params[1358]
            lv3145 = R.call_tir(cls.matmul13, (lv3143, lv1510), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1511: R.Tensor((2048, 1280), dtype="float32") = model_params[1359]
            lv3147 = R.call_tir(cls.matmul16, (inp_2, lv1511), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1512: R.Tensor((2048, 1280), dtype="float32") = model_params[1360]
            lv3149 = R.call_tir(cls.matmul16, (inp_2, lv1512), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv792_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3145,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv793_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3147,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv794_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3149,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv795_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv792_1, lv793_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3161 = R.call_tir(cls.softmax3, (lv795_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3162 = R.call_tir(cls.matmul18, (lv3161, lv794_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv796_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3162,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1513: R.Tensor((1280, 1280), dtype="float32") = model_params[1361]
            lv1514: R.Tensor((1280,), dtype="float32") = model_params[513]
            lv797_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv796_1, lv1513, lv1514, lv791_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1515: R.Tensor((1280,), dtype="float32") = model_params[521]
            lv1516: R.Tensor((1280,), dtype="float32") = model_params[520]
            lv3170 = R.call_tir(cls.layer_norm1, (lv797_1, lv1515, lv1516), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1517_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1362]
            lv1518: R.Tensor((10240,), dtype="float32") = model_params[514]
            lv798_1 = R.call_tir(cls.fused_matmul19_add17, (lv3170, lv1517_1, lv1518), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv799_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv798_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1519_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1363]
            lv1520: R.Tensor((1280,), dtype="float32") = model_params[515]
            lv800_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv799_1, lv1519_1, lv1520, lv797_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1521_1: R.Tensor((1280,), dtype="float32") = model_params[527]
            lv1522: R.Tensor((1280,), dtype="float32") = model_params[526]
            lv3183 = R.call_tir(cls.layer_norm1, (lv800_1, lv1521_1, lv1522), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1523_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1364]
            lv3185 = R.call_tir(cls.matmul13, (lv3183, lv1523_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1524: R.Tensor((1280, 1280), dtype="float32") = model_params[1365]
            lv3187 = R.call_tir(cls.matmul13, (lv3183, lv1524), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1525: R.Tensor((1280, 1280), dtype="float32") = model_params[1366]
            lv3189 = R.call_tir(cls.matmul13, (lv3183, lv1525), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv801_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3185,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv802_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3187,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv803_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3189,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv804_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv801_1, lv802_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3201 = R.call_tir(cls.softmax2, (lv804_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3202 = R.call_tir(cls.matmul15, (lv3201, lv803_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv805_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3202,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1526: R.Tensor((1280, 1280), dtype="float32") = model_params[1367]
            lv1527: R.Tensor((1280,), dtype="float32") = model_params[522]
            lv806_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv805_1, lv1526, lv1527, lv800_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1528: R.Tensor((1280,), dtype="float32") = model_params[529]
            lv1529: R.Tensor((1280,), dtype="float32") = model_params[528]
            lv3210 = R.call_tir(cls.layer_norm1, (lv806_1, lv1528, lv1529), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1530: R.Tensor((1280, 1280), dtype="float32") = model_params[1368]
            lv3212 = R.call_tir(cls.matmul13, (lv3210, lv1530), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1531: R.Tensor((2048, 1280), dtype="float32") = model_params[1369]
            lv3214 = R.call_tir(cls.matmul16, (inp_2, lv1531), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1532: R.Tensor((2048, 1280), dtype="float32") = model_params[1370]
            lv3216 = R.call_tir(cls.matmul16, (inp_2, lv1532), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv807_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3212,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv808_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3214,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv809_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3216,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv810_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv807_2, lv808_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3228 = R.call_tir(cls.softmax3, (lv810_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3229 = R.call_tir(cls.matmul18, (lv3228, lv809_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv811_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3229,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1533: R.Tensor((1280, 1280), dtype="float32") = model_params[1371]
            lv1534: R.Tensor((1280,), dtype="float32") = model_params[523]
            lv812_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv811_1, lv1533, lv1534, lv806_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1535_1: R.Tensor((1280,), dtype="float32") = model_params[531]
            lv1536_1: R.Tensor((1280,), dtype="float32") = model_params[530]
            lv3237 = R.call_tir(cls.layer_norm1, (lv812_1, lv1535_1, lv1536_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1537: R.Tensor((1280, 10240), dtype="float32") = model_params[1372]
            lv1538: R.Tensor((10240,), dtype="float32") = model_params[524]
            lv813_1 = R.call_tir(cls.fused_matmul19_add17, (lv3237, lv1537, lv1538), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv814_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv813_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1539: R.Tensor((5120, 1280), dtype="float32") = model_params[1373]
            lv1540: R.Tensor((1280,), dtype="float32") = model_params[525]
            lv815_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv814_1, lv1539, lv1540, lv812_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1541: R.Tensor((1280, 1280), dtype="float32") = model_params[1374]
            lv1542: R.Tensor((1280,), dtype="float32") = model_params[431]
            lv816_2 = R.call_tir(cls.fused_matmul13_add15, (lv815_1, lv1541, lv1542), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv817_1 = R.call_tir(cls.fused_reshape21_transpose25_add14_concatenate4, (lv816_2, lv663_1, lv334), out_sinfo=R.Tensor((1, 2560, 16, 16), dtype="float32"))
            lv1543: R.Tensor((2560,), dtype="float32") = model_params[752]
            lv1544_1: R.Tensor((2560,), dtype="float32") = model_params[751]
            lv818_2 = R.call_tir(cls.fused_group_norm7_silu6, (lv817_1, lv1543, lv1544_1), out_sinfo=R.Tensor((1, 2560, 16, 16), dtype="float32"))
            lv3262 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1545: R.Tensor((1280, 1280), dtype="float32") = model_params[1376]
            lv1546: R.Tensor((1280,), dtype="float32") = model_params[755]
            lv819_1 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv3262, lv1545, lv1546), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv3267 = R.call_tir(cls.reshape16, (lv819_1,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv1547: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[748]
            lv1548: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1375]
            lv820_2 = R.call_tir(cls.fused_conv2d10_add13_add13, (lv818_2, lv1547, lv1548, lv3267), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1549: R.Tensor((1280,), dtype="float32") = model_params[754]
            lv1550: R.Tensor((1280,), dtype="float32") = model_params[753]
            lv821_1 = R.call_tir(cls.fused_group_norm5_silu5, (lv820_2, lv1549, lv1550), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1551: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[750]
            lv1552: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1378]
            lv822_2 = R.call_tir(cls.fused_conv2d11_add13, (lv817_1, lv1551, lv1552), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1553: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[749]
            lv1554: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1377]
            lv823_1 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv821_1, lv1553, lv1554, lv822_2), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1555: R.Tensor((1280,), dtype="float32") = model_params[533]
            lv1556: R.Tensor((1280,), dtype="float32") = model_params[532]
            lv3279 = R.call_tir(cls.group_norm6, (lv823_1, lv1555, lv1556), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv824_1 = R.call_tir(cls.fused_transpose16_reshape17, (lv3279,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1557_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1379]
            lv1558: R.Tensor((1280,), dtype="float32") = model_params[534]
            lv825_1 = R.call_tir(cls.fused_matmul13_add15, (lv824_1, lv1557_1, lv1558), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1559_1: R.Tensor((1280,), dtype="float32") = model_params[541]
            lv1560: R.Tensor((1280,), dtype="float32") = model_params[540]
            lv3285 = R.call_tir(cls.layer_norm1, (lv825_1, lv1559_1, lv1560), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1561_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1380]
            lv3287 = R.call_tir(cls.matmul13, (lv3285, lv1561_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1562: R.Tensor((1280, 1280), dtype="float32") = model_params[1381]
            lv3289 = R.call_tir(cls.matmul13, (lv3285, lv1562), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1563_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1382]
            lv3291 = R.call_tir(cls.matmul13, (lv3285, lv1563_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv826_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3287,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv827_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3289,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv828_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3291,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv829_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv826_1, lv827_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3303 = R.call_tir(cls.softmax2, (lv829_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3304 = R.call_tir(cls.matmul15, (lv3303, lv828_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv830_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3304,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1564: R.Tensor((1280, 1280), dtype="float32") = model_params[1383]
            lv1565: R.Tensor((1280,), dtype="float32") = model_params[536]
            lv831_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv830_1, lv1564, lv1565, lv825_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1566: R.Tensor((1280,), dtype="float32") = model_params[543]
            lv1567: R.Tensor((1280,), dtype="float32") = model_params[542]
            lv3312 = R.call_tir(cls.layer_norm1, (lv831_1, lv1566, lv1567), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1568: R.Tensor((1280, 1280), dtype="float32") = model_params[1384]
            lv3314 = R.call_tir(cls.matmul13, (lv3312, lv1568), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1569: R.Tensor((2048, 1280), dtype="float32") = model_params[1385]
            lv3316 = R.call_tir(cls.matmul16, (inp_2, lv1569), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1570: R.Tensor((2048, 1280), dtype="float32") = model_params[1386]
            lv3318 = R.call_tir(cls.matmul16, (inp_2, lv1570), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv832_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3314,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv833_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3316,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv834_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv3318,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv835_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv832_1, lv833_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3330 = R.call_tir(cls.softmax3, (lv835_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3331 = R.call_tir(cls.matmul18, (lv3330, lv834_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv836_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3331,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1571: R.Tensor((1280, 1280), dtype="float32") = model_params[1387]
            lv1572: R.Tensor((1280,), dtype="float32") = model_params[537]
            lv837_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv836_1, lv1571, lv1572, lv831_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1573: R.Tensor((1280,), dtype="float32") = model_params[545]
            lv1574: R.Tensor((1280,), dtype="float32") = model_params[544]
            lv3339 = R.call_tir(cls.layer_norm1, (lv837_1, lv1573, lv1574), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1575_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1388]
            lv1576_1: R.Tensor((10240,), dtype="float32") = model_params[538]
            lv838_1 = R.call_tir(cls.fused_matmul19_add17, (lv3339, lv1575_1, lv1576_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv839_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv838_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1577: R.Tensor((5120, 1280), dtype="float32") = model_params[1389]
            lv1578: R.Tensor((1280,), dtype="float32") = model_params[539]
            lv840_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv839_1, lv1577, lv1578, lv837_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1579: R.Tensor((1280,), dtype="float32") = model_params[551]
            lv1580: R.Tensor((1280,), dtype="float32") = model_params[550]
            lv3352 = R.call_tir(cls.layer_norm1, (lv840_1, lv1579, lv1580), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1581: R.Tensor((1280, 1280), dtype="float32") = model_params[1390]
            lv3354 = R.call_tir(cls.matmul13, (lv3352, lv1581), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1582: R.Tensor((1280, 1280), dtype="float32") = model_params[1391]
            lv3356 = R.call_tir(cls.matmul13, (lv3352, lv1582), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1583: R.Tensor((1280, 1280), dtype="float32") = model_params[1392]
            lv3358 = R.call_tir(cls.matmul13, (lv3352, lv1583), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv841_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3354,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv842_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3356,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv843_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3358,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv844_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv841_1, lv842_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3370 = R.call_tir(cls.softmax2, (lv844_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3371 = R.call_tir(cls.matmul15, (lv3370, lv843_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv845_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3371,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1584_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1393]
            lv1585: R.Tensor((1280,), dtype="float32") = model_params[546]
            lv846_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv845_1, lv1584_1, lv1585, lv840_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1586_1: R.Tensor((1280,), dtype="float32") = model_params[553]
            lv1587: R.Tensor((1280,), dtype="float32") = model_params[552]
            lv3379 = R.call_tir(cls.layer_norm1, (lv846_1, lv1586_1, lv1587), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1588_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1394]
            lv3381 = R.call_tir(cls.matmul13, (lv3379, lv1588_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1589: R.Tensor((2048, 1280), dtype="float32") = model_params[1395]
            lv3383 = R.call_tir(cls.matmul16, (inp_2, lv1589), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1590_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1396]
            lv3385 = R.call_tir(cls.matmul16, (inp_2, lv1590_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv847_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3381,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv848_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3383,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv849_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3385,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv850_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv847_1, lv848_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3397 = R.call_tir(cls.softmax3, (lv850_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3398 = R.call_tir(cls.matmul18, (lv3397, lv849_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv851_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3398,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1591: R.Tensor((1280, 1280), dtype="float32") = model_params[1397]
            lv1592: R.Tensor((1280,), dtype="float32") = model_params[547]
            lv852_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv851_1, lv1591, lv1592, lv846_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1593: R.Tensor((1280,), dtype="float32") = model_params[555]
            lv1594: R.Tensor((1280,), dtype="float32") = model_params[554]
            lv3406 = R.call_tir(cls.layer_norm1, (lv852_1, lv1593, lv1594), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1595: R.Tensor((1280, 10240), dtype="float32") = model_params[1398]
            lv1596: R.Tensor((10240,), dtype="float32") = model_params[548]
            lv853_1 = R.call_tir(cls.fused_matmul19_add17, (lv3406, lv1595, lv1596), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv854_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv853_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1597: R.Tensor((5120, 1280), dtype="float32") = model_params[1399]
            lv1598: R.Tensor((1280,), dtype="float32") = model_params[549]
            lv855_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv854_1, lv1597, lv1598, lv852_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1599: R.Tensor((1280,), dtype="float32") = model_params[561]
            lv1600: R.Tensor((1280,), dtype="float32") = model_params[560]
            lv3419 = R.call_tir(cls.layer_norm1, (lv855_1, lv1599, lv1600), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1601: R.Tensor((1280, 1280), dtype="float32") = model_params[1400]
            lv3421 = R.call_tir(cls.matmul13, (lv3419, lv1601), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1602_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1401]
            lv3423 = R.call_tir(cls.matmul13, (lv3419, lv1602_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1603_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1402]
            lv3425 = R.call_tir(cls.matmul13, (lv3419, lv1603_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv856_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3421,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv857_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3423,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv858_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3425,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv859_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv856_2, lv857_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3437 = R.call_tir(cls.softmax2, (lv859_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3438 = R.call_tir(cls.matmul15, (lv3437, lv858_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv860_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv3438,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1604: R.Tensor((1280, 1280), dtype="float32") = model_params[1403]
            lv1605: R.Tensor((1280,), dtype="float32") = model_params[556]
            lv861_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv860_2, lv1604, lv1605, lv855_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1606: R.Tensor((1280,), dtype="float32") = model_params[563]
            lv1607: R.Tensor((1280,), dtype="float32") = model_params[562]
            lv3446 = R.call_tir(cls.layer_norm1, (lv861_1, lv1606, lv1607), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1608: R.Tensor((1280, 1280), dtype="float32") = model_params[1404]
            lv3448 = R.call_tir(cls.matmul13, (lv3446, lv1608), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1609: R.Tensor((2048, 1280), dtype="float32") = model_params[1405]
            lv3450 = R.call_tir(cls.matmul16, (inp_2, lv1609), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1610: R.Tensor((2048, 1280), dtype="float32") = model_params[1406]
            lv3452 = R.call_tir(cls.matmul16, (inp_2, lv1610), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv862_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3448,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv863_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3450,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv864_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3452,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv865_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv862_2, lv863_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3464 = R.call_tir(cls.softmax3, (lv865_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3465 = R.call_tir(cls.matmul18, (lv3464, lv864_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv866_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3465,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1611_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1407]
            lv1612: R.Tensor((1280,), dtype="float32") = model_params[557]
            lv867_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv866_1, lv1611_1, lv1612, lv861_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1613: R.Tensor((1280,), dtype="float32") = model_params[565]
            lv1614: R.Tensor((1280,), dtype="float32") = model_params[564]
            lv3473 = R.call_tir(cls.layer_norm1, (lv867_1, lv1613, lv1614), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1615: R.Tensor((1280, 10240), dtype="float32") = model_params[1408]
            lv1616: R.Tensor((10240,), dtype="float32") = model_params[558]
            lv868_1 = R.call_tir(cls.fused_matmul19_add17, (lv3473, lv1615, lv1616), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv869_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv868_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1617: R.Tensor((5120, 1280), dtype="float32") = model_params[1409]
            lv1618: R.Tensor((1280,), dtype="float32") = model_params[559]
            lv870_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv869_1, lv1617, lv1618, lv867_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1619: R.Tensor((1280,), dtype="float32") = model_params[571]
            lv1620: R.Tensor((1280,), dtype="float32") = model_params[570]
            lv3486 = R.call_tir(cls.layer_norm1, (lv870_1, lv1619, lv1620), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1621: R.Tensor((1280, 1280), dtype="float32") = model_params[1410]
            lv3488 = R.call_tir(cls.matmul13, (lv3486, lv1621), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1622: R.Tensor((1280, 1280), dtype="float32") = model_params[1411]
            lv3490 = R.call_tir(cls.matmul13, (lv3486, lv1622), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1623: R.Tensor((1280, 1280), dtype="float32") = model_params[1412]
            lv3492 = R.call_tir(cls.matmul13, (lv3486, lv1623), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv871_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3488,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv872_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3490,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv873_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3492,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv874_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv871_1, lv872_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3504 = R.call_tir(cls.softmax2, (lv874_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3505 = R.call_tir(cls.matmul15, (lv3504, lv873_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv875_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv3505,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1624_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1413]
            lv1625: R.Tensor((1280,), dtype="float32") = model_params[566]
            lv876_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv875_2, lv1624_1, lv1625, lv870_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1626_1: R.Tensor((1280,), dtype="float32") = model_params[573]
            lv1627: R.Tensor((1280,), dtype="float32") = model_params[572]
            lv3513 = R.call_tir(cls.layer_norm1, (lv876_1, lv1626_1, lv1627), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1628_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1414]
            lv3515 = R.call_tir(cls.matmul13, (lv3513, lv1628_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1629: R.Tensor((2048, 1280), dtype="float32") = model_params[1415]
            lv3517 = R.call_tir(cls.matmul16, (inp_2, lv1629), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1630_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1416]
            lv3519 = R.call_tir(cls.matmul16, (inp_2, lv1630_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv877_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3515,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv878_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3517,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv879_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3519,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv880_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv877_1, lv878_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3531 = R.call_tir(cls.softmax3, (lv880_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3532 = R.call_tir(cls.matmul18, (lv3531, lv879_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv881_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3532,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1631: R.Tensor((1280, 1280), dtype="float32") = model_params[1417]
            lv1632: R.Tensor((1280,), dtype="float32") = model_params[567]
            lv882_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv881_1, lv1631, lv1632, lv876_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1633: R.Tensor((1280,), dtype="float32") = model_params[575]
            lv1634: R.Tensor((1280,), dtype="float32") = model_params[574]
            lv3540 = R.call_tir(cls.layer_norm1, (lv882_1, lv1633, lv1634), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1635: R.Tensor((1280, 10240), dtype="float32") = model_params[1418]
            lv1636: R.Tensor((10240,), dtype="float32") = model_params[568]
            lv883_2 = R.call_tir(cls.fused_matmul19_add17, (lv3540, lv1635, lv1636), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv884_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv883_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1637: R.Tensor((5120, 1280), dtype="float32") = model_params[1419]
            lv1638: R.Tensor((1280,), dtype="float32") = model_params[569]
            lv885_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv884_1, lv1637, lv1638, lv882_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1639: R.Tensor((1280,), dtype="float32") = model_params[581]
            lv1640: R.Tensor((1280,), dtype="float32") = model_params[580]
            lv3553 = R.call_tir(cls.layer_norm1, (lv885_2, lv1639, lv1640), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1641: R.Tensor((1280, 1280), dtype="float32") = model_params[1420]
            lv3555 = R.call_tir(cls.matmul13, (lv3553, lv1641), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1642_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1421]
            lv3557 = R.call_tir(cls.matmul13, (lv3553, lv1642_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1643_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1422]
            lv3559 = R.call_tir(cls.matmul13, (lv3553, lv1643_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv886_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3555,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv887_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3557,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv888_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3559,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv889_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv886_1, lv887_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3571 = R.call_tir(cls.softmax2, (lv889_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3572 = R.call_tir(cls.matmul15, (lv3571, lv888_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv890_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3572,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1644: R.Tensor((1280, 1280), dtype="float32") = model_params[1423]
            lv1645: R.Tensor((1280,), dtype="float32") = model_params[576]
            lv891_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv890_1, lv1644, lv1645, lv885_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1646: R.Tensor((1280,), dtype="float32") = model_params[583]
            lv1647: R.Tensor((1280,), dtype="float32") = model_params[582]
            lv3580 = R.call_tir(cls.layer_norm1, (lv891_1, lv1646, lv1647), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1648: R.Tensor((1280, 1280), dtype="float32") = model_params[1424]
            lv3582 = R.call_tir(cls.matmul13, (lv3580, lv1648), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1649: R.Tensor((2048, 1280), dtype="float32") = model_params[1425]
            lv3584 = R.call_tir(cls.matmul16, (inp_2, lv1649), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1650: R.Tensor((2048, 1280), dtype="float32") = model_params[1426]
            lv3586 = R.call_tir(cls.matmul16, (inp_2, lv1650), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv892_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3582,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv893_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3584,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv894_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3586,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv895_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv892_1, lv893_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3598 = R.call_tir(cls.softmax3, (lv895_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3599 = R.call_tir(cls.matmul18, (lv3598, lv894_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv896_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3599,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1651_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1427]
            lv1652: R.Tensor((1280,), dtype="float32") = model_params[577]
            lv897_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv896_1, lv1651_1, lv1652, lv891_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1653_1: R.Tensor((1280,), dtype="float32") = model_params[585]
            lv1654: R.Tensor((1280,), dtype="float32") = model_params[584]
            lv3607 = R.call_tir(cls.layer_norm1, (lv897_1, lv1653_1, lv1654), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1655_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1428]
            lv1656: R.Tensor((10240,), dtype="float32") = model_params[578]
            lv898_1 = R.call_tir(cls.fused_matmul19_add17, (lv3607, lv1655_1, lv1656), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv899_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv898_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1657_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1429]
            lv1658: R.Tensor((1280,), dtype="float32") = model_params[579]
            lv900_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv899_1, lv1657_1, lv1658, lv897_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1659: R.Tensor((1280,), dtype="float32") = model_params[591]
            lv1660: R.Tensor((1280,), dtype="float32") = model_params[590]
            lv3620 = R.call_tir(cls.layer_norm1, (lv900_1, lv1659, lv1660), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1661: R.Tensor((1280, 1280), dtype="float32") = model_params[1430]
            lv3622 = R.call_tir(cls.matmul13, (lv3620, lv1661), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1662: R.Tensor((1280, 1280), dtype="float32") = model_params[1431]
            lv3624 = R.call_tir(cls.matmul13, (lv3620, lv1662), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1663: R.Tensor((1280, 1280), dtype="float32") = model_params[1432]
            lv3626 = R.call_tir(cls.matmul13, (lv3620, lv1663), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv901_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3622,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv902_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3624,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv903_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3626,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv904_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv901_2, lv902_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3638 = R.call_tir(cls.softmax2, (lv904_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3639 = R.call_tir(cls.matmul15, (lv3638, lv903_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv905_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3639,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1664: R.Tensor((1280, 1280), dtype="float32") = model_params[1433]
            lv1665: R.Tensor((1280,), dtype="float32") = model_params[586]
            lv906_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv905_1, lv1664, lv1665, lv900_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1666: R.Tensor((1280,), dtype="float32") = model_params[593]
            lv1667: R.Tensor((1280,), dtype="float32") = model_params[592]
            lv3647 = R.call_tir(cls.layer_norm1, (lv906_1, lv1666, lv1667), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1668: R.Tensor((1280, 1280), dtype="float32") = model_params[1434]
            lv3649 = R.call_tir(cls.matmul13, (lv3647, lv1668), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1669_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1435]
            lv3651 = R.call_tir(cls.matmul16, (inp_2, lv1669_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1670_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1436]
            lv3653 = R.call_tir(cls.matmul16, (inp_2, lv1670_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv907_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3649,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv908_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3651,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv909_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3653,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv910_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv907_1, lv908_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3665 = R.call_tir(cls.softmax3, (lv910_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3666 = R.call_tir(cls.matmul18, (lv3665, lv909_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv911_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3666,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1671: R.Tensor((1280, 1280), dtype="float32") = model_params[1437]
            lv1672: R.Tensor((1280,), dtype="float32") = model_params[587]
            lv912_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv911_1, lv1671, lv1672, lv906_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1673: R.Tensor((1280,), dtype="float32") = model_params[595]
            lv1674: R.Tensor((1280,), dtype="float32") = model_params[594]
            lv3674 = R.call_tir(cls.layer_norm1, (lv912_1, lv1673, lv1674), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1675: R.Tensor((1280, 10240), dtype="float32") = model_params[1438]
            lv1676: R.Tensor((10240,), dtype="float32") = model_params[588]
            lv913_1 = R.call_tir(cls.fused_matmul19_add17, (lv3674, lv1675, lv1676), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv914_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv913_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1677: R.Tensor((5120, 1280), dtype="float32") = model_params[1439]
            lv1678_1: R.Tensor((1280,), dtype="float32") = model_params[589]
            lv915_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv914_1, lv1677, lv1678_1, lv912_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1679: R.Tensor((1280,), dtype="float32") = model_params[601]
            lv1680: R.Tensor((1280,), dtype="float32") = model_params[600]
            lv3687 = R.call_tir(cls.layer_norm1, (lv915_1, lv1679, lv1680), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1681: R.Tensor((1280, 1280), dtype="float32") = model_params[1440]
            lv3689 = R.call_tir(cls.matmul13, (lv3687, lv1681), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1682: R.Tensor((1280, 1280), dtype="float32") = model_params[1441]
            lv3691 = R.call_tir(cls.matmul13, (lv3687, lv1682), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1683: R.Tensor((1280, 1280), dtype="float32") = model_params[1442]
            lv3693 = R.call_tir(cls.matmul13, (lv3687, lv1683), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv916_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3689,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv917_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3691,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv918_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3693,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv919_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv916_1, lv917_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3705 = R.call_tir(cls.softmax2, (lv919_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3706 = R.call_tir(cls.matmul15, (lv3705, lv918_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv920_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3706,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1684: R.Tensor((1280, 1280), dtype="float32") = model_params[1443]
            lv1685: R.Tensor((1280,), dtype="float32") = model_params[596]
            lv921_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv920_1, lv1684, lv1685, lv915_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1686: R.Tensor((1280,), dtype="float32") = model_params[603]
            lv1687: R.Tensor((1280,), dtype="float32") = model_params[602]
            lv3714 = R.call_tir(cls.layer_norm1, (lv921_1, lv1686, lv1687), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1688: R.Tensor((1280, 1280), dtype="float32") = model_params[1444]
            lv3716 = R.call_tir(cls.matmul13, (lv3714, lv1688), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1689: R.Tensor((2048, 1280), dtype="float32") = model_params[1445]
            lv3718 = R.call_tir(cls.matmul16, (inp_2, lv1689), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1690: R.Tensor((2048, 1280), dtype="float32") = model_params[1446]
            lv3720 = R.call_tir(cls.matmul16, (inp_2, lv1690), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv922_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3716,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv923_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3718,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv924_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3720,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv925_2 = R.call_tir(cls.fused_matmul17_multiply8, (lv922_1, lv923_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3732 = R.call_tir(cls.softmax3, (lv925_2,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3733 = R.call_tir(cls.matmul18, (lv3732, lv924_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv926_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3733,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1691_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1447]
            lv1692: R.Tensor((1280,), dtype="float32") = model_params[597]
            lv927_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv926_1, lv1691_1, lv1692, lv921_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1693_1: R.Tensor((1280,), dtype="float32") = model_params[605]
            lv1694: R.Tensor((1280,), dtype="float32") = model_params[604]
            lv3741 = R.call_tir(cls.layer_norm1, (lv927_2, lv1693_1, lv1694), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1695_1: R.Tensor((1280, 10240), dtype="float32") = model_params[1448]
            lv1696: R.Tensor((10240,), dtype="float32") = model_params[598]
            lv928_1 = R.call_tir(cls.fused_matmul19_add17, (lv3741, lv1695_1, lv1696), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv929_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv928_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1697_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1449]
            lv1698: R.Tensor((1280,), dtype="float32") = model_params[599]
            lv930_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv929_2, lv1697_1, lv1698, lv927_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1699: R.Tensor((1280,), dtype="float32") = model_params[611]
            lv1700: R.Tensor((1280,), dtype="float32") = model_params[610]
            lv3754 = R.call_tir(cls.layer_norm1, (lv930_1, lv1699, lv1700), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1701: R.Tensor((1280, 1280), dtype="float32") = model_params[1450]
            lv3756 = R.call_tir(cls.matmul13, (lv3754, lv1701), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1702: R.Tensor((1280, 1280), dtype="float32") = model_params[1451]
            lv3758 = R.call_tir(cls.matmul13, (lv3754, lv1702), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1703: R.Tensor((1280, 1280), dtype="float32") = model_params[1452]
            lv3760 = R.call_tir(cls.matmul13, (lv3754, lv1703), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv931_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3756,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv932_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3758,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv933_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3760,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv934_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv931_1, lv932_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3772 = R.call_tir(cls.softmax2, (lv934_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3773 = R.call_tir(cls.matmul15, (lv3772, lv933_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv935_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3773,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1704: R.Tensor((1280, 1280), dtype="float32") = model_params[1453]
            lv1705: R.Tensor((1280,), dtype="float32") = model_params[606]
            lv936_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv935_1, lv1704, lv1705, lv930_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1706: R.Tensor((1280,), dtype="float32") = model_params[613]
            lv1707: R.Tensor((1280,), dtype="float32") = model_params[612]
            lv3781 = R.call_tir(cls.layer_norm1, (lv936_1, lv1706, lv1707), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1708: R.Tensor((1280, 1280), dtype="float32") = model_params[1454]
            lv3783 = R.call_tir(cls.matmul13, (lv3781, lv1708), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1709_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1455]
            lv3785 = R.call_tir(cls.matmul16, (inp_2, lv1709_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1710_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1456]
            lv3787 = R.call_tir(cls.matmul16, (inp_2, lv1710_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv937_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3783,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv938_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3785,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv939_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv3787,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv940_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv937_1, lv938_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3799 = R.call_tir(cls.softmax3, (lv940_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3800 = R.call_tir(cls.matmul18, (lv3799, lv939_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv941_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv3800,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1711: R.Tensor((1280, 1280), dtype="float32") = model_params[1457]
            lv1712: R.Tensor((1280,), dtype="float32") = model_params[607]
            lv942_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv941_2, lv1711, lv1712, lv936_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1713: R.Tensor((1280,), dtype="float32") = model_params[615]
            lv1714: R.Tensor((1280,), dtype="float32") = model_params[614]
            lv3808 = R.call_tir(cls.layer_norm1, (lv942_2, lv1713, lv1714), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1715: R.Tensor((1280, 10240), dtype="float32") = model_params[1458]
            lv1716: R.Tensor((10240,), dtype="float32") = model_params[608]
            lv943_1 = R.call_tir(cls.fused_matmul19_add17, (lv3808, lv1715, lv1716), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv944_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv943_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1717: R.Tensor((5120, 1280), dtype="float32") = model_params[1459]
            lv1718_1: R.Tensor((1280,), dtype="float32") = model_params[609]
            lv945_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv944_1, lv1717, lv1718_1, lv942_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1719: R.Tensor((1280,), dtype="float32") = model_params[621]
            lv1720_1: R.Tensor((1280,), dtype="float32") = model_params[620]
            lv3821 = R.call_tir(cls.layer_norm1, (lv945_1, lv1719, lv1720_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1721: R.Tensor((1280, 1280), dtype="float32") = model_params[1460]
            lv3823 = R.call_tir(cls.matmul13, (lv3821, lv1721), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1722_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1461]
            lv3825 = R.call_tir(cls.matmul13, (lv3821, lv1722_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1723: R.Tensor((1280, 1280), dtype="float32") = model_params[1462]
            lv3827 = R.call_tir(cls.matmul13, (lv3821, lv1723), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv946_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3823,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv947_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3825,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv948_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3827,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv949_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv946_1, lv947_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3839 = R.call_tir(cls.softmax2, (lv949_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3840 = R.call_tir(cls.matmul15, (lv3839, lv948_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv950_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv3840,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1724_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1463]
            lv1725: R.Tensor((1280,), dtype="float32") = model_params[616]
            lv951_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv950_2, lv1724_1, lv1725, lv945_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1726: R.Tensor((1280,), dtype="float32") = model_params[623]
            lv1727: R.Tensor((1280,), dtype="float32") = model_params[622]
            lv3848 = R.call_tir(cls.layer_norm1, (lv951_1, lv1726, lv1727), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1728: R.Tensor((1280, 1280), dtype="float32") = model_params[1464]
            lv3850 = R.call_tir(cls.matmul13, (lv3848, lv1728), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1729: R.Tensor((2048, 1280), dtype="float32") = model_params[1465]
            lv3852 = R.call_tir(cls.matmul16, (inp_2, lv1729), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1730: R.Tensor((2048, 1280), dtype="float32") = model_params[1466]
            lv3854 = R.call_tir(cls.matmul16, (inp_2, lv1730), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv952_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv3850,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv953_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3852,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv954_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv3854,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv955_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv952_2, lv953_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3866 = R.call_tir(cls.softmax3, (lv955_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3867 = R.call_tir(cls.matmul18, (lv3866, lv954_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv956_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv3867,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1731: R.Tensor((1280, 1280), dtype="float32") = model_params[1467]
            lv1732: R.Tensor((1280,), dtype="float32") = model_params[617]
            lv957_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv956_2, lv1731, lv1732, lv951_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1733: R.Tensor((1280,), dtype="float32") = model_params[625]
            lv1734: R.Tensor((1280,), dtype="float32") = model_params[624]
            lv3875 = R.call_tir(cls.layer_norm1, (lv957_1, lv1733, lv1734), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1735: R.Tensor((1280, 10240), dtype="float32") = model_params[1468]
            lv1736_1: R.Tensor((10240,), dtype="float32") = model_params[618]
            lv958_1 = R.call_tir(cls.fused_matmul19_add17, (lv3875, lv1735, lv1736_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv959_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv958_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1737_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1469]
            lv1738: R.Tensor((1280,), dtype="float32") = model_params[619]
            lv960_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv959_1, lv1737_1, lv1738, lv957_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1739: R.Tensor((1280,), dtype="float32") = model_params[631]
            lv1740: R.Tensor((1280,), dtype="float32") = model_params[630]
            lv3888 = R.call_tir(cls.layer_norm1, (lv960_1, lv1739, lv1740), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1741: R.Tensor((1280, 1280), dtype="float32") = model_params[1470]
            lv3890 = R.call_tir(cls.matmul13, (lv3888, lv1741), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1742: R.Tensor((1280, 1280), dtype="float32") = model_params[1471]
            lv3892 = R.call_tir(cls.matmul13, (lv3888, lv1742), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1743: R.Tensor((1280, 1280), dtype="float32") = model_params[1472]
            lv3894 = R.call_tir(cls.matmul13, (lv3888, lv1743), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv961_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3890,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv962_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3892,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv963_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3894,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv964_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv961_1, lv962_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3906 = R.call_tir(cls.softmax2, (lv964_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv3907 = R.call_tir(cls.matmul15, (lv3906, lv963_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv965_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3907,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1744: R.Tensor((1280, 1280), dtype="float32") = model_params[1473]
            lv1745_1: R.Tensor((1280,), dtype="float32") = model_params[626]
            lv966_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv965_1, lv1744, lv1745_1, lv960_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1746: R.Tensor((1280,), dtype="float32") = model_params[633]
            lv1747: R.Tensor((1280,), dtype="float32") = model_params[632]
            lv3915 = R.call_tir(cls.layer_norm1, (lv966_1, lv1746, lv1747), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1748: R.Tensor((1280, 1280), dtype="float32") = model_params[1474]
            lv3917 = R.call_tir(cls.matmul13, (lv3915, lv1748), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1749: R.Tensor((2048, 1280), dtype="float32") = model_params[1475]
            lv3919 = R.call_tir(cls.matmul16, (inp_2, lv1749), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1750: R.Tensor((2048, 1280), dtype="float32") = model_params[1476]
            lv3921 = R.call_tir(cls.matmul16, (inp_2, lv1750), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv967_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3917,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv968_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv3919,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv969_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv3921,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv970_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv967_1, lv968_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3933 = R.call_tir(cls.softmax3, (lv970_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv3934 = R.call_tir(cls.matmul18, (lv3933, lv969_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv971_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv3934,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1751: R.Tensor((1280, 1280), dtype="float32") = model_params[1477]
            lv1752: R.Tensor((1280,), dtype="float32") = model_params[627]
            lv972_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv971_1, lv1751, lv1752, lv966_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1753: R.Tensor((1280,), dtype="float32") = model_params[635]
            lv1754: R.Tensor((1280,), dtype="float32") = model_params[634]
            lv3942 = R.call_tir(cls.layer_norm1, (lv972_1, lv1753, lv1754), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1755: R.Tensor((1280, 10240), dtype="float32") = model_params[1478]
            lv1756: R.Tensor((10240,), dtype="float32") = model_params[628]
            lv973_1 = R.call_tir(cls.fused_matmul19_add17, (lv3942, lv1755, lv1756), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv974_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv973_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1757: R.Tensor((5120, 1280), dtype="float32") = model_params[1479]
            lv1758_1: R.Tensor((1280,), dtype="float32") = model_params[629]
            lv975_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv974_1, lv1757, lv1758_1, lv972_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1759: R.Tensor((1280, 1280), dtype="float32") = model_params[1480]
            lv1760_1: R.Tensor((1280,), dtype="float32") = model_params[535]
            lv976_1 = R.call_tir(cls.fused_matmul13_add15, (lv975_1, lv1759, lv1760_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv977_2 = R.call_tir(cls.fused_reshape21_transpose25_add14_concatenate5, (lv976_1, lv823_1, lv174), out_sinfo=R.Tensor((1, 1920, 16, 16), dtype="float32"))
            lv1761: R.Tensor((1920,), dtype="float32") = model_params[760]
            lv1762_1: R.Tensor((1920,), dtype="float32") = model_params[759]
            lv978_1 = R.call_tir(cls.fused_group_norm8_silu7, (lv977_2, lv1761, lv1762_1), out_sinfo=R.Tensor((1, 1920, 16, 16), dtype="float32"))
            lv3967 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1763: R.Tensor((1280, 1280), dtype="float32") = model_params[1482]
            lv1764_1: R.Tensor((1280,), dtype="float32") = model_params[763]
            lv979_1 = R.call_tir(cls.fused_matmul1_add_strided_slice7, (lv3967, lv1763, lv1764_1), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv3972 = R.call_tir(cls.reshape16, (lv979_1,), out_sinfo=R.Tensor((1, 1280, 1, 1), dtype="float32"))
            lv1765: R.Tensor((1280, 1920, 3, 3), dtype="float32") = model_params[756]
            lv1766: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1481]
            lv980_1 = R.call_tir(cls.fused_conv2d12_add13_add13, (lv978_1, lv1765, lv1766, lv3972), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1767: R.Tensor((1280,), dtype="float32") = model_params[762]
            lv1768: R.Tensor((1280,), dtype="float32") = model_params[761]
            lv981_1 = R.call_tir(cls.fused_group_norm5_silu5, (lv980_1, lv1767, lv1768), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1769: R.Tensor((1280, 1920, 1, 1), dtype="float32") = model_params[758]
            lv1770: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1484]
            lv982_1 = R.call_tir(cls.fused_conv2d13_add13, (lv977_2, lv1769, lv1770), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1771: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[757]
            lv1772: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1483]
            lv983_1 = R.call_tir(cls.fused_conv2d8_add13_add14_divide4, (lv981_1, lv1771, lv1772, lv982_1), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv1773: R.Tensor((1280,), dtype="float32") = model_params[637]
            lv1774: R.Tensor((1280,), dtype="float32") = model_params[636]
            lv3984 = R.call_tir(cls.group_norm6, (lv983_1, lv1773, lv1774), out_sinfo=R.Tensor((1, 1280, 16, 16), dtype="float32"))
            lv984_1 = R.call_tir(cls.fused_transpose16_reshape17, (lv3984,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1775: R.Tensor((1280, 1280), dtype="float32") = model_params[1485]
            lv1776_1: R.Tensor((1280,), dtype="float32") = model_params[638]
            lv985_1 = R.call_tir(cls.fused_matmul13_add15, (lv984_1, lv1775, lv1776_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1777_1: R.Tensor((1280,), dtype="float32") = model_params[645]
            lv1778: R.Tensor((1280,), dtype="float32") = model_params[644]
            lv3990 = R.call_tir(cls.layer_norm1, (lv985_1, lv1777_1, lv1778), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1779: R.Tensor((1280, 1280), dtype="float32") = model_params[1486]
            lv3992 = R.call_tir(cls.matmul13, (lv3990, lv1779), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1780: R.Tensor((1280, 1280), dtype="float32") = model_params[1487]
            lv3994 = R.call_tir(cls.matmul13, (lv3990, lv1780), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1781: R.Tensor((1280, 1280), dtype="float32") = model_params[1488]
            lv3996 = R.call_tir(cls.matmul13, (lv3990, lv1781), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv986_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3992,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv987_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv3994,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv988_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv3996,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv989_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv986_1, lv987_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4008 = R.call_tir(cls.softmax2, (lv989_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4009 = R.call_tir(cls.matmul15, (lv4008, lv988_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv990_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv4009,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1782: R.Tensor((1280, 1280), dtype="float32") = model_params[1489]
            lv1783: R.Tensor((1280,), dtype="float32") = model_params[640]
            lv991_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv990_2, lv1782, lv1783, lv985_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1784: R.Tensor((1280,), dtype="float32") = model_params[647]
            lv1785_1: R.Tensor((1280,), dtype="float32") = model_params[646]
            lv4017 = R.call_tir(cls.layer_norm1, (lv991_1, lv1784, lv1785_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1786: R.Tensor((1280, 1280), dtype="float32") = model_params[1490]
            lv4019 = R.call_tir(cls.matmul13, (lv4017, lv1786), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1787_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1491]
            lv4021 = R.call_tir(cls.matmul16, (inp_2, lv1787_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1788: R.Tensor((2048, 1280), dtype="float32") = model_params[1492]
            lv4023 = R.call_tir(cls.matmul16, (inp_2, lv1788), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv992_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv4019,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv993_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4021,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv994_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv4023,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv995_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv992_2, lv993_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4035 = R.call_tir(cls.softmax3, (lv995_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4036 = R.call_tir(cls.matmul18, (lv4035, lv994_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv996_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv4036,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1789_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1493]
            lv1790: R.Tensor((1280,), dtype="float32") = model_params[641]
            lv997_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv996_2, lv1789_1, lv1790, lv991_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1791_1: R.Tensor((1280,), dtype="float32") = model_params[649]
            lv1792: R.Tensor((1280,), dtype="float32") = model_params[648]
            lv4044 = R.call_tir(cls.layer_norm1, (lv997_1, lv1791_1, lv1792), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1793: R.Tensor((1280, 10240), dtype="float32") = model_params[1494]
            lv1794: R.Tensor((10240,), dtype="float32") = model_params[642]
            lv998_1 = R.call_tir(cls.fused_matmul19_add17, (lv4044, lv1793, lv1794), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv999_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv998_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1795: R.Tensor((5120, 1280), dtype="float32") = model_params[1495]
            lv1796: R.Tensor((1280,), dtype="float32") = model_params[643]
            lv1000_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv999_1, lv1795, lv1796, lv997_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1797: R.Tensor((1280,), dtype="float32") = model_params[655]
            lv1798: R.Tensor((1280,), dtype="float32") = model_params[654]
            lv4057 = R.call_tir(cls.layer_norm1, (lv1000_1, lv1797, lv1798), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1799: R.Tensor((1280, 1280), dtype="float32") = model_params[1496]
            lv4059 = R.call_tir(cls.matmul13, (lv4057, lv1799), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1800: R.Tensor((1280, 1280), dtype="float32") = model_params[1497]
            lv4061 = R.call_tir(cls.matmul13, (lv4057, lv1800), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1801: R.Tensor((1280, 1280), dtype="float32") = model_params[1498]
            lv4063 = R.call_tir(cls.matmul13, (lv4057, lv1801), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1001_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4059,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1002_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4061,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1003_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4063,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1004_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1001_1, lv1002_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4075 = R.call_tir(cls.softmax2, (lv1004_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4076 = R.call_tir(cls.matmul15, (lv4075, lv1003_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1005_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4076,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1802: R.Tensor((1280, 1280), dtype="float32") = model_params[1499]
            lv1803_1: R.Tensor((1280,), dtype="float32") = model_params[650]
            lv1006_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1005_1, lv1802, lv1803_1, lv1000_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1804_1: R.Tensor((1280,), dtype="float32") = model_params[657]
            lv1805: R.Tensor((1280,), dtype="float32") = model_params[656]
            lv4084 = R.call_tir(cls.layer_norm1, (lv1006_1, lv1804_1, lv1805), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1806: R.Tensor((1280, 1280), dtype="float32") = model_params[1500]
            lv4086 = R.call_tir(cls.matmul13, (lv4084, lv1806), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1807: R.Tensor((2048, 1280), dtype="float32") = model_params[1501]
            lv4088 = R.call_tir(cls.matmul16, (inp_2, lv1807), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1808: R.Tensor((2048, 1280), dtype="float32") = model_params[1502]
            lv4090 = R.call_tir(cls.matmul16, (inp_2, lv1808), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1007_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4086,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1008_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4088,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1009_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv4090,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1010_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1007_1, lv1008_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4102 = R.call_tir(cls.softmax3, (lv1010_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4103 = R.call_tir(cls.matmul18, (lv4102, lv1009_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1011_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4103,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1809: R.Tensor((1280, 1280), dtype="float32") = model_params[1503]
            lv1810: R.Tensor((1280,), dtype="float32") = model_params[651]
            lv1012_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1011_1, lv1809, lv1810, lv1006_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1811: R.Tensor((1280,), dtype="float32") = model_params[659]
            lv1812_1: R.Tensor((1280,), dtype="float32") = model_params[658]
            lv4111 = R.call_tir(cls.layer_norm1, (lv1012_1, lv1811, lv1812_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1813: R.Tensor((1280, 10240), dtype="float32") = model_params[1504]
            lv1814: R.Tensor((10240,), dtype="float32") = model_params[652]
            lv1013_1 = R.call_tir(cls.fused_matmul19_add17, (lv4111, lv1813, lv1814), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1014_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1013_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1815: R.Tensor((5120, 1280), dtype="float32") = model_params[1505]
            lv1816: R.Tensor((1280,), dtype="float32") = model_params[653]
            lv1015_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1014_1, lv1815, lv1816, lv1012_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1817: R.Tensor((1280,), dtype="float32") = model_params[665]
            lv1818: R.Tensor((1280,), dtype="float32") = model_params[664]
            lv4124 = R.call_tir(cls.layer_norm1, (lv1015_1, lv1817, lv1818), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1819: R.Tensor((1280, 1280), dtype="float32") = model_params[1506]
            lv4126 = R.call_tir(cls.matmul13, (lv4124, lv1819), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1820: R.Tensor((1280, 1280), dtype="float32") = model_params[1507]
            lv4128 = R.call_tir(cls.matmul13, (lv4124, lv1820), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1821: R.Tensor((1280, 1280), dtype="float32") = model_params[1508]
            lv4130 = R.call_tir(cls.matmul13, (lv4124, lv1821), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1016_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4126,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1017_2 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4128,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1018_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4130,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1019_2 = R.call_tir(cls.fused_matmul14_multiply7, (lv1016_1, lv1017_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4142 = R.call_tir(cls.softmax2, (lv1019_2,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4143 = R.call_tir(cls.matmul15, (lv4142, lv1018_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1020_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4143,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1822: R.Tensor((1280, 1280), dtype="float32") = model_params[1509]
            lv1823: R.Tensor((1280,), dtype="float32") = model_params[660]
            lv1021_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1020_1, lv1822, lv1823, lv1015_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1824: R.Tensor((1280,), dtype="float32") = model_params[667]
            lv1825: R.Tensor((1280,), dtype="float32") = model_params[666]
            lv4151 = R.call_tir(cls.layer_norm1, (lv1021_2, lv1824, lv1825), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1826: R.Tensor((1280, 1280), dtype="float32") = model_params[1510]
            lv4153 = R.call_tir(cls.matmul13, (lv4151, lv1826), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1827: R.Tensor((2048, 1280), dtype="float32") = model_params[1511]
            lv4155 = R.call_tir(cls.matmul16, (inp_2, lv1827), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1828: R.Tensor((2048, 1280), dtype="float32") = model_params[1512]
            lv4157 = R.call_tir(cls.matmul16, (inp_2, lv1828), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1022_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4153,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1023_2 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4155,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1024_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4157,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1025_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1022_1, lv1023_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4169 = R.call_tir(cls.softmax3, (lv1025_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4170 = R.call_tir(cls.matmul18, (lv4169, lv1024_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1026_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4170,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1829: R.Tensor((1280, 1280), dtype="float32") = model_params[1513]
            lv1830: R.Tensor((1280,), dtype="float32") = model_params[661]
            lv1027_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1026_1, lv1829, lv1830, lv1021_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1831: R.Tensor((1280,), dtype="float32") = model_params[669]
            lv1832: R.Tensor((1280,), dtype="float32") = model_params[668]
            lv4178 = R.call_tir(cls.layer_norm1, (lv1027_1, lv1831, lv1832), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1833: R.Tensor((1280, 10240), dtype="float32") = model_params[1514]
            lv1834: R.Tensor((10240,), dtype="float32") = model_params[662]
            lv1028_1 = R.call_tir(cls.fused_matmul19_add17, (lv4178, lv1833, lv1834), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1029_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1028_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1835: R.Tensor((5120, 1280), dtype="float32") = model_params[1515]
            lv1836_1: R.Tensor((1280,), dtype="float32") = model_params[663]
            lv1030_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1029_1, lv1835, lv1836_1, lv1027_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1837: R.Tensor((1280,), dtype="float32") = model_params[675]
            lv1838: R.Tensor((1280,), dtype="float32") = model_params[674]
            lv4191 = R.call_tir(cls.layer_norm1, (lv1030_1, lv1837, lv1838), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1839: R.Tensor((1280, 1280), dtype="float32") = model_params[1516]
            lv4193 = R.call_tir(cls.matmul13, (lv4191, lv1839), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1840: R.Tensor((1280, 1280), dtype="float32") = model_params[1517]
            lv4195 = R.call_tir(cls.matmul13, (lv4191, lv1840), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1841_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1518]
            lv4197 = R.call_tir(cls.matmul13, (lv4191, lv1841_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1031_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4193,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1032_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4195,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1033_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4197,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1034_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1031_1, lv1032_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4209 = R.call_tir(cls.softmax2, (lv1034_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4210 = R.call_tir(cls.matmul15, (lv4209, lv1033_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1035_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv4210,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1842: R.Tensor((1280, 1280), dtype="float32") = model_params[1519]
            lv1843: R.Tensor((1280,), dtype="float32") = model_params[670]
            lv1036_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1035_2, lv1842, lv1843, lv1030_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1844: R.Tensor((1280,), dtype="float32") = model_params[677]
            lv1845: R.Tensor((1280,), dtype="float32") = model_params[676]
            lv4218 = R.call_tir(cls.layer_norm1, (lv1036_2, lv1844, lv1845), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1846: R.Tensor((1280, 1280), dtype="float32") = model_params[1520]
            lv4220 = R.call_tir(cls.matmul13, (lv4218, lv1846), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1847: R.Tensor((2048, 1280), dtype="float32") = model_params[1521]
            lv4222 = R.call_tir(cls.matmul16, (inp_2, lv1847), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1848: R.Tensor((2048, 1280), dtype="float32") = model_params[1522]
            lv4224 = R.call_tir(cls.matmul16, (inp_2, lv1848), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1037_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4220,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1038_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4222,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1039_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4224,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1040_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1037_1, lv1038_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4236 = R.call_tir(cls.softmax3, (lv1040_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4237 = R.call_tir(cls.matmul18, (lv4236, lv1039_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1041_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4237,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1849: R.Tensor((1280, 1280), dtype="float32") = model_params[1523]
            lv1850_1: R.Tensor((1280,), dtype="float32") = model_params[671]
            lv1042_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1041_1, lv1849, lv1850_1, lv1036_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1851: R.Tensor((1280,), dtype="float32") = model_params[679]
            lv1852: R.Tensor((1280,), dtype="float32") = model_params[678]
            lv4245 = R.call_tir(cls.layer_norm1, (lv1042_1, lv1851, lv1852), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1853: R.Tensor((1280, 10240), dtype="float32") = model_params[1524]
            lv1854: R.Tensor((10240,), dtype="float32") = model_params[672]
            lv1043_1 = R.call_tir(cls.fused_matmul19_add17, (lv4245, lv1853, lv1854), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1044_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1043_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1855: R.Tensor((5120, 1280), dtype="float32") = model_params[1525]
            lv1856_1: R.Tensor((1280,), dtype="float32") = model_params[673]
            lv1045_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1044_2, lv1855, lv1856_1, lv1042_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1857: R.Tensor((1280,), dtype="float32") = model_params[685]
            lv1858_1: R.Tensor((1280,), dtype="float32") = model_params[684]
            lv4258 = R.call_tir(cls.layer_norm1, (lv1045_1, lv1857, lv1858_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1859: R.Tensor((1280, 1280), dtype="float32") = model_params[1526]
            lv4260 = R.call_tir(cls.matmul13, (lv4258, lv1859), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1860_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1527]
            lv4262 = R.call_tir(cls.matmul13, (lv4258, lv1860_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1861: R.Tensor((1280, 1280), dtype="float32") = model_params[1528]
            lv4264 = R.call_tir(cls.matmul13, (lv4258, lv1861), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1046_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4260,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1047_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4262,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1048_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4264,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1049_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1046_1, lv1047_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4276 = R.call_tir(cls.softmax2, (lv1049_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4277 = R.call_tir(cls.matmul15, (lv4276, lv1048_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1050_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4277,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1862_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1529]
            lv1863: R.Tensor((1280,), dtype="float32") = model_params[680]
            lv1051_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1050_1, lv1862_1, lv1863, lv1045_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1864: R.Tensor((1280,), dtype="float32") = model_params[687]
            lv1865: R.Tensor((1280,), dtype="float32") = model_params[686]
            lv4285 = R.call_tir(cls.layer_norm1, (lv1051_1, lv1864, lv1865), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1866: R.Tensor((1280, 1280), dtype="float32") = model_params[1530]
            lv4287 = R.call_tir(cls.matmul13, (lv4285, lv1866), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1867: R.Tensor((2048, 1280), dtype="float32") = model_params[1531]
            lv4289 = R.call_tir(cls.matmul16, (inp_2, lv1867), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1868: R.Tensor((2048, 1280), dtype="float32") = model_params[1532]
            lv4291 = R.call_tir(cls.matmul16, (inp_2, lv1868), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1052_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4287,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1053_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4289,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1054_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4291,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1055_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1052_1, lv1053_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4303 = R.call_tir(cls.softmax3, (lv1055_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4304 = R.call_tir(cls.matmul18, (lv4303, lv1054_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1056_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4304,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1869: R.Tensor((1280, 1280), dtype="float32") = model_params[1533]
            lv1870: R.Tensor((1280,), dtype="float32") = model_params[681]
            lv1057_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1056_1, lv1869, lv1870, lv1051_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1871: R.Tensor((1280,), dtype="float32") = model_params[689]
            lv1872: R.Tensor((1280,), dtype="float32") = model_params[688]
            lv4312 = R.call_tir(cls.layer_norm1, (lv1057_2, lv1871, lv1872), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1873: R.Tensor((1280, 10240), dtype="float32") = model_params[1534]
            lv1874_1: R.Tensor((10240,), dtype="float32") = model_params[682]
            lv1058_1 = R.call_tir(cls.fused_matmul19_add17, (lv4312, lv1873, lv1874_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1059_2 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1058_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1875_1: R.Tensor((5120, 1280), dtype="float32") = model_params[1535]
            lv1876: R.Tensor((1280,), dtype="float32") = model_params[683]
            lv1060_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1059_2, lv1875_1, lv1876, lv1057_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1877: R.Tensor((1280,), dtype="float32") = model_params[695]
            lv1878: R.Tensor((1280,), dtype="float32") = model_params[694]
            lv4325 = R.call_tir(cls.layer_norm1, (lv1060_1, lv1877, lv1878), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1879: R.Tensor((1280, 1280), dtype="float32") = model_params[1536]
            lv4327 = R.call_tir(cls.matmul13, (lv4325, lv1879), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1880: R.Tensor((1280, 1280), dtype="float32") = model_params[1537]
            lv4329 = R.call_tir(cls.matmul13, (lv4325, lv1880), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1881: R.Tensor((1280, 1280), dtype="float32") = model_params[1538]
            lv4331 = R.call_tir(cls.matmul13, (lv4325, lv1881), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1061_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv4327,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1062_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4329,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1063_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv4331,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1064_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1061_2, lv1062_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4343 = R.call_tir(cls.softmax2, (lv1064_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4344 = R.call_tir(cls.matmul15, (lv4343, lv1063_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1065_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4344,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1882: R.Tensor((1280, 1280), dtype="float32") = model_params[1539]
            lv1883_1: R.Tensor((1280,), dtype="float32") = model_params[690]
            lv1066_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1065_1, lv1882, lv1883_1, lv1060_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1884: R.Tensor((1280,), dtype="float32") = model_params[697]
            lv1885_1: R.Tensor((1280,), dtype="float32") = model_params[696]
            lv4352 = R.call_tir(cls.layer_norm1, (lv1066_1, lv1884, lv1885_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1886: R.Tensor((1280, 1280), dtype="float32") = model_params[1540]
            lv4354 = R.call_tir(cls.matmul13, (lv4352, lv1886), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1887_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1541]
            lv4356 = R.call_tir(cls.matmul16, (inp_2, lv1887_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1888: R.Tensor((2048, 1280), dtype="float32") = model_params[1542]
            lv4358 = R.call_tir(cls.matmul16, (inp_2, lv1888), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1067_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4354,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1068_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4356,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1069_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4358,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1070_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1067_1, lv1068_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4370 = R.call_tir(cls.softmax3, (lv1070_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4371 = R.call_tir(cls.matmul18, (lv4370, lv1069_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1071_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4371,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1889_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1543]
            lv1890: R.Tensor((1280,), dtype="float32") = model_params[691]
            lv1072_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1071_1, lv1889_1, lv1890, lv1066_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1891: R.Tensor((1280,), dtype="float32") = model_params[699]
            lv1892: R.Tensor((1280,), dtype="float32") = model_params[698]
            lv4379 = R.call_tir(cls.layer_norm1, (lv1072_1, lv1891, lv1892), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1893: R.Tensor((1280, 10240), dtype="float32") = model_params[1544]
            lv1894: R.Tensor((10240,), dtype="float32") = model_params[692]
            lv1073_1 = R.call_tir(cls.fused_matmul19_add17, (lv4379, lv1893, lv1894), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1074_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1073_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1895: R.Tensor((5120, 1280), dtype="float32") = model_params[1545]
            lv1896: R.Tensor((1280,), dtype="float32") = model_params[693]
            lv1075_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1074_1, lv1895, lv1896, lv1072_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1897: R.Tensor((1280,), dtype="float32") = model_params[705]
            lv1898: R.Tensor((1280,), dtype="float32") = model_params[704]
            lv4392 = R.call_tir(cls.layer_norm1, (lv1075_2, lv1897, lv1898), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1899: R.Tensor((1280, 1280), dtype="float32") = model_params[1546]
            lv4394 = R.call_tir(cls.matmul13, (lv4392, lv1899), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1900: R.Tensor((1280, 1280), dtype="float32") = model_params[1547]
            lv4396 = R.call_tir(cls.matmul13, (lv4392, lv1900), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1901_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1548]
            lv4398 = R.call_tir(cls.matmul13, (lv4392, lv1901_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1076_2 = R.call_tir(cls.fused_reshape18_transpose17, (lv4394,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1077_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4396,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1078_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4398,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1079_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1076_2, lv1077_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4410 = R.call_tir(cls.softmax2, (lv1079_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4411 = R.call_tir(cls.matmul15, (lv4410, lv1078_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1080_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4411,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1902_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1549]
            lv1903: R.Tensor((1280,), dtype="float32") = model_params[700]
            lv1081_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1080_1, lv1902_1, lv1903, lv1075_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1904: R.Tensor((1280,), dtype="float32") = model_params[707]
            lv1905: R.Tensor((1280,), dtype="float32") = model_params[706]
            lv4419 = R.call_tir(cls.layer_norm1, (lv1081_1, lv1904, lv1905), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1906: R.Tensor((1280, 1280), dtype="float32") = model_params[1550]
            lv4421 = R.call_tir(cls.matmul13, (lv4419, lv1906), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1907: R.Tensor((2048, 1280), dtype="float32") = model_params[1551]
            lv4423 = R.call_tir(cls.matmul16, (inp_2, lv1907), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1908: R.Tensor((2048, 1280), dtype="float32") = model_params[1552]
            lv4425 = R.call_tir(cls.matmul16, (inp_2, lv1908), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1082_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4421,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1083_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4423,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1084_2 = R.call_tir(cls.fused_reshape20_transpose21, (lv4425,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1085_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1082_1, lv1083_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4437 = R.call_tir(cls.softmax3, (lv1085_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4438 = R.call_tir(cls.matmul18, (lv4437, lv1084_2), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1086_2 = R.call_tir(cls.fused_transpose19_reshape19, (lv4438,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1909: R.Tensor((1280, 1280), dtype="float32") = model_params[1553]
            lv1910_1: R.Tensor((1280,), dtype="float32") = model_params[701]
            lv1087_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1086_2, lv1909, lv1910_1, lv1081_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1911: R.Tensor((1280,), dtype="float32") = model_params[709]
            lv1912: R.Tensor((1280,), dtype="float32") = model_params[708]
            lv4446 = R.call_tir(cls.layer_norm1, (lv1087_1, lv1911, lv1912), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1913: R.Tensor((1280, 10240), dtype="float32") = model_params[1554]
            lv1914: R.Tensor((10240,), dtype="float32") = model_params[702]
            lv1088_2 = R.call_tir(cls.fused_matmul19_add17, (lv4446, lv1913, lv1914), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1089_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1088_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1915: R.Tensor((5120, 1280), dtype="float32") = model_params[1555]
            lv1916: R.Tensor((1280,), dtype="float32") = model_params[703]
            lv1090_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1089_1, lv1915, lv1916, lv1087_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1917: R.Tensor((1280,), dtype="float32") = model_params[715]
            lv1918: R.Tensor((1280,), dtype="float32") = model_params[714]
            lv4459 = R.call_tir(cls.layer_norm1, (lv1090_2, lv1917, lv1918), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1919: R.Tensor((1280, 1280), dtype="float32") = model_params[1556]
            lv4461 = R.call_tir(cls.matmul13, (lv4459, lv1919), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1920: R.Tensor((1280, 1280), dtype="float32") = model_params[1557]
            lv4463 = R.call_tir(cls.matmul13, (lv4459, lv1920), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1921: R.Tensor((1280, 1280), dtype="float32") = model_params[1558]
            lv4465 = R.call_tir(cls.matmul13, (lv4459, lv1921), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1091_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4461,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1092_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4463,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1093_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4465,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1094_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1091_1, lv1092_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4477 = R.call_tir(cls.softmax2, (lv1094_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4478 = R.call_tir(cls.matmul15, (lv4477, lv1093_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1095_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4478,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1922: R.Tensor((1280, 1280), dtype="float32") = model_params[1559]
            lv1923_1: R.Tensor((1280,), dtype="float32") = model_params[710]
            lv1096_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1095_1, lv1922, lv1923_1, lv1090_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1924: R.Tensor((1280,), dtype="float32") = model_params[717]
            lv1925_1: R.Tensor((1280,), dtype="float32") = model_params[716]
            lv4486 = R.call_tir(cls.layer_norm1, (lv1096_1, lv1924, lv1925_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1926: R.Tensor((1280, 1280), dtype="float32") = model_params[1560]
            lv4488 = R.call_tir(cls.matmul13, (lv4486, lv1926), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1927_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1561]
            lv4490 = R.call_tir(cls.matmul16, (inp_2, lv1927_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1928: R.Tensor((2048, 1280), dtype="float32") = model_params[1562]
            lv4492 = R.call_tir(cls.matmul16, (inp_2, lv1928), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1097_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4488,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1098_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4490,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1099_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4492,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1100_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1097_1, lv1098_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4504 = R.call_tir(cls.softmax3, (lv1100_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4505 = R.call_tir(cls.matmul18, (lv4504, lv1099_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1101_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4505,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1929_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1563]
            lv1930: R.Tensor((1280,), dtype="float32") = model_params[711]
            lv1102_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1101_1, lv1929_1, lv1930, lv1096_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1931: R.Tensor((1280,), dtype="float32") = model_params[719]
            lv1932: R.Tensor((1280,), dtype="float32") = model_params[718]
            lv4513 = R.call_tir(cls.layer_norm1, (lv1102_2, lv1931, lv1932), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1933: R.Tensor((1280, 10240), dtype="float32") = model_params[1564]
            lv1934: R.Tensor((10240,), dtype="float32") = model_params[712]
            lv1103_2 = R.call_tir(cls.fused_matmul19_add17, (lv4513, lv1933, lv1934), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1104_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1103_2,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1935: R.Tensor((5120, 1280), dtype="float32") = model_params[1565]
            lv1936: R.Tensor((1280,), dtype="float32") = model_params[713]
            lv1105_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1104_1, lv1935, lv1936, lv1102_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1937: R.Tensor((1280,), dtype="float32") = model_params[725]
            lv1938: R.Tensor((1280,), dtype="float32") = model_params[724]
            lv4526 = R.call_tir(cls.layer_norm1, (lv1105_1, lv1937, lv1938), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1939: R.Tensor((1280, 1280), dtype="float32") = model_params[1566]
            lv4528 = R.call_tir(cls.matmul13, (lv4526, lv1939), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1940: R.Tensor((1280, 1280), dtype="float32") = model_params[1567]
            lv4530 = R.call_tir(cls.matmul13, (lv4526, lv1940), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1941_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1568]
            lv4532 = R.call_tir(cls.matmul13, (lv4526, lv1941_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1106_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4528,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1107_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4530,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1108_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4532,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1109_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1106_1, lv1107_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4544 = R.call_tir(cls.softmax2, (lv1109_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4545 = R.call_tir(cls.matmul15, (lv4544, lv1108_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1110_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4545,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1942_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1569]
            lv1943: R.Tensor((1280,), dtype="float32") = model_params[720]
            lv1111_2 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1110_1, lv1942_1, lv1943, lv1105_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1944: R.Tensor((1280,), dtype="float32") = model_params[727]
            lv1945: R.Tensor((1280,), dtype="float32") = model_params[726]
            lv4553 = R.call_tir(cls.layer_norm1, (lv1111_2, lv1944, lv1945), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1946: R.Tensor((1280, 1280), dtype="float32") = model_params[1570]
            lv4555 = R.call_tir(cls.matmul13, (lv4553, lv1946), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1947: R.Tensor((2048, 1280), dtype="float32") = model_params[1571]
            lv4557 = R.call_tir(cls.matmul16, (inp_2, lv1947), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1948: R.Tensor((2048, 1280), dtype="float32") = model_params[1572]
            lv4559 = R.call_tir(cls.matmul16, (inp_2, lv1948), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1112_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4555,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1113_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4557,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1114_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4559,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1115_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1112_1, lv1113_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4571 = R.call_tir(cls.softmax3, (lv1115_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4572 = R.call_tir(cls.matmul18, (lv4571, lv1114_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1116_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4572,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1949: R.Tensor((1280, 1280), dtype="float32") = model_params[1573]
            lv1950_1: R.Tensor((1280,), dtype="float32") = model_params[721]
            lv1117_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1116_1, lv1949, lv1950_1, lv1111_2), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1951: R.Tensor((1280,), dtype="float32") = model_params[729]
            lv1952_1: R.Tensor((1280,), dtype="float32") = model_params[728]
            lv4580 = R.call_tir(cls.layer_norm1, (lv1117_1, lv1951, lv1952_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1953: R.Tensor((1280, 10240), dtype="float32") = model_params[1574]
            lv1954_1: R.Tensor((10240,), dtype="float32") = model_params[722]
            lv1118_1 = R.call_tir(cls.fused_matmul19_add17, (lv4580, lv1953, lv1954_1), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1119_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1118_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1955: R.Tensor((5120, 1280), dtype="float32") = model_params[1575]
            lv1956_1: R.Tensor((1280,), dtype="float32") = model_params[723]
            lv1120_1 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1119_1, lv1955, lv1956_1, lv1117_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1957: R.Tensor((1280,), dtype="float32") = model_params[735]
            lv1958: R.Tensor((1280,), dtype="float32") = model_params[734]
            lv4593 = R.call_tir(cls.layer_norm1, (lv1120_1, lv1957, lv1958), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1959: R.Tensor((1280, 1280), dtype="float32") = model_params[1576]
            lv4595 = R.call_tir(cls.matmul13, (lv4593, lv1959), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1960: R.Tensor((1280, 1280), dtype="float32") = model_params[1577]
            lv4597 = R.call_tir(cls.matmul13, (lv4593, lv1960), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1961: R.Tensor((1280, 1280), dtype="float32") = model_params[1578]
            lv4599 = R.call_tir(cls.matmul13, (lv4593, lv1961), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1121_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4595,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1122_1 = R.call_tir(cls.fused_reshape18_transpose17_transpose18, (lv4597,), out_sinfo=R.Tensor((1, 20, 64, 256), dtype="float32"))
            lv1123_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4599,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1124_1 = R.call_tir(cls.fused_matmul14_multiply7, (lv1121_1, lv1122_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4611 = R.call_tir(cls.softmax2, (lv1124_1,), out_sinfo=R.Tensor((1, 20, 256, 256), dtype="float32"))
            lv4612 = R.call_tir(cls.matmul15, (lv4611, lv1123_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1125_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4612,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1962: R.Tensor((1280, 1280), dtype="float32") = model_params[1579]
            lv1963: R.Tensor((1280,), dtype="float32") = model_params[730]
            lv1126_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1125_1, lv1962, lv1963, lv1120_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1964: R.Tensor((1280,), dtype="float32") = model_params[737]
            lv1965: R.Tensor((1280,), dtype="float32") = model_params[736]
            lv4620 = R.call_tir(cls.layer_norm1, (lv1126_1, lv1964, lv1965), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1966: R.Tensor((1280, 1280), dtype="float32") = model_params[1580]
            lv4622 = R.call_tir(cls.matmul13, (lv4620, lv1966), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1967: R.Tensor((2048, 1280), dtype="float32") = model_params[1581]
            lv4624 = R.call_tir(cls.matmul16, (inp_2, lv1967), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1968_1: R.Tensor((2048, 1280), dtype="float32") = model_params[1582]
            lv4626 = R.call_tir(cls.matmul16, (inp_2, lv1968_1), out_sinfo=R.Tensor((1, 77, 1280), dtype="float32"))
            lv1127_1 = R.call_tir(cls.fused_reshape18_transpose17, (lv4622,), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1128_1 = R.call_tir(cls.fused_reshape20_transpose21_transpose22, (lv4624,), out_sinfo=R.Tensor((1, 20, 64, 77), dtype="float32"))
            lv1129_1 = R.call_tir(cls.fused_reshape20_transpose21, (lv4626,), out_sinfo=R.Tensor((1, 20, 77, 64), dtype="float32"))
            lv1130_1 = R.call_tir(cls.fused_matmul17_multiply8, (lv1127_1, lv1128_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4638 = R.call_tir(cls.softmax3, (lv1130_1,), out_sinfo=R.Tensor((1, 20, 256, 77), dtype="float32"))
            lv4639 = R.call_tir(cls.matmul18, (lv4638, lv1129_1), out_sinfo=R.Tensor((1, 20, 256, 64), dtype="float32"))
            lv1131_1 = R.call_tir(cls.fused_transpose19_reshape19, (lv4639,), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1969_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1583]
            lv1970: R.Tensor((1280,), dtype="float32") = model_params[731]
            lv1132_1 = R.call_tir(cls.fused_matmul13_add15_divide5_add16, (lv1131_1, lv1969_1, lv1970, lv1126_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1971: R.Tensor((1280,), dtype="float32") = model_params[739]
            lv1972: R.Tensor((1280,), dtype="float32") = model_params[738]
            lv4647 = R.call_tir(cls.layer_norm1, (lv1132_1, lv1971, lv1972), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1973: R.Tensor((1280, 10240), dtype="float32") = model_params[1584]
            lv1974: R.Tensor((10240,), dtype="float32") = model_params[732]
            lv1133_1 = R.call_tir(cls.fused_matmul19_add17, (lv4647, lv1973, lv1974), out_sinfo=R.Tensor((1, 256, 10240), dtype="float32"))
            lv1134_1 = R.call_tir(cls.fused_split1_gelu1_multiply9, (lv1133_1,), out_sinfo=R.Tensor((1, 256, 5120), dtype="float32"))
            lv1975: R.Tensor((5120, 1280), dtype="float32") = model_params[1585]
            lv1976: R.Tensor((1280,), dtype="float32") = model_params[733]
            lv1135_2 = R.call_tir(cls.fused_matmul20_add15_add16, (lv1134_1, lv1975, lv1976, lv1132_1), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1977_1: R.Tensor((1280, 1280), dtype="float32") = model_params[1586]
            lv1978: R.Tensor((1280,), dtype="float32") = model_params[639]
            lv1136_1 = R.call_tir(cls.fused_matmul13_add15, (lv1135_2, lv1977_1, lv1978), out_sinfo=R.Tensor((1, 256, 1280), dtype="float32"))
            lv1137_1 = R.call_tir(cls.fused_reshape21_transpose25_add14_resize2d, (lv1136_1, lv983_1), out_sinfo=R.Tensor((1, 1280, 32, 32), dtype="float32"))
            lv1979: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[764]
            lv1980: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[1587]
            lv1138_1 = R.call_tir(cls.fused_conv2d14_add18, (lv1137_1, lv1979, lv1980), out_sinfo=R.Tensor((1, 1280, 32, 32), dtype="float32"))
            lv4670 = R.call_tir(cls.concatenate6, (lv1138_1, lv173), out_sinfo=R.Tensor((1, 1920, 32, 32), dtype="float32"))
            lv1981: R.Tensor((1920,), dtype="float32") = model_params[841]
            lv1982: R.Tensor((1920,), dtype="float32") = model_params[840]
            lv1139_1 = R.call_tir(cls.fused_group_norm9_silu8, (lv4670, lv1981, lv1982), out_sinfo=R.Tensor((1, 1920, 32, 32), dtype="float32"))
            lv4676 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv1983: R.Tensor((1280, 640), dtype="float32") = model_params[1589]
            lv1984: R.Tensor((640,), dtype="float32") = model_params[844]
            lv1140_2 = R.call_tir(cls.fused_matmul4_add7_strided_slice6, (lv4676, lv1983, lv1984), out_sinfo=R.Tensor((1, 640), dtype="float32"))
            lv4681 = R.call_tir(cls.reshape9, (lv1140_2,), out_sinfo=R.Tensor((1, 640, 1, 1), dtype="float32"))
            lv1985: R.Tensor((640, 1920, 3, 3), dtype="float32") = model_params[837]
            lv1986: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1588]
            lv1141_1 = R.call_tir(cls.fused_conv2d15_add6_add6, (lv1139_1, lv1985, lv1986, lv4681), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1987: R.Tensor((640,), dtype="float32") = model_params[843]
            lv1988: R.Tensor((640,), dtype="float32") = model_params[842]
            lv1142_1 = R.call_tir(cls.fused_group_norm2_silu3, (lv1141_1, lv1987, lv1988), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1989: R.Tensor((640, 1920, 1, 1), dtype="float32") = model_params[839]
            lv1990_1: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1591]
            lv1143_1 = R.call_tir(cls.fused_conv2d16_add6, (lv4670, lv1989, lv1990_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1991: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[838]
            lv1992_1: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1590]
            lv1144_1 = R.call_tir(cls.fused_conv2d4_add6_add8_divide1, (lv1142_1, lv1991, lv1992_1, lv1143_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1993: R.Tensor((640,), dtype="float32") = model_params[766]
            lv1994_1: R.Tensor((640,), dtype="float32") = model_params[765]
            lv4693 = R.call_tir(cls.group_norm3, (lv1144_1, lv1993, lv1994_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1145_1 = R.call_tir(cls.fused_transpose5_reshape10, (lv4693,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1995: R.Tensor((640, 640), dtype="float32") = model_params[1592]
            lv1996_1: R.Tensor((640,), dtype="float32") = model_params[767]
            lv1146_1 = R.call_tir(cls.fused_matmul5_add9, (lv1145_1, lv1995, lv1996_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1997: R.Tensor((640,), dtype="float32") = model_params[774]
            lv1998: R.Tensor((640,), dtype="float32") = model_params[773]
            lv4699 = R.call_tir(cls.layer_norm, (lv1146_1, lv1997, lv1998), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1999: R.Tensor((640, 640), dtype="float32") = model_params[1593]
            lv4701 = R.call_tir(cls.matmul5, (lv4699, lv1999), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2000: R.Tensor((640, 640), dtype="float32") = model_params[1594]
            lv4703 = R.call_tir(cls.matmul5, (lv4699, lv2000), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2001: R.Tensor((640, 640), dtype="float32") = model_params[1595]
            lv4705 = R.call_tir(cls.matmul5, (lv4699, lv2001), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1147_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4701,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1148_1 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv4703,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1149_2 = R.call_tir(cls.fused_reshape11_transpose7, (lv4705,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1150_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1147_1, lv1148_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4717 = R.call_tir(cls.softmax, (lv1150_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4718 = R.call_tir(cls.matmul7, (lv4717, lv1149_2), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1151_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4718,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2002: R.Tensor((640, 640), dtype="float32") = model_params[1596]
            lv2003: R.Tensor((640,), dtype="float32") = model_params[769]
            lv1152_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1151_1, lv2002, lv2003, lv1146_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2004: R.Tensor((640,), dtype="float32") = model_params[776]
            lv2005: R.Tensor((640,), dtype="float32") = model_params[775]
            lv4726 = R.call_tir(cls.layer_norm, (lv1152_1, lv2004, lv2005), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2006: R.Tensor((640, 640), dtype="float32") = model_params[1597]
            lv4728 = R.call_tir(cls.matmul5, (lv4726, lv2006), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2007: R.Tensor((2048, 640), dtype="float32") = model_params[1598]
            lv4730 = R.call_tir(cls.matmul8, (inp_2, lv2007), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2008_1: R.Tensor((2048, 640), dtype="float32") = model_params[1599]
            lv4732 = R.call_tir(cls.matmul8, (inp_2, lv2008_1), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1153_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4728,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1154_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv4730,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1155_2 = R.call_tir(cls.fused_reshape13_transpose11, (lv4732,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1156_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv1153_1, lv1154_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4744 = R.call_tir(cls.softmax1, (lv1156_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4745 = R.call_tir(cls.matmul10, (lv4744, lv1155_2), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1157_2 = R.call_tir(cls.fused_transpose9_reshape12, (lv4745,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2009_1: R.Tensor((640, 640), dtype="float32") = model_params[1600]
            lv2010: R.Tensor((640,), dtype="float32") = model_params[770]
            lv1158_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1157_2, lv2009_1, lv2010, lv1152_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2011: R.Tensor((640,), dtype="float32") = model_params[778]
            lv2012: R.Tensor((640,), dtype="float32") = model_params[777]
            lv4753 = R.call_tir(cls.layer_norm, (lv1158_1, lv2011, lv2012), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2013: R.Tensor((640, 5120), dtype="float32") = model_params[1601]
            lv2014: R.Tensor((5120,), dtype="float32") = model_params[771]
            lv1159_2 = R.call_tir(cls.fused_matmul11_add11, (lv4753, lv2013, lv2014), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1160_1 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1159_2,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2015: R.Tensor((2560, 640), dtype="float32") = model_params[1602]
            lv2016: R.Tensor((640,), dtype="float32") = model_params[772]
            lv1161_2 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1160_1, lv2015, lv2016, lv1158_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2017_1: R.Tensor((640,), dtype="float32") = model_params[784]
            lv2018: R.Tensor((640,), dtype="float32") = model_params[783]
            lv4766 = R.call_tir(cls.layer_norm, (lv1161_2, lv2017_1, lv2018), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2019_1: R.Tensor((640, 640), dtype="float32") = model_params[1603]
            lv4768 = R.call_tir(cls.matmul5, (lv4766, lv2019_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2020: R.Tensor((640, 640), dtype="float32") = model_params[1604]
            lv4770 = R.call_tir(cls.matmul5, (lv4766, lv2020), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2021_1: R.Tensor((640, 640), dtype="float32") = model_params[1605]
            lv4772 = R.call_tir(cls.matmul5, (lv4766, lv2021_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1162_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4768,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1163_1 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv4770,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1164_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4772,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1165_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1162_1, lv1163_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4784 = R.call_tir(cls.softmax, (lv1165_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4785 = R.call_tir(cls.matmul7, (lv4784, lv1164_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1166_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4785,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2022: R.Tensor((640, 640), dtype="float32") = model_params[1606]
            lv2023_1: R.Tensor((640,), dtype="float32") = model_params[779]
            lv1167_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1166_1, lv2022, lv2023_1, lv1161_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2024: R.Tensor((640,), dtype="float32") = model_params[786]
            lv2025: R.Tensor((640,), dtype="float32") = model_params[785]
            lv4793 = R.call_tir(cls.layer_norm, (lv1167_1, lv2024, lv2025), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2026: R.Tensor((640, 640), dtype="float32") = model_params[1607]
            lv4795 = R.call_tir(cls.matmul5, (lv4793, lv2026), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2027: R.Tensor((2048, 640), dtype="float32") = model_params[1608]
            lv4797 = R.call_tir(cls.matmul8, (inp_2, lv2027), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2028: R.Tensor((2048, 640), dtype="float32") = model_params[1609]
            lv4799 = R.call_tir(cls.matmul8, (inp_2, lv2028), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1168_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4795,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1169_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv4797,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1170_1 = R.call_tir(cls.fused_reshape13_transpose11, (lv4799,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1171_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv1168_1, lv1169_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4811 = R.call_tir(cls.softmax1, (lv1171_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4812 = R.call_tir(cls.matmul10, (lv4811, lv1170_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1172_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4812,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2029: R.Tensor((640, 640), dtype="float32") = model_params[1610]
            lv2030: R.Tensor((640,), dtype="float32") = model_params[780]
            lv1173_2 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1172_1, lv2029, lv2030, lv1167_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2031: R.Tensor((640,), dtype="float32") = model_params[788]
            lv2032: R.Tensor((640,), dtype="float32") = model_params[787]
            lv4820 = R.call_tir(cls.layer_norm, (lv1173_2, lv2031, lv2032), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2033: R.Tensor((640, 5120), dtype="float32") = model_params[1611]
            lv2034: R.Tensor((5120,), dtype="float32") = model_params[781]
            lv1174_2 = R.call_tir(cls.fused_matmul11_add11, (lv4820, lv2033, lv2034), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1175_1 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1174_2,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2035_1: R.Tensor((2560, 640), dtype="float32") = model_params[1612]
            lv2036_1: R.Tensor((640,), dtype="float32") = model_params[782]
            lv1176_1 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1175_1, lv2035_1, lv2036_1, lv1173_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2037: R.Tensor((640, 640), dtype="float32") = model_params[1613]
            lv2038: R.Tensor((640,), dtype="float32") = model_params[768]
            lv1177_1 = R.call_tir(cls.fused_matmul5_add9, (lv1176_1, lv2037, lv2038), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1178_1 = R.call_tir(cls.fused_reshape14_transpose15_add8_concatenate7, (lv1177_1, lv1144_1, lv134), out_sinfo=R.Tensor((1, 1280, 32, 32), dtype="float32"))
            lv2039: R.Tensor((1280,), dtype="float32") = model_params[849]
            lv2040: R.Tensor((1280,), dtype="float32") = model_params[848]
            lv1179_1 = R.call_tir(cls.fused_group_norm10_silu9, (lv1178_1, lv2039, lv2040), out_sinfo=R.Tensor((1, 1280, 32, 32), dtype="float32"))
            lv4845 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2041: R.Tensor((1280, 640), dtype="float32") = model_params[1615]
            lv2042: R.Tensor((640,), dtype="float32") = model_params[852]
            lv1180_1 = R.call_tir(cls.fused_matmul4_add7_strided_slice6, (lv4845, lv2041, lv2042), out_sinfo=R.Tensor((1, 640), dtype="float32"))
            lv4850 = R.call_tir(cls.reshape9, (lv1180_1,), out_sinfo=R.Tensor((1, 640, 1, 1), dtype="float32"))
            lv2043: R.Tensor((640, 1280, 3, 3), dtype="float32") = model_params[845]
            lv2044_1: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1614]
            lv1181_1 = R.call_tir(cls.fused_conv2d17_add6_add6, (lv1179_1, lv2043, lv2044_1, lv4850), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2045: R.Tensor((640,), dtype="float32") = model_params[851]
            lv2046: R.Tensor((640,), dtype="float32") = model_params[850]
            lv1182_2 = R.call_tir(cls.fused_group_norm2_silu3, (lv1181_1, lv2045, lv2046), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2047: R.Tensor((640, 1280, 1, 1), dtype="float32") = model_params[847]
            lv2048: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1617]
            lv1183_1 = R.call_tir(cls.fused_conv2d18_add6, (lv1178_1, lv2047, lv2048), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2049: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[846]
            lv2050: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1616]
            lv1184_2 = R.call_tir(cls.fused_conv2d4_add6_add8_divide1, (lv1182_2, lv2049, lv2050, lv1183_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2051: R.Tensor((640,), dtype="float32") = model_params[790]
            lv2052: R.Tensor((640,), dtype="float32") = model_params[789]
            lv4862 = R.call_tir(cls.group_norm3, (lv1184_2, lv2051, lv2052), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1185_1 = R.call_tir(cls.fused_transpose5_reshape10, (lv4862,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2053: R.Tensor((640, 640), dtype="float32") = model_params[1618]
            lv2054: R.Tensor((640,), dtype="float32") = model_params[791]
            lv1186_2 = R.call_tir(cls.fused_matmul5_add9, (lv1185_1, lv2053, lv2054), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2055: R.Tensor((640,), dtype="float32") = model_params[798]
            lv2056: R.Tensor((640,), dtype="float32") = model_params[797]
            lv4868 = R.call_tir(cls.layer_norm, (lv1186_2, lv2055, lv2056), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2057_1: R.Tensor((640, 640), dtype="float32") = model_params[1619]
            lv4870 = R.call_tir(cls.matmul5, (lv4868, lv2057_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2058: R.Tensor((640, 640), dtype="float32") = model_params[1620]
            lv4872 = R.call_tir(cls.matmul5, (lv4868, lv2058), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2059_1: R.Tensor((640, 640), dtype="float32") = model_params[1621]
            lv4874 = R.call_tir(cls.matmul5, (lv4868, lv2059_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1187_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4870,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1188_2 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv4872,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1189_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4874,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1190_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1187_1, lv1188_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4886 = R.call_tir(cls.softmax, (lv1190_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4887 = R.call_tir(cls.matmul7, (lv4886, lv1189_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1191_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4887,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2060: R.Tensor((640, 640), dtype="float32") = model_params[1622]
            lv2061_1: R.Tensor((640,), dtype="float32") = model_params[793]
            lv1192_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1191_1, lv2060, lv2061_1, lv1186_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2062: R.Tensor((640,), dtype="float32") = model_params[800]
            lv2063_1: R.Tensor((640,), dtype="float32") = model_params[799]
            lv4895 = R.call_tir(cls.layer_norm, (lv1192_1, lv2062, lv2063_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2064: R.Tensor((640, 640), dtype="float32") = model_params[1623]
            lv4897 = R.call_tir(cls.matmul5, (lv4895, lv2064), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2065: R.Tensor((2048, 640), dtype="float32") = model_params[1624]
            lv4899 = R.call_tir(cls.matmul8, (inp_2, lv2065), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2066: R.Tensor((2048, 640), dtype="float32") = model_params[1625]
            lv4901 = R.call_tir(cls.matmul8, (inp_2, lv2066), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1193_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4897,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1194_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv4899,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1195_1 = R.call_tir(cls.fused_reshape13_transpose11, (lv4901,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1196_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv1193_1, lv1194_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4913 = R.call_tir(cls.softmax1, (lv1196_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4914 = R.call_tir(cls.matmul10, (lv4913, lv1195_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1197_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4914,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2067: R.Tensor((640, 640), dtype="float32") = model_params[1626]
            lv2068: R.Tensor((640,), dtype="float32") = model_params[794]
            lv1198_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1197_1, lv2067, lv2068, lv1192_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2069: R.Tensor((640,), dtype="float32") = model_params[802]
            lv2070: R.Tensor((640,), dtype="float32") = model_params[801]
            lv4922 = R.call_tir(cls.layer_norm, (lv1198_1, lv2069, lv2070), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2071: R.Tensor((640, 5120), dtype="float32") = model_params[1627]
            lv2072: R.Tensor((5120,), dtype="float32") = model_params[795]
            lv1199_1 = R.call_tir(cls.fused_matmul11_add11, (lv4922, lv2071, lv2072), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1200_2 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1199_1,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2073: R.Tensor((2560, 640), dtype="float32") = model_params[1628]
            lv2074: R.Tensor((640,), dtype="float32") = model_params[796]
            lv1201_2 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1200_2, lv2073, lv2074, lv1198_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2075_1: R.Tensor((640,), dtype="float32") = model_params[808]
            lv2076_1: R.Tensor((640,), dtype="float32") = model_params[807]
            lv4935 = R.call_tir(cls.layer_norm, (lv1201_2, lv2075_1, lv2076_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2077: R.Tensor((640, 640), dtype="float32") = model_params[1629]
            lv4937 = R.call_tir(cls.matmul5, (lv4935, lv2077), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2078: R.Tensor((640, 640), dtype="float32") = model_params[1630]
            lv4939 = R.call_tir(cls.matmul5, (lv4935, lv2078), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2079: R.Tensor((640, 640), dtype="float32") = model_params[1631]
            lv4941 = R.call_tir(cls.matmul5, (lv4935, lv2079), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1202_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4937,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1203_1 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv4939,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1204_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4941,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1205_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1202_1, lv1203_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4953 = R.call_tir(cls.softmax, (lv1205_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv4954 = R.call_tir(cls.matmul7, (lv4953, lv1204_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1206_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4954,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2080: R.Tensor((640, 640), dtype="float32") = model_params[1632]
            lv2081: R.Tensor((640,), dtype="float32") = model_params[803]
            lv1207_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1206_1, lv2080, lv2081, lv1201_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2082: R.Tensor((640,), dtype="float32") = model_params[810]
            lv2083: R.Tensor((640,), dtype="float32") = model_params[809]
            lv4962 = R.call_tir(cls.layer_norm, (lv1207_1, lv2082, lv2083), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2084_1: R.Tensor((640, 640), dtype="float32") = model_params[1633]
            lv4964 = R.call_tir(cls.matmul5, (lv4962, lv2084_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2085: R.Tensor((2048, 640), dtype="float32") = model_params[1634]
            lv4966 = R.call_tir(cls.matmul8, (inp_2, lv2085), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2086_1: R.Tensor((2048, 640), dtype="float32") = model_params[1635]
            lv4968 = R.call_tir(cls.matmul8, (inp_2, lv2086_1), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1208_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv4964,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1209_2 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv4966,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1210_1 = R.call_tir(cls.fused_reshape13_transpose11, (lv4968,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1211_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv1208_1, lv1209_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4980 = R.call_tir(cls.softmax1, (lv1211_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv4981 = R.call_tir(cls.matmul10, (lv4980, lv1210_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1212_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv4981,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2087: R.Tensor((640, 640), dtype="float32") = model_params[1636]
            lv2088_1: R.Tensor((640,), dtype="float32") = model_params[804]
            lv1213_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1212_1, lv2087, lv2088_1, lv1207_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2089: R.Tensor((640,), dtype="float32") = model_params[812]
            lv2090_1: R.Tensor((640,), dtype="float32") = model_params[811]
            lv4989 = R.call_tir(cls.layer_norm, (lv1213_1, lv2089, lv2090_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2091: R.Tensor((640, 5120), dtype="float32") = model_params[1637]
            lv2092: R.Tensor((5120,), dtype="float32") = model_params[805]
            lv1214_1 = R.call_tir(cls.fused_matmul11_add11, (lv4989, lv2091, lv2092), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1215_1 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1214_1,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2093: R.Tensor((2560, 640), dtype="float32") = model_params[1638]
            lv2094: R.Tensor((640,), dtype="float32") = model_params[806]
            lv1216_1 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1215_1, lv2093, lv2094, lv1213_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2095: R.Tensor((640, 640), dtype="float32") = model_params[1639]
            lv2096: R.Tensor((640,), dtype="float32") = model_params[792]
            lv1217_1 = R.call_tir(cls.fused_matmul5_add9, (lv1216_1, lv2095, lv2096), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1218_1 = R.call_tir(cls.fused_reshape14_transpose15_add8_concatenate8, (lv1217_1, lv1184_2, lv94), out_sinfo=R.Tensor((1, 960, 32, 32), dtype="float32"))
            lv2097: R.Tensor((960,), dtype="float32") = model_params[857]
            lv2098: R.Tensor((960,), dtype="float32") = model_params[856]
            lv1219_1 = R.call_tir(cls.fused_group_norm11_silu10, (lv1218_1, lv2097, lv2098), out_sinfo=R.Tensor((1, 960, 32, 32), dtype="float32"))
            lv5014 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2099: R.Tensor((1280, 640), dtype="float32") = model_params[1641]
            lv2100: R.Tensor((640,), dtype="float32") = model_params[860]
            lv1220_1 = R.call_tir(cls.fused_matmul4_add7_strided_slice6, (lv5014, lv2099, lv2100), out_sinfo=R.Tensor((1, 640), dtype="float32"))
            lv5019 = R.call_tir(cls.reshape9, (lv1220_1,), out_sinfo=R.Tensor((1, 640, 1, 1), dtype="float32"))
            lv2101: R.Tensor((640, 960, 3, 3), dtype="float32") = model_params[853]
            lv2102_1: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1640]
            lv1221_1 = R.call_tir(cls.fused_conv2d19_add6_add6, (lv1219_1, lv2101, lv2102_1, lv5019), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2103_1: R.Tensor((640,), dtype="float32") = model_params[859]
            lv2104: R.Tensor((640,), dtype="float32") = model_params[858]
            lv1222_2 = R.call_tir(cls.fused_group_norm2_silu3, (lv1221_1, lv2103_1, lv2104), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2105: R.Tensor((640, 960, 1, 1), dtype="float32") = model_params[855]
            lv2106: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1643]
            lv1223_1 = R.call_tir(cls.fused_conv2d20_add6, (lv1218_1, lv2105, lv2106), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2107: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[854]
            lv2108: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1642]
            lv1224_2 = R.call_tir(cls.fused_conv2d4_add6_add8_divide1, (lv1222_2, lv2107, lv2108, lv1223_1), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv2109: R.Tensor((640,), dtype="float32") = model_params[814]
            lv2110: R.Tensor((640,), dtype="float32") = model_params[813]
            lv5031 = R.call_tir(cls.group_norm3, (lv1224_2, lv2109, lv2110), out_sinfo=R.Tensor((1, 640, 32, 32), dtype="float32"))
            lv1225_1 = R.call_tir(cls.fused_transpose5_reshape10, (lv5031,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2111_1: R.Tensor((640, 640), dtype="float32") = model_params[1644]
            lv2112: R.Tensor((640,), dtype="float32") = model_params[815]
            lv1226_2 = R.call_tir(cls.fused_matmul5_add9, (lv1225_1, lv2111_1, lv2112), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2113: R.Tensor((640,), dtype="float32") = model_params[822]
            lv2114: R.Tensor((640,), dtype="float32") = model_params[821]
            lv5037 = R.call_tir(cls.layer_norm, (lv1226_2, lv2113, lv2114), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2115: R.Tensor((640, 640), dtype="float32") = model_params[1645]
            lv5039 = R.call_tir(cls.matmul5, (lv5037, lv2115), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2116: R.Tensor((640, 640), dtype="float32") = model_params[1646]
            lv5041 = R.call_tir(cls.matmul5, (lv5037, lv2116), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2117: R.Tensor((640, 640), dtype="float32") = model_params[1647]
            lv5043 = R.call_tir(cls.matmul5, (lv5037, lv2117), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1227_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5039,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1228_2 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv5041,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1229_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5043,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1230_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1227_1, lv1228_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv5055 = R.call_tir(cls.softmax, (lv1230_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv5056 = R.call_tir(cls.matmul7, (lv5055, lv1229_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1231_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv5056,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2118: R.Tensor((640, 640), dtype="float32") = model_params[1648]
            lv2119: R.Tensor((640,), dtype="float32") = model_params[817]
            lv1232_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1231_1, lv2118, lv2119, lv1226_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2120: R.Tensor((640,), dtype="float32") = model_params[824]
            lv2121: R.Tensor((640,), dtype="float32") = model_params[823]
            lv5064 = R.call_tir(cls.layer_norm, (lv1232_1, lv2120, lv2121), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2122: R.Tensor((640, 640), dtype="float32") = model_params[1649]
            lv5066 = R.call_tir(cls.matmul5, (lv5064, lv2122), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2123: R.Tensor((2048, 640), dtype="float32") = model_params[1650]
            lv5068 = R.call_tir(cls.matmul8, (inp_2, lv2123), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2124_1: R.Tensor((2048, 640), dtype="float32") = model_params[1651]
            lv5070 = R.call_tir(cls.matmul8, (inp_2, lv2124_1), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1233_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5066,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1234_1 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv5068,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1235_1 = R.call_tir(cls.fused_reshape13_transpose11, (lv5070,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1236_1 = R.call_tir(cls.fused_matmul9_multiply5, (lv1233_1, lv1234_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv5082 = R.call_tir(cls.softmax1, (lv1236_1,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv5083 = R.call_tir(cls.matmul10, (lv5082, lv1235_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1237_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv5083,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2125: R.Tensor((640, 640), dtype="float32") = model_params[1652]
            lv2126_1: R.Tensor((640,), dtype="float32") = model_params[818]
            lv1238_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1237_1, lv2125, lv2126_1, lv1232_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2127: R.Tensor((640,), dtype="float32") = model_params[826]
            lv2128_1: R.Tensor((640,), dtype="float32") = model_params[825]
            lv5091 = R.call_tir(cls.layer_norm, (lv1238_1, lv2127, lv2128_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2129: R.Tensor((640, 5120), dtype="float32") = model_params[1653]
            lv2130_1: R.Tensor((5120,), dtype="float32") = model_params[819]
            lv1239_1 = R.call_tir(cls.fused_matmul11_add11, (lv5091, lv2129, lv2130_1), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1240_2 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1239_1,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2131: R.Tensor((2560, 640), dtype="float32") = model_params[1654]
            lv2132: R.Tensor((640,), dtype="float32") = model_params[820]
            lv1241_2 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1240_2, lv2131, lv2132, lv1238_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2133: R.Tensor((640,), dtype="float32") = model_params[832]
            lv2134: R.Tensor((640,), dtype="float32") = model_params[831]
            lv5104 = R.call_tir(cls.layer_norm, (lv1241_2, lv2133, lv2134), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2135: R.Tensor((640, 640), dtype="float32") = model_params[1655]
            lv5106 = R.call_tir(cls.matmul5, (lv5104, lv2135), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2136: R.Tensor((640, 640), dtype="float32") = model_params[1656]
            lv5108 = R.call_tir(cls.matmul5, (lv5104, lv2136), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2137: R.Tensor((640, 640), dtype="float32") = model_params[1657]
            lv5110 = R.call_tir(cls.matmul5, (lv5104, lv2137), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1242_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5106,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1243_1 = R.call_tir(cls.fused_reshape11_transpose7_transpose8, (lv5108,), out_sinfo=R.Tensor((1, 10, 64, 1024), dtype="float32"))
            lv1244_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5110,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1245_1 = R.call_tir(cls.fused_matmul6_multiply4, (lv1242_1, lv1243_1, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv5122 = R.call_tir(cls.softmax, (lv1245_1,), out_sinfo=R.Tensor((1, 10, 1024, 1024), dtype="float32"))
            lv5123 = R.call_tir(cls.matmul7, (lv5122, lv1244_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1246_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv5123,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2138: R.Tensor((640, 640), dtype="float32") = model_params[1658]
            lv2139: R.Tensor((640,), dtype="float32") = model_params[827]
            lv1247_1 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1246_1, lv2138, lv2139, lv1241_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2140: R.Tensor((640,), dtype="float32") = model_params[834]
            lv2141: R.Tensor((640,), dtype="float32") = model_params[833]
            lv5131 = R.call_tir(cls.layer_norm, (lv1247_1, lv2140, lv2141), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2142_1: R.Tensor((640, 640), dtype="float32") = model_params[1659]
            lv5133 = R.call_tir(cls.matmul5, (lv5131, lv2142_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2143_1: R.Tensor((2048, 640), dtype="float32") = model_params[1660]
            lv5135 = R.call_tir(cls.matmul8, (inp_2, lv2143_1), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv2144: R.Tensor((2048, 640), dtype="float32") = model_params[1661]
            lv5137 = R.call_tir(cls.matmul8, (inp_2, lv2144), out_sinfo=R.Tensor((1, 77, 640), dtype="float32"))
            lv1248_1 = R.call_tir(cls.fused_reshape11_transpose7, (lv5133,), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1249_2 = R.call_tir(cls.fused_reshape13_transpose11_transpose12, (lv5135,), out_sinfo=R.Tensor((1, 10, 64, 77), dtype="float32"))
            lv1250_1 = R.call_tir(cls.fused_reshape13_transpose11, (lv5137,), out_sinfo=R.Tensor((1, 10, 77, 64), dtype="float32"))
            lv1251_2 = R.call_tir(cls.fused_matmul9_multiply5, (lv1248_1, lv1249_2, R.const(0.125, "float32")), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv5149 = R.call_tir(cls.softmax1, (lv1251_2,), out_sinfo=R.Tensor((1, 10, 1024, 77), dtype="float32"))
            lv5150 = R.call_tir(cls.matmul10, (lv5149, lv1250_1), out_sinfo=R.Tensor((1, 10, 1024, 64), dtype="float32"))
            lv1252_1 = R.call_tir(cls.fused_transpose9_reshape12, (lv5150,), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2145: R.Tensor((640, 640), dtype="float32") = model_params[1662]
            lv2146: R.Tensor((640,), dtype="float32") = model_params[828]
            lv1253_2 = R.call_tir(cls.fused_matmul5_add9_divide3_add10, (lv1252_1, lv2145, lv2146, lv1247_1), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2147: R.Tensor((640,), dtype="float32") = model_params[836]
            lv2148: R.Tensor((640,), dtype="float32") = model_params[835]
            lv5158 = R.call_tir(cls.layer_norm, (lv1253_2, lv2147, lv2148), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2149: R.Tensor((640, 5120), dtype="float32") = model_params[1663]
            lv2150: R.Tensor((5120,), dtype="float32") = model_params[829]
            lv1254_1 = R.call_tir(cls.fused_matmul11_add11, (lv5158, lv2149, lv2150), out_sinfo=R.Tensor((1, 1024, 5120), dtype="float32"))
            lv1255_2 = R.call_tir(cls.fused_split_gelu_multiply6, (lv1254_1,), out_sinfo=R.Tensor((1, 1024, 2560), dtype="float32"))
            lv2151_1: R.Tensor((2560, 640), dtype="float32") = model_params[1664]
            lv2152: R.Tensor((640,), dtype="float32") = model_params[830]
            lv1256_1 = R.call_tir(cls.fused_matmul12_add9_add10, (lv1255_2, lv2151_1, lv2152, lv1253_2), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv2153_1: R.Tensor((640, 640), dtype="float32") = model_params[1665]
            lv2154: R.Tensor((640,), dtype="float32") = model_params[816]
            lv1257_1 = R.call_tir(cls.fused_matmul5_add9, (lv1256_1, lv2153_1, lv2154), out_sinfo=R.Tensor((1, 1024, 640), dtype="float32"))
            lv1258_1 = R.call_tir(cls.fused_reshape14_transpose15_add8_resize2d1, (lv1257_1, lv1224_2), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv2155_1: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[861]
            lv2156: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[1666]
            lv1259_1 = R.call_tir(cls.fused_conv2d21_add19, (lv1258_1, lv2155_1, lv2156), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv5181 = R.call_tir(cls.concatenate9, (lv1259_1, lv93), out_sinfo=R.Tensor((1, 960, 64, 64), dtype="float32"))
            lv2157_1: R.Tensor((960,), dtype="float32") = model_params[866]
            lv2158: R.Tensor((960,), dtype="float32") = model_params[865]
            lv1260_1 = R.call_tir(cls.fused_group_norm12_silu11, (lv5181, lv2157_1, lv2158), out_sinfo=R.Tensor((1, 960, 64, 64), dtype="float32"))
            lv5187 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2159: R.Tensor((1280, 320), dtype="float32") = model_params[1668]
            lv2160: R.Tensor((320,), dtype="float32") = model_params[869]
            lv1261_1 = R.call_tir(cls.fused_matmul3_add3_cast1, (lv5187, lv2159, lv2160), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv5192 = R.call_tir(cls.reshape7, (lv1261_1,), out_sinfo=R.Tensor((1, 320, 1, 1), dtype="float32"))
            lv2161: R.Tensor((320, 960, 3, 3), dtype="float32") = model_params[862]
            lv2162: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1667]
            lv1262_1 = R.call_tir(cls.fused_conv2d22_add2_add2, (lv1260_1, lv2161, lv2162, lv5192), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2163: R.Tensor((320,), dtype="float32") = model_params[868]
            lv2164: R.Tensor((320,), dtype="float32") = model_params[867]
            lv1263_1 = R.call_tir(cls.fused_group_norm_silu1, (lv1262_1, lv2163, lv2164), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2165: R.Tensor((320, 960, 1, 1), dtype="float32") = model_params[864]
            lv2166: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1670]
            lv1264_1 = R.call_tir(cls.fused_conv2d23_add2, (lv5181, lv2165, lv2166), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2167: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[863]
            lv2168: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1669]
            lv1265_1 = R.call_tir(cls.fused_conv2d1_add2_add4_divide, (lv1263_1, lv2167, lv2168, lv1264_1), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv5204 = R.call_tir(cls.concatenate10, (lv1265_1, lv88), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv2169_1: R.Tensor((640,), dtype="float32") = model_params[874]
            lv2170_1: R.Tensor((640,), dtype="float32") = model_params[873]
            lv1266_1 = R.call_tir(cls.fused_group_norm13_silu12, (lv5204, lv2169_1, lv2170_1), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv5210 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2171: R.Tensor((1280, 320), dtype="float32") = model_params[1672]
            lv2172: R.Tensor((320,), dtype="float32") = model_params[877]
            lv1267_2 = R.call_tir(cls.fused_matmul3_add3_cast1, (lv5210, lv2171, lv2172), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv5215 = R.call_tir(cls.reshape7, (lv1267_2,), out_sinfo=R.Tensor((1, 320, 1, 1), dtype="float32"))
            lv2173: R.Tensor((320, 640, 3, 3), dtype="float32") = model_params[870]
            lv2174: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1671]
            lv1268_2 = R.call_tir(cls.fused_conv2d24_add2_add2, (lv1266_1, lv2173, lv2174, lv5215), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2175: R.Tensor((320,), dtype="float32") = model_params[876]
            lv2176: R.Tensor((320,), dtype="float32") = model_params[875]
            lv1269_1 = R.call_tir(cls.fused_group_norm_silu1, (lv1268_2, lv2175, lv2176), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2177: R.Tensor((320, 640, 1, 1), dtype="float32") = model_params[872]
            lv2178_1: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1674]
            lv1270_1 = R.call_tir(cls.fused_conv2d25_add2, (lv5204, lv2177, lv2178_1), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2179: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[871]
            lv2180: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1673]
            lv1271_1 = R.call_tir(cls.fused_conv2d1_add2_add4_divide, (lv1269_1, lv2179, lv2180, lv1270_1), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv5227 = R.call_tir(cls.concatenate10, (lv1271_1, lv83), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv2181: R.Tensor((640,), dtype="float32") = model_params[882]
            lv2182: R.Tensor((640,), dtype="float32") = model_params[881]
            lv1272_1 = R.call_tir(cls.fused_group_norm13_silu12, (lv5227, lv2181, lv2182), out_sinfo=R.Tensor((1, 640, 64, 64), dtype="float32"))
            lv5233 = R.call_tir(cls.silu, (lv82,), out_sinfo=R.Tensor((1, 1280), dtype="float32"))
            lv2183: R.Tensor((1280, 320), dtype="float32") = model_params[1676]
            lv2184: R.Tensor((320,), dtype="float32") = model_params[885]
            lv1273_1 = R.call_tir(cls.fused_matmul3_add3_cast1, (lv5233, lv2183, lv2184), out_sinfo=R.Tensor((1, 320), dtype="float32"))
            lv5238 = R.call_tir(cls.reshape7, (lv1273_1,), out_sinfo=R.Tensor((1, 320, 1, 1), dtype="float32"))
            lv2185: R.Tensor((320, 640, 3, 3), dtype="float32") = model_params[878]
            lv2186: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1675]
            lv1274_1 = R.call_tir(cls.fused_conv2d24_add2_add2, (lv1272_1, lv2185, lv2186, lv5238), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2187: R.Tensor((320,), dtype="float32") = model_params[884]
            lv2188: R.Tensor((320,), dtype="float32") = model_params[883]
            lv1275_1 = R.call_tir(cls.fused_group_norm_silu1, (lv1274_1, lv2187, lv2188), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2189: R.Tensor((320, 640, 1, 1), dtype="float32") = model_params[880]
            lv2190: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1678]
            lv1276_2 = R.call_tir(cls.fused_conv2d25_add2, (lv5227, lv2189, lv2190), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2191_1: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[879]
            lv2192: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[1677]
            lv1277_1 = R.call_tir(cls.fused_conv2d1_add2_add4_divide, (lv1275_1, lv2191_1, lv2192, lv1276_2), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2193_1: R.Tensor((320,), dtype="float32") = model_params[4]
            lv2194: R.Tensor((320,), dtype="float32") = model_params[3]
            lv1278_1 = R.call_tir(cls.fused_group_norm_silu1, (lv1277_1, lv2193_1, lv2194), out_sinfo=R.Tensor((1, 320, 64, 64), dtype="float32"))
            lv2195_1: R.Tensor((4, 320, 3, 3), dtype="float32") = model_params[5]
            lv2196: R.Tensor((1, 4, 1, 1), dtype="float32") = model_params[1679]
            lv1279_1 = R.call_tir(cls.fused_conv2d26_add20, (lv1278_1, lv2195_1, lv2196), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tensor((1, 4, 64, 64), dtype="float32") = lv1279_1
            R.output(gv)
        return gv

    @R.function
    def vae(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), model_params: R.Tuple(R.Tensor((512, 4, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((3, 128, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((256, 512, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 512, 1, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((128, 256, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 256, 1, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((4, 4, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 3, 1, 1), dtype="float32"))) -> R.Tensor((1, 512, 512, 3), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.multiply10, (inp_0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv2197: R.Tensor((4, 4, 1, 1), dtype="float32") = model_params[99]
            lv2198: R.Tensor((1, 4, 1, 1), dtype="float32") = model_params[100]
            lv_1 = R.call_tir(cls.fused_conv2d27_add20, (lv, lv2197, lv2198), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv2199: R.Tensor((512, 4, 3, 3), dtype="float32") = model_params[0]
            lv2200: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[101]
            lv1 = R.call_tir(cls.fused_conv2d28_add21, (lv_1, lv2199, lv2200), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2201: R.Tensor((512,), dtype="float32") = model_params[13]
            lv2202: R.Tensor((512,), dtype="float32") = model_params[12]
            lv2 = R.call_tir(cls.fused_group_norm14_silu13, (lv1, lv2201, lv2202), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2203: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[10]
            lv2204: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[102]
            lv3 = R.call_tir(cls.fused_conv2d29_add21, (lv2, lv2203, lv2204), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2205: R.Tensor((512,), dtype="float32") = model_params[15]
            lv2206: R.Tensor((512,), dtype="float32") = model_params[14]
            lv4 = R.call_tir(cls.fused_group_norm14_silu13, (lv3, lv2205, lv2206), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2207: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[11]
            lv2208: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[103]
            lv5 = R.call_tir(cls.fused_conv2d29_add21_add22_divide6, (lv4, lv2207, lv2208, lv1), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv6 = R.call_tir(cls.fused_reshape24_transpose26_transpose27, (lv5,), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv2209: R.Tensor((512,), dtype="float32") = model_params[5]
            lv2210: R.Tensor((512,), dtype="float32") = model_params[4]
            lv22 = R.call_tir(cls.group_norm15, (lv6, lv2209, lv2210), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv23 = R.call_tir(cls.transpose26, (lv22,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv2211: R.Tensor((512, 512), dtype="float32") = model_params[104]
            lv2212: R.Tensor((512,), dtype="float32") = model_params[8]
            lv7 = R.call_tir(cls.fused_matmul21_add23, (lv23, lv2211, lv2212), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv2213: R.Tensor((512, 512), dtype="float32") = model_params[105]
            lv2214: R.Tensor((512,), dtype="float32") = model_params[6]
            lv8 = R.call_tir(cls.fused_matmul21_add23, (lv23, lv2213, lv2214), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv2215: R.Tensor((512, 512), dtype="float32") = model_params[106]
            lv2216: R.Tensor((512,), dtype="float32") = model_params[9]
            lv9 = R.call_tir(cls.fused_matmul21_add23, (lv23, lv2215, lv2216), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv10 = R.call_tir(cls.fused_reshape25_transpose29, (lv7,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv11 = R.call_tir(cls.fused_reshape25_transpose29_transpose30, (lv8,), out_sinfo=R.Tensor((1, 1, 512, 4096), dtype="float32"))
            lv12 = R.call_tir(cls.fused_reshape25_transpose29, (lv9,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv13 = R.call_tir(cls.fused_matmul22_multiply11, (lv10, lv11, R.const(0.044194173067808151, "float32")), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv44 = R.call_tir(cls.softmax4, (lv13,), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv45 = R.call_tir(cls.matmul23, (lv44, lv12), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv14 = R.call_tir(cls.fused_transpose31_reshape26, (lv45,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv2217: R.Tensor((512, 512), dtype="float32") = model_params[107]
            lv2218: R.Tensor((512,), dtype="float32") = model_params[7]
            lv15 = R.call_tir(cls.fused_matmul21_add23, (lv14, lv2217, lv2218), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv16 = R.call_tir(cls.fused_transpose27_reshape27_add22_divide6, (lv15, lv5), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2219: R.Tensor((512,), dtype="float32") = model_params[19]
            lv2220: R.Tensor((512,), dtype="float32") = model_params[18]
            lv17 = R.call_tir(cls.fused_group_norm14_silu13, (lv16, lv2219, lv2220), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2221: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[16]
            lv2222: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[108]
            lv18 = R.call_tir(cls.fused_conv2d29_add21, (lv17, lv2221, lv2222), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2223: R.Tensor((512,), dtype="float32") = model_params[21]
            lv2224: R.Tensor((512,), dtype="float32") = model_params[20]
            lv19 = R.call_tir(cls.fused_group_norm14_silu13, (lv18, lv2223, lv2224), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2225: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[17]
            lv2226: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[109]
            lv20 = R.call_tir(cls.fused_conv2d29_add21_add22_divide6_divide6, (lv19, lv2225, lv2226, lv16), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2227: R.Tensor((512,), dtype="float32") = model_params[25]
            lv2228: R.Tensor((512,), dtype="float32") = model_params[24]
            lv21 = R.call_tir(cls.fused_group_norm14_silu13, (lv20, lv2227, lv2228), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2229: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[22]
            lv2230: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[110]
            lv22_1 = R.call_tir(cls.fused_conv2d29_add21, (lv21, lv2229, lv2230), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2231: R.Tensor((512,), dtype="float32") = model_params[27]
            lv2232: R.Tensor((512,), dtype="float32") = model_params[26]
            lv23_1 = R.call_tir(cls.fused_group_norm14_silu13, (lv22_1, lv2231, lv2232), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2233: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[23]
            lv2234: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[111]
            lv24 = R.call_tir(cls.fused_conv2d29_add21_add22_divide6, (lv23_1, lv2233, lv2234, lv20), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2235: R.Tensor((512,), dtype="float32") = model_params[31]
            lv2236: R.Tensor((512,), dtype="float32") = model_params[30]
            lv25 = R.call_tir(cls.fused_group_norm14_silu13, (lv24, lv2235, lv2236), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2237: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[28]
            lv2238: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[112]
            lv26 = R.call_tir(cls.fused_conv2d29_add21, (lv25, lv2237, lv2238), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2239: R.Tensor((512,), dtype="float32") = model_params[33]
            lv2240: R.Tensor((512,), dtype="float32") = model_params[32]
            lv27 = R.call_tir(cls.fused_group_norm14_silu13, (lv26, lv2239, lv2240), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2241: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[29]
            lv2242: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[113]
            lv28 = R.call_tir(cls.fused_conv2d29_add21_add22_divide6, (lv27, lv2241, lv2242, lv24), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2243: R.Tensor((512,), dtype="float32") = model_params[37]
            lv2244: R.Tensor((512,), dtype="float32") = model_params[36]
            lv29 = R.call_tir(cls.fused_group_norm14_silu13, (lv28, lv2243, lv2244), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2245: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[34]
            lv2246: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[114]
            lv30 = R.call_tir(cls.fused_conv2d29_add21, (lv29, lv2245, lv2246), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2247: R.Tensor((512,), dtype="float32") = model_params[39]
            lv2248: R.Tensor((512,), dtype="float32") = model_params[38]
            lv31 = R.call_tir(cls.fused_group_norm14_silu13, (lv30, lv2247, lv2248), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv2249: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[35]
            lv2250: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[115]
            lv32 = R.call_tir(cls.fused_conv2d29_add21_add22_divide6, (lv31, lv2249, lv2250, lv28), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv104 = R.call_tir(cls.resize2d2, (lv32,), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2251: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[40]
            lv2252: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[116]
            lv33 = R.call_tir(cls.fused_conv2d30_add24, (lv104, lv2251, lv2252), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2253: R.Tensor((512,), dtype="float32") = model_params[44]
            lv2254: R.Tensor((512,), dtype="float32") = model_params[43]
            lv34 = R.call_tir(cls.fused_group_norm16_silu14, (lv33, lv2253, lv2254), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2255: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[41]
            lv2256: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[117]
            lv35 = R.call_tir(cls.fused_conv2d30_add24, (lv34, lv2255, lv2256), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2257: R.Tensor((512,), dtype="float32") = model_params[46]
            lv2258: R.Tensor((512,), dtype="float32") = model_params[45]
            lv36 = R.call_tir(cls.fused_group_norm16_silu14, (lv35, lv2257, lv2258), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2259: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[42]
            lv2260: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[118]
            lv37 = R.call_tir(cls.fused_conv2d30_add24_add25_divide7, (lv36, lv2259, lv2260, lv33), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2261: R.Tensor((512,), dtype="float32") = model_params[50]
            lv2262: R.Tensor((512,), dtype="float32") = model_params[49]
            lv38 = R.call_tir(cls.fused_group_norm16_silu14, (lv37, lv2261, lv2262), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2263: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[47]
            lv2264: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[119]
            lv39 = R.call_tir(cls.fused_conv2d30_add24, (lv38, lv2263, lv2264), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2265: R.Tensor((512,), dtype="float32") = model_params[52]
            lv2266: R.Tensor((512,), dtype="float32") = model_params[51]
            lv40 = R.call_tir(cls.fused_group_norm16_silu14, (lv39, lv2265, lv2266), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2267: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[48]
            lv2268: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[120]
            lv41 = R.call_tir(cls.fused_conv2d30_add24_add25_divide7, (lv40, lv2267, lv2268, lv37), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2269: R.Tensor((512,), dtype="float32") = model_params[56]
            lv2270: R.Tensor((512,), dtype="float32") = model_params[55]
            lv42 = R.call_tir(cls.fused_group_norm16_silu14, (lv41, lv2269, lv2270), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2271: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[53]
            lv2272: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[121]
            lv43 = R.call_tir(cls.fused_conv2d30_add24, (lv42, lv2271, lv2272), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2273: R.Tensor((512,), dtype="float32") = model_params[58]
            lv2274: R.Tensor((512,), dtype="float32") = model_params[57]
            lv44_1 = R.call_tir(cls.fused_group_norm16_silu14, (lv43, lv2273, lv2274), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv2275: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[54]
            lv2276: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[122]
            lv45_1 = R.call_tir(cls.fused_conv2d30_add24_add25_divide7, (lv44_1, lv2275, lv2276, lv41), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv144 = R.call_tir(cls.resize2d3, (lv45_1,), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv2277: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[59]
            lv2278: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[123]
            lv46 = R.call_tir(cls.fused_conv2d31_add26, (lv144, lv2277, lv2278), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv2279: R.Tensor((512,), dtype="float32") = model_params[64]
            lv2280: R.Tensor((512,), dtype="float32") = model_params[63]
            lv47 = R.call_tir(cls.fused_group_norm17_silu15, (lv46, lv2279, lv2280), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv2281: R.Tensor((256, 512, 3, 3), dtype="float32") = model_params[60]
            lv2282: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[124]
            lv48 = R.call_tir(cls.fused_conv2d32_add27, (lv47, lv2281, lv2282), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2283: R.Tensor((256,), dtype="float32") = model_params[66]
            lv2284: R.Tensor((256,), dtype="float32") = model_params[65]
            lv49 = R.call_tir(cls.fused_group_norm18_silu16, (lv48, lv2283, lv2284), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2285: R.Tensor((256, 512, 1, 1), dtype="float32") = model_params[62]
            lv2286: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[126]
            lv50 = R.call_tir(cls.fused_conv2d34_add27, (lv46, lv2285, lv2286), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2287: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[61]
            lv2288: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[125]
            lv51 = R.call_tir(cls.fused_conv2d33_add27_add28_divide8, (lv49, lv2287, lv2288, lv50), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2289: R.Tensor((256,), dtype="float32") = model_params[70]
            lv2290: R.Tensor((256,), dtype="float32") = model_params[69]
            lv52 = R.call_tir(cls.fused_group_norm18_silu16, (lv51, lv2289, lv2290), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2291: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[67]
            lv2292: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[127]
            lv53 = R.call_tir(cls.fused_conv2d33_add27, (lv52, lv2291, lv2292), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2293: R.Tensor((256,), dtype="float32") = model_params[72]
            lv2294: R.Tensor((256,), dtype="float32") = model_params[71]
            lv54 = R.call_tir(cls.fused_group_norm18_silu16, (lv53, lv2293, lv2294), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2295: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[68]
            lv2296: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[128]
            lv55 = R.call_tir(cls.fused_conv2d33_add27_add28_divide8, (lv54, lv2295, lv2296, lv51), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2297: R.Tensor((256,), dtype="float32") = model_params[76]
            lv2298: R.Tensor((256,), dtype="float32") = model_params[75]
            lv56 = R.call_tir(cls.fused_group_norm18_silu16, (lv55, lv2297, lv2298), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2299: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[73]
            lv2300: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[129]
            lv57 = R.call_tir(cls.fused_conv2d33_add27, (lv56, lv2299, lv2300), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2301: R.Tensor((256,), dtype="float32") = model_params[78]
            lv2302: R.Tensor((256,), dtype="float32") = model_params[77]
            lv58 = R.call_tir(cls.fused_group_norm18_silu16, (lv57, lv2301, lv2302), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv2303: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[74]
            lv2304: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[130]
            lv59 = R.call_tir(cls.fused_conv2d33_add27_add28_divide8, (lv58, lv2303, lv2304, lv55), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv187 = R.call_tir(cls.resize2d4, (lv59,), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv2305: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[79]
            lv2306: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[131]
            lv60 = R.call_tir(cls.fused_conv2d35_add29, (lv187, lv2305, lv2306), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv2307: R.Tensor((256,), dtype="float32") = model_params[84]
            lv2308: R.Tensor((256,), dtype="float32") = model_params[83]
            lv61 = R.call_tir(cls.fused_group_norm19_silu17, (lv60, lv2307, lv2308), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv2309: R.Tensor((128, 256, 3, 3), dtype="float32") = model_params[80]
            lv2310: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[132]
            lv62 = R.call_tir(cls.fused_conv2d36_add30, (lv61, lv2309, lv2310), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2311: R.Tensor((128,), dtype="float32") = model_params[86]
            lv2312: R.Tensor((128,), dtype="float32") = model_params[85]
            lv63 = R.call_tir(cls.fused_group_norm20_silu18, (lv62, lv2311, lv2312), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2313: R.Tensor((128, 256, 1, 1), dtype="float32") = model_params[82]
            lv2314: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[134]
            lv64 = R.call_tir(cls.fused_conv2d38_add30, (lv60, lv2313, lv2314), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2315: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[81]
            lv2316: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[133]
            lv65 = R.call_tir(cls.fused_conv2d37_add30_add31_divide9, (lv63, lv2315, lv2316, lv64), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2317: R.Tensor((128,), dtype="float32") = model_params[90]
            lv2318: R.Tensor((128,), dtype="float32") = model_params[89]
            lv66 = R.call_tir(cls.fused_group_norm20_silu18, (lv65, lv2317, lv2318), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2319: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[87]
            lv2320: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[135]
            lv67 = R.call_tir(cls.fused_conv2d37_add30, (lv66, lv2319, lv2320), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2321: R.Tensor((128,), dtype="float32") = model_params[92]
            lv2322: R.Tensor((128,), dtype="float32") = model_params[91]
            lv68 = R.call_tir(cls.fused_group_norm20_silu18, (lv67, lv2321, lv2322), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2323: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[88]
            lv2324: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[136]
            lv69 = R.call_tir(cls.fused_conv2d37_add30_add31_divide9, (lv68, lv2323, lv2324, lv65), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2325: R.Tensor((128,), dtype="float32") = model_params[96]
            lv2326: R.Tensor((128,), dtype="float32") = model_params[95]
            lv70 = R.call_tir(cls.fused_group_norm20_silu18, (lv69, lv2325, lv2326), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2327: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[93]
            lv2328: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[137]
            lv71 = R.call_tir(cls.fused_conv2d37_add30, (lv70, lv2327, lv2328), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2329: R.Tensor((128,), dtype="float32") = model_params[98]
            lv2330: R.Tensor((128,), dtype="float32") = model_params[97]
            lv72 = R.call_tir(cls.fused_group_norm20_silu18, (lv71, lv2329, lv2330), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2331: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[94]
            lv2332: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[138]
            lv73 = R.call_tir(cls.fused_conv2d37_add30_add31_divide9, (lv72, lv2331, lv2332, lv69), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2333: R.Tensor((128,), dtype="float32") = model_params[2]
            lv2334: R.Tensor((128,), dtype="float32") = model_params[1]
            lv74 = R.call_tir(cls.fused_group_norm20_silu18, (lv73, lv2333, lv2334), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv2335: R.Tensor((3, 128, 3, 3), dtype="float32") = model_params[3]
            lv2336: R.Tensor((1, 3, 1, 1), dtype="float32") = model_params[139]
            lv75 = R.call_tir(cls.fused_conv2d39_add32_divide10_add33_tir_clip, (lv74, lv2335, lv2336), out_sinfo=R.Tensor((1, 3, 512, 512), dtype="float32"))
            lv76 = R.call_tir(cls.fused_transpose32_multiply12_tir_round, (lv75,), out_sinfo=R.Tensor((1, 512, 512, 3), dtype="float32"))
            gv: R.Tensor((1, 512, 512, 3), dtype="float32") = lv76
            R.output(gv)
        return gv
